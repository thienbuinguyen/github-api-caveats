{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "from random import sample, shuffle, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caveat_files_dir = './output/java_12_spec_caveat_sentences/'\n",
    "caveats_dict = {}\n",
    "\n",
    "files = glob.glob(caveat_files_dir + '*.json')\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        arr = json.load(f)\n",
    "        for caveat in arr:\n",
    "            caveats_dict[caveat['id']] = caveat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 4712\n",
      "Number of caveats: 49023\n",
      "Number of class level caveat sentences: 10589\n",
      "Number of api level caveat sentences: 38270\n",
      "Number of misc caveat objects: 26426\n",
      "Number of deprecated caveats: 1522\n",
      "{'constructor': 4172, 'method': 33827, 'field': 6403}\n"
     ]
    }
   ],
   "source": [
    "# Print statistics about the caveats found using keywords\n",
    "\n",
    "caveat_type_count = {} # map caveat types to their counts\n",
    "print(\"Number of classes: {}\".format(len(files)))\n",
    "\n",
    "class_level_sentences = 0\n",
    "caveat_sentences = 0\n",
    "caveat_misc_objects = 0\n",
    "deprecated = 0\n",
    "num_caveats = len(caveats_dict)\n",
    "\n",
    "for key in caveats_dict:\n",
    "    caveat = caveats_dict[key]\n",
    "    \n",
    "    if \"class_level_caveat_sentences\" in caveat:\n",
    "        class_level_sentences += len(caveat[\"class_level_caveat_sentences\"])\n",
    "    else:\n",
    "        caveat_sentences += len(caveat['caveat_sentences'])\n",
    "        caveat_misc_objects += len(caveat['caveat_misc'])\n",
    "\n",
    "    if caveat['deprecated']:\n",
    "        deprecated += 1\n",
    "    \n",
    "    if 'type' in caveat:\n",
    "        if caveat['type'] == '':\n",
    "            print(caveat)\n",
    "            break\n",
    "        \n",
    "        if not caveat['type'] in caveat_type_count:\n",
    "            caveat_type_count[caveat['type']] = 1\n",
    "        else:\n",
    "            caveat_type_count[caveat['type']] += 1\n",
    "\n",
    "print(\"Number of caveats: {}\".format(num_caveats))\n",
    "print(\"Number of class level caveat sentences: {}\".format(class_level_sentences))\n",
    "print(\"Number of api level caveat sentences: {}\".format(caveat_sentences))  \n",
    "print(\"Number of misc caveat objects: {}\".format(caveat_misc_objects))\n",
    "print(\"Number of deprecated caveats: {}\".format(deprecated))\n",
    "print(caveat_type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_doccano_import_file(file, arr):\n",
    "    \"\"\" Write the dict objects within an input array to a json file in doccano import format \"\"\"\n",
    "    with open(file, 'w+') as f:\n",
    "        for obj in arr:\n",
    "            f.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class sentences: 10125\n",
      "Number of method sentences: 33155\n",
      "Number of field sentences: 1844\n",
      "Number of constructor sentences: 2184\n",
      "-----------------------------------\n",
      "Number of exception sentences: 31679\n",
      "Number of return sentences: 11279\n",
      "Number of parameter sentences: 11376\n"
     ]
    }
   ],
   "source": [
    "caveat_sentences_array = []\n",
    "sentence_id_to_caveat_id = {}\n",
    "\n",
    "for key in caveats_dict:\n",
    "    caveat = caveats_dict[key]\n",
    "    \n",
    "    if caveat['deprecated']:\n",
    "        continue\n",
    " \n",
    "    if 'class_level_caveat_sentences' in caveat:\n",
    "        for sentence in caveat['class_level_caveat_sentences']:\n",
    "            if len(sentence) > 0:\n",
    "                sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                caveat_sentences_array.append({\n",
    "                    'text': sentence, \n",
    "                     'id': len(caveat_sentences_array),\n",
    "                     'type': 'class'\n",
    "                })\n",
    "    else:\n",
    "        for sentence in caveat['caveat_sentences']:\n",
    "            if len(sentence) > 0:\n",
    "                sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                caveat_sentences_array.append({\n",
    "                    'text': sentence, \n",
    "                     'id': len(caveat_sentences_array),\n",
    "                     'type': caveat['type']\n",
    "                })\n",
    "\n",
    "            # add sentences for throws, returns etc...\n",
    "            if caveat['type'] in ['method', 'constructor']:\n",
    "                for misc in caveat['caveat_misc']:\n",
    "                    for misc_sentence in misc['text_list']:\n",
    "                        if len(misc_sentence) > 0:\n",
    "                            sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                            caveat_sentences_array.append({\n",
    "                                'text': misc_sentence, \n",
    "                                 'id': len(caveat_sentences_array),\n",
    "                                'type': 'misc@' + misc['name']\n",
    "                            })\n",
    "\n",
    "class_sentences = [o for o in caveat_sentences_array if o['type'] == 'class' and len(o['text']) <= 400]\n",
    "method_sentences = [o for o in caveat_sentences_array if o['type'] == 'method' and len(o['text']) <= 400]\n",
    "field_sentences = [o for o in caveat_sentences_array if o['type'] == 'field' and len(o['text']) <= 400]\n",
    "constructor_sentences = [o for o in caveat_sentences_array if o['type'] == 'constructor' and len(o['text']) <= 400]\n",
    "misc_sentences = [o for o in caveat_sentences_array if 'misc' in o['type'] and len(o['text']) <= 400]\n",
    "        \n",
    "print(\"Number of class sentences: {}\".format(len(class_sentences)))\n",
    "print(\"Number of method sentences: {}\".format(len(method_sentences)))\n",
    "print(\"Number of field sentences: {}\".format(len(field_sentences)))\n",
    "print(\"Number of constructor sentences: {}\".format(len(constructor_sentences)))\n",
    "\n",
    "exception_sentences = [o for o in misc_sentences if 'Throws:' == o['type'].split('@')[1]]\n",
    "return_sentences = [o for o in misc_sentences if 'Returns:' == o['type'].split('@')[1]]\n",
    "parameter_sentences = [o for o in misc_sentences if 'Parameters:' == o['type'].split('@')[1]]\n",
    "\n",
    "print('-----------------------------------')\n",
    "print('Number of exception sentences: {}'.format(len(exception_sentences)))\n",
    "print('Number of return sentences: {}'.format(len(return_sentences)))\n",
    "print('Number of parameter sentences: {}'.format(len(parameter_sentences)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique class sentences: 9089\n",
      "Number of unique method sentences: 22309\n",
      "Number of unique field sentences: 1580\n",
      "Number of unique constructor sentences: 1302\n",
      "-----------------------------------\n",
      "Number of exception sentences: 6098\n",
      "Number of return sentences: 4076\n",
      "Number of parameter sentences: 2767\n"
     ]
    }
   ],
   "source": [
    "def get_unique_text_objs(arr):\n",
    "    seen_text = set()\n",
    "    filtered = []\n",
    "    for obj in arr:\n",
    "        if not obj['text'] in seen_text:\n",
    "            filtered.append(obj)\n",
    "            seen_text.add(obj['text'])\n",
    "        \n",
    "    return filtered\n",
    "\n",
    "seed(42)\n",
    "\n",
    "filtered_class_sentences = get_unique_text_objs(class_sentences)\n",
    "filtered_method_sentences = get_unique_text_objs(method_sentences)\n",
    "filtered_field_sentences = get_unique_text_objs(field_sentences)\n",
    "filtered_constructor_sentences = get_unique_text_objs(constructor_sentences)\n",
    "# misc sentences\n",
    "filtered_exception_sentences = get_unique_text_objs(exception_sentences)\n",
    "filtered_return_sentences = get_unique_text_objs(return_sentences)\n",
    "filtered_parameter_sentences = get_unique_text_objs(parameter_sentences)\n",
    "\n",
    "print(\"Number of unique class sentences: {}\".format(len(filtered_class_sentences)))\n",
    "print(\"Number of unique method sentences: {}\".format(len(filtered_method_sentences)))\n",
    "print(\"Number of unique field sentences: {}\".format(len(filtered_field_sentences)))\n",
    "print(\"Number of unique constructor sentences: {}\".format(len(filtered_constructor_sentences)))\n",
    "print('-----------------------------------')\n",
    "print('Number of exception sentences: {}'.format(len(filtered_exception_sentences)))\n",
    "print('Number of return sentences: {}'.format(len(filtered_return_sentences)))\n",
    "print('Number of parameter sentences: {}'.format(len(filtered_parameter_sentences)))\n",
    "\n",
    "# sample 384 of each list for 95% confidence interval with 5% error margin\n",
    "filtered_class_sentences = sample(filtered_class_sentences, 384)\n",
    "filtered_method_sentences = sample(filtered_method_sentences, 384)\n",
    "filtered_field_sentences = sample(filtered_field_sentences, 384)\n",
    "filtered_constructor_sentences = sample(filtered_constructor_sentences, 384)\n",
    "# misc sentences\n",
    "filtered_exception_sentences = sample(filtered_exception_sentences, 384)\n",
    "filtered_return_sentences = sample(filtered_return_sentences, 384)\n",
    "filtered_parameter_sentences = sample(filtered_parameter_sentences, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to annotate to different files\n",
    "write_to_doccano_import_file('./output/method_sentences_doccano.jsonl', filtered_method_sentences)\n",
    "write_to_doccano_import_file('./output/constructor_sentences_doccano.jsonl', filtered_constructor_sentences)\n",
    "write_to_doccano_import_file('./output/exception_sentences_doccano.jsonl', filtered_exception_sentences)\n",
    "write_to_doccano_import_file('./output/return_sentences_doccano.jsonl', filtered_return_sentences)\n",
    "write_to_doccano_import_file('./output/parameter_sentences_doccano.jsonl', filtered_parameter_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify labelled data from doccano\n",
    "def get_labelled_doccano_data(labelled_file, metadata):\n",
    "    with open(labelled_file) as f, open(metadata) as metadata_f:\n",
    "        id_to_label = {}\n",
    "        arr = json.load(metadata_f)\n",
    "        for e in arr:\n",
    "            id_to_label[e['id']] = e['text']\n",
    "\n",
    "        data = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            obj = json.loads(line)\n",
    "            obj['labels'] = [id_to_label[i['label']] for i in obj['annotations']]\n",
    "            obj.pop('annotations', None)\n",
    "            obj.pop('meta', None)\n",
    "            obj.pop('annotation_approver', None)\n",
    "            data.append(obj)\n",
    "    \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelled_data(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def get_label_counts(arr):\n",
    "    counts = {}\n",
    "    \n",
    "    for e in arr:\n",
    "        for label in e['labels']:\n",
    "            if not label in counts:\n",
    "                counts[label] = 1\n",
    "            else:\n",
    "                counts[label] += 1\n",
    "                \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type 'TextIOWrapper' is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ff8b5bc280dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./labelled_return_sentences.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreturn_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcept_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_labelled_doccano_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/labelled_exception_sentences_doccano.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./output/exception_metadata.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcept_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_labelled_doccano_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/labelled_parameters_doccano.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./output/parameter_metadata.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcept_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_labelled_doccano_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/labelled_return_doccano.jsonl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./output/return_metadata.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/nlp/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/nlp/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    435\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/nlp/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type 'TextIOWrapper' is not JSON serializable"
     ]
    }
   ],
   "source": [
    "with open('./labelled_exception_sentences.json', 'w+') as except_f, \\\n",
    "    open('./labelled_parameter_sentences.json', 'w+') as param_f, \\\n",
    "    open('./labelled_return_sentences.json', 'w+') as return_f:\n",
    "    \n",
    "    json.dump(get_labelled_doccano_data('./output/labelled_exception_sentences_doccano.jsonl', './output/exception_metadata.json'))\n",
    "    json.dump(except_f, get_labelled_doccano_data('./output/labelled_parameters_doccano.jsonl', './output/parameter_metadata.json'))\n",
    "    json.dump(except_f, get_labelled_doccano_data('./output/labelled_return_doccano.jsonl', './output/return_metadata.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
