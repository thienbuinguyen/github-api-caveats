{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson\n",
    "import glob\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from random import sample, shuffle, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the java document caveats\n",
    "caveat_files_dir = './output/java_12_spec_caveat_sentences_revised/'\n",
    "caveats_dict = {}\n",
    "class_to_caveat_ids = {}\n",
    "\n",
    "files = glob.glob(caveat_files_dir + '*.json')\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        arr = ujson.load(f)\n",
    "        full_class_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        simple_class_name = full_class_name.split('.')[-1]\n",
    "        for caveat in arr:\n",
    "            caveats_dict[caveat['id']] = caveat\n",
    "            \n",
    "            if simple_class_name not in class_to_caveat_ids:\n",
    "                class_to_caveat_ids[simple_class_name] = []\n",
    "            class_to_caveat_ids[simple_class_name].append(caveat['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Overall Statistics-------------\n",
      "Number of classes: 4712\n",
      "Number of class level caveat sentences: 9964\n",
      "Number of api level caveat sentences: 37578\n",
      "Number of misc sentences: 67701\n",
      "Number of deprecated caveats: 1522\n",
      "Number of sentences for deprecated caveats: 2562\n",
      "{'constructor': 4172, 'method': 33827, 'field': 6403}\n",
      "44402\n",
      "Total sentences: 115243\n"
     ]
    }
   ],
   "source": [
    "# Print statistics about the caveats found using keywords\n",
    "\n",
    "caveat_type_count = {} # map caveat types to their counts\n",
    "\n",
    "class_level_sentences = 0\n",
    "caveat_sentences = 0\n",
    "caveat_misc_sentences = 0\n",
    "deprecated = 0\n",
    "num_caveats = len(caveats_dict)\n",
    "sent_deprecated = 0\n",
    "\n",
    "for key in caveats_dict:\n",
    "    caveat = caveats_dict[key]\n",
    "    \n",
    "    if caveat['deprecated']:\n",
    "        if \"class_caveat_sentences\" in caveat:\n",
    "            sent_deprecated += len(caveat[\"class_caveat_sentences\"])\n",
    "        else:        \n",
    "            sent_deprecated += len(caveat['sentences'])\n",
    "            for misc in caveat['caveat_misc']:\n",
    "                if 'parameters' in misc:\n",
    "                    for obj in misc['parameters']:\n",
    "                        sent_deprecated += len(obj['sentences'])\n",
    "                elif 'exceptions' in misc:\n",
    "                    for obj in misc['exceptions']:\n",
    "                        sent_deprecated += len(obj['sentences'])\n",
    "                else:\n",
    "                     sent_deprecated += len(misc['list'])\n",
    "\n",
    "    if \"class_caveat_sentences\" in caveat:\n",
    "        class_level_sentences += len(caveat[\"class_caveat_sentences\"])\n",
    "    else:        \n",
    "        caveat_sentences += len(caveat['sentences'])\n",
    "        for misc in caveat['caveat_misc']:\n",
    "            if 'parameters' in misc:\n",
    "                for obj in misc['parameters']:\n",
    "                    caveat_misc_sentences += len(obj['sentences'])\n",
    "            elif 'exceptions' in misc:\n",
    "                for obj in misc['exceptions']:\n",
    "                    caveat_misc_sentences += len(obj['sentences'])\n",
    "            else:\n",
    "                 caveat_misc_sentences += len(misc['list'])\n",
    "\n",
    "    if caveat['deprecated']:\n",
    "        deprecated += 1\n",
    "    \n",
    "    if 'type' in caveat:        \n",
    "        if not caveat['type'] in caveat_type_count:\n",
    "            caveat_type_count[caveat['type']] = 1\n",
    "        else:\n",
    "            caveat_type_count[caveat['type']] += 1\n",
    "\n",
    "print(\"-------------Overall Statistics-------------\")\n",
    "print(\"Number of classes: {}\".format(len(files)))\n",
    "print(\"Number of class level caveat sentences: {}\".format(class_level_sentences))\n",
    "print(\"Number of api level caveat sentences: {}\".format(caveat_sentences))  \n",
    "print(\"Number of misc sentences: {}\".format(caveat_misc_sentences))\n",
    "print(\"Number of deprecated caveats: {}\".format(deprecated))\n",
    "print(\"Number of sentences for deprecated caveats: {}\".format(sent_deprecated))\n",
    "print(caveat_type_count)\n",
    "print(sum([caveat_type_count[k] for k in caveat_type_count]))\n",
    "print(\"Total sentences: {}\".format(class_level_sentences + caveat_sentences + caveat_misc_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_doccano_import_file(file, arr):\n",
    "    \"\"\" Write the dict objects within an input array to a json file in doccano import format \"\"\"\n",
    "    with open(file, 'w+') as f:\n",
    "        for obj in arr:\n",
    "            f.write(ujson.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_caveat_sentences': ['try { // Encode a String into bytes String inputString = \"blahblahblah\"; byte[] input = inputString.getBytes(\"UTF-8\"); // Compress the bytes byte[] output = new byte[100]; Deflater compresser = new Deflater(); compresser.setInput(input); compresser.finish(); int compressedDataLength = compresser.deflate(output); compresser.end(); // Decompress the bytes Inflater decompresser = new Inflater(); decompresser.setInput(output, 0, compressedDataLength); byte[] result = new byte[100]; int resultLength = decompresser.inflate(result); decompresser.end(); // Decode the bytes into a String String outputString = new String(result, 0, resultLength, \"UTF-8\"); } catch (java.io.UnsupportedEncodingException ex) { // handle } catch (java.util.zip.DataFormatException ex) { // handle }'], 'deprecated': False, 'id': 28877}\n",
      "-------------Filtered Sentence Statistics-------------\n",
      "Number of class sentences: 9819\n",
      "Number of method sentences: 32190\n",
      "Number of field sentences: 1779\n",
      "Number of constructor sentences: 2735\n",
      "Number of exception sentences: 32242\n",
      "Number of return sentences: 11549\n",
      "Number of parameter sentences: 12446\n",
      "Total: 105690\n"
     ]
    }
   ],
   "source": [
    "caveat_sentences_array = []\n",
    "sentence_id_to_caveat_id = {}\n",
    "c = 0\n",
    "\n",
    "for key in caveats_dict:\n",
    "    caveat = caveats_dict[key]\n",
    "    \n",
    "    if caveat['deprecated']:\n",
    "        continue\n",
    " \n",
    "    if 'class_caveat_sentences' in caveat:\n",
    "        for sentence in caveat['class_caveat_sentences']:\n",
    "            if len(sentence) > 0:\n",
    "                sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                if '// Encode a String into bytes String inputString = \"blahblahblah\"' in sentence:\n",
    "                    print(caveat)\n",
    "                caveat_sentences_array.append({\n",
    "                    'text': sentence, \n",
    "                     'id': len(caveat_sentences_array),\n",
    "                     'type': 'class'\n",
    "                })\n",
    "    else:\n",
    "        for sentence in caveat['sentences']:\n",
    "            if len(sentence) > 0:\n",
    "                sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                caveat_sentences_array.append({\n",
    "                    'text': sentence, \n",
    "                     'id': len(caveat_sentences_array),\n",
    "                     'type': caveat['type']\n",
    "                })\n",
    "\n",
    "            # add sentences for throws, returns etc...\n",
    "            if caveat['type'] in ['method', 'constructor']:\n",
    "                for misc in caveat['caveat_misc']:\n",
    "                    for misc_obj in misc['list']:\n",
    "                        if type(misc_obj) == str:\n",
    "                            sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                            caveat_sentences_array.append({\n",
    "                                'text': misc_obj, \n",
    "                                 'id': len(caveat_sentences_array),\n",
    "                                'type': 'misc@' + misc['name']})\n",
    "                        else: # parameters and exception objects\n",
    "                            for misc_sentence in misc_obj['sentences']:\n",
    "                                sentence_id_to_caveat_id[len(caveat_sentences_array)] = caveat['id']\n",
    "                                caveat_sentences_array.append({\n",
    "                                    'text': misc_sentence, \n",
    "                                     'id': len(caveat_sentences_array),\n",
    "                                    'type': 'misc@' + misc['name']})\n",
    "\n",
    "# restrict text length\n",
    "# class_sentences = [o for o in caveat_sentences_array if o['type'] == 'class' and len(o['text']) <= 400]\n",
    "# method_sentences = [o for o in caveat_sentences_array if o['type'] == 'method' and len(o['text']) <= 400]\n",
    "# field_sentences = [o for o in caveat_sentences_array if o['type'] == 'field' and len(o['text']) <= 400]\n",
    "# constructor_sentences = [o for o in caveat_sentences_array if o['type'] == 'constructor' and len(o['text']) <= 400]\n",
    "# misc_sentences = [o for o in caveat_sentences_array if 'misc' in o['type'] and len(o['text']) <= 400]\n",
    "\n",
    "class_sentences = [o for o in caveat_sentences_array if o['type'] == 'class']\n",
    "method_sentences = [o for o in caveat_sentences_array if o['type'] == 'method']\n",
    "field_sentences = [o for o in caveat_sentences_array if o['type'] == 'field']\n",
    "constructor_sentences = [o for o in caveat_sentences_array if o['type'] == 'constructor']\n",
    "misc_sentences = [o for o in caveat_sentences_array if 'misc' in o['type']]\n",
    "\n",
    "print(\"-------------Filtered Sentence Statistics-------------\")\n",
    "print(\"Number of class sentences: {}\".format(len(class_sentences)))\n",
    "print(\"Number of method sentences: {}\".format(len(method_sentences)))\n",
    "print(\"Number of field sentences: {}\".format(len(field_sentences)))\n",
    "print(\"Number of constructor sentences: {}\".format(len(constructor_sentences)))\n",
    "\n",
    "exception_sentences = [o for o in misc_sentences if 'Throws:' == o['type'].split('@')[1]]\n",
    "return_sentences = [o for o in misc_sentences if 'Returns:' == o['type'].split('@')[1]]\n",
    "parameter_sentences = [o for o in misc_sentences if 'Parameters:' == o['type'].split('@')[1]]\n",
    "\n",
    "print('Number of exception sentences: {}'.format(len(exception_sentences)))\n",
    "print('Number of return sentences: {}'.format(len(return_sentences)))\n",
    "print('Number of parameter sentences: {}'.format(len(parameter_sentences)))\n",
    "print('Total: {}'.format(len(caveat_sentences_array)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Examples: Creating and using text boundaries: public static void main(String args[]) { if (args.length == 1) { String stringToExamine = args[0]; //print each word in order BreakIterator boundary = BreakIterator.getWordInstance(); boundary.setText(stringToExamine); printEachForward(boundary, stringToExamine); //print each sentence in reverse order boundary = BreakIterator.getSentenceInstance(Locale.US); boundary.setText(stringToExamine); printEachBackward(boundary, stringToExamine); printFirst(boundary, stringToExamine); printLast(boundary, stringToExamine); } } Print each element in order: public static void printEachForward(BreakIterator boundary, String source) { int start = boundary.first(); for (int end = boundary.next(); end != BreakIterator.DONE; start = end, end = boundary.next()) { System.out.println(source.substring(start,end)); } } Print each element in reverse order: public static void printEachBackward(BreakIterator boundary, String source) { int end = boundary.last(); for (int start = boundary.previous(); start != BreakIterator.DONE; end = start, start = boundary.previous()) { System.out.println(source.substring(start,end)); } } Print first element: public static void printFirst(BreakIterator boundary, String source) { int start = boundary.first(); int end = boundary.next(); System.out.println(source.substring(start,end)); } Print last element: public static void printLast(BreakIterator boundary, String source) { int end = boundary.last(); int start = boundary.previous(); System.out.println(source.substring(start,end)); } Print the element at a specified position: public static void printAt(BreakIterator boundary, int pos, String source) { int end = boundary.following(pos); int start = boundary.previous(); System.out.println(source.substring(start,end)); } Find the next word: public static int nextWordStartAfter(int pos, String text) { BreakIterator wb = BreakIterator.getWordInstance(); wb.setText(text); int last = wb.following(pos); int current = wb.next(); while (current != BreakIterator.DONE) { for (int p = last; p < current; p++) { if (Character.isLetter(text.codePointAt(p))) return last; } last = current; current = wb.next(); } return BreakIterator.DONE; } (The iterator returned by BreakIterator.getWordInstance() is unique in that the break positions it returns don't represent both the start and end of the thing being iterated over.\", 'id': 3619, 'type': 'class'}\n"
     ]
    }
   ],
   "source": [
    "a = [o for o in caveat_sentences_array if o['type'] == 'class' and len(o['text']) > 400]\n",
    "print(a[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7645\n",
      "BreakIterator\n"
     ]
    }
   ],
   "source": [
    "sent_id = 3619\n",
    "print(sentence_id_to_caveat_id[sent_id])\n",
    "\n",
    "for k in class_to_caveat_ids:\n",
    "    if caveats_dict[sentence_id_to_caveat_id[sent_id]]['id'] in class_to_caveat_ids[k]:\n",
    "        print(k)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Unique Filtered Sentence Statistics-------------\n",
      "Number of unique class sentences: 8718\n",
      "Number of unique method sentences: 21984\n",
      "Number of unique field sentences: 1535\n",
      "Number of unique constructor sentences: 1958\n",
      "Number of unique exception sentences: 4852\n",
      "Number of unique return sentences: 4306\n",
      "Number of unique parameter sentences: 2653\n"
     ]
    }
   ],
   "source": [
    "def get_unique_text_objs(arr):\n",
    "    seen_text = set()\n",
    "    filtered = []\n",
    "    for obj in arr:\n",
    "        if not obj['text'] in seen_text:\n",
    "            filtered.append(obj)\n",
    "            seen_text.add(obj['text'])\n",
    "        \n",
    "    return filtered\n",
    "\n",
    "seed(42)\n",
    "\n",
    "\n",
    "filtered_class_sentences = get_unique_text_objs(class_sentences)\n",
    "filtered_method_sentences = get_unique_text_objs(method_sentences)\n",
    "filtered_field_sentences = get_unique_text_objs(field_sentences)\n",
    "filtered_constructor_sentences = get_unique_text_objs(constructor_sentences)\n",
    "# misc sentences\n",
    "filtered_exception_sentences = get_unique_text_objs(exception_sentences)\n",
    "filtered_return_sentences = get_unique_text_objs(return_sentences)\n",
    "filtered_parameter_sentences = get_unique_text_objs(parameter_sentences)\n",
    "\n",
    "print(\"-------------Unique Filtered Sentence Statistics-------------\")\n",
    "print(\"Number of unique class sentences: {}\".format(len(filtered_class_sentences)))\n",
    "print(\"Number of unique method sentences: {}\".format(len(filtered_method_sentences)))\n",
    "print(\"Number of unique field sentences: {}\".format(len(filtered_field_sentences)))\n",
    "print(\"Number of unique constructor sentences: {}\".format(len(filtered_constructor_sentences)))\n",
    "print('Number of unique exception sentences: {}'.format(len(filtered_exception_sentences)))\n",
    "print('Number of unique return sentences: {}'.format(len(filtered_return_sentences)))\n",
    "print('Number of unique parameter sentences: {}'.format(len(filtered_parameter_sentences)))\n",
    "\n",
    "# # sample 384 of each list for 95% confidence interval with 5% error margin\n",
    "# filtered_class_sentences = sample(filtered_class_sentences, 384)\n",
    "# filtered_method_sentences = sample(filtered_method_sentences, 384)\n",
    "# filtered_field_sentences = sample(filtered_field_sentences, 384)\n",
    "# filtered_constructor_sentences = sample(filtered_constructor_sentences, 384)\n",
    "# # misc sentences\n",
    "# filtered_exception_sentences = sample(filtered_exception_sentences, 384)\n",
    "# filtered_return_sentences = sample(filtered_return_sentences, 384)\n",
    "# filtered_parameter_sentences = sample(filtered_parameter_sentences, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to annotate to different files\n",
    "shuffle(filtered_exception_sentences)\n",
    "shuffle(filtered_parameter_sentences)\n",
    "write_to_doccano_import_file('./output/exception_sentences_doccano.jsonl', filtered_exception_sentences)\n",
    "write_to_doccano_import_file('./output/parameter_sentences_doccano.jsonl', filtered_parameter_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify labelled data from doccano\n",
    "def get_modified_doccano_data(labelled_file, metadata):\n",
    "    with open(labelled_file) as f, open(metadata) as metadata_f:\n",
    "        id_to_label = {}\n",
    "        arr = ujson.load(metadata_f)\n",
    "        for e in arr:\n",
    "            id_to_label[e['id']] = e['text']\n",
    "\n",
    "        data = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            obj = ujson.loads(line)\n",
    "            obj['labels'] = [id_to_label[i['label']] for i in obj['annotations']]\n",
    "            obj.pop('annotations', None)\n",
    "            obj.pop('meta', None)\n",
    "            obj.pop('annotation_approver', None)\n",
    "            data.append(obj)\n",
    "    \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labelled_data(file):\n",
    "    with open(file) as f:\n",
    "        return ujson.load(f)\n",
    "    \n",
    "def get_label_counts(arr):\n",
    "    counts = {}\n",
    "    \n",
    "    for e in arr:\n",
    "        for label in e['labels']:\n",
    "            if not label in counts:\n",
    "                counts[label] = 1\n",
    "            else:\n",
    "                counts[label] += 1\n",
    "                \n",
    "    return counts\n",
    "\n",
    "def write_modified_doccano_data(doccano_file, metadata_file, output_file):\n",
    "    with open(output_file, 'w+') as f:\n",
    "        arr = get_modified_doccano_data(doccano_file, metadata_file)\n",
    "        for obj in arr:\n",
    "            f.write(ujson.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_modified_doccano_data('./output/labelled/new_labelled_param.jsonl' ,\n",
    "                           './output/labelled/new_param_labels.json',\n",
    "                           './labelled_data/new_parameter_full.jsonl')\n",
    "\n",
    "write_modified_doccano_data('./output/labelled/new_labelled_exception.jsonl' ,\n",
    "                           './output/labelled/new_except_labels.json',\n",
    "                           './labelled_data/new_exception_full.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exception': 21, 'Guard': 32, 'Other': 264, 'Temporal': 6}\n",
      "{'Other': 360, 'Temporal': 4, 'Exception': 9, 'Guard': 5}\n",
      "{'Other': 351, 'Temporal': 1}\n",
      "{'Other': 257, 'Temporal': 2, 'Guard': 75, 'Exception': 2}\n"
     ]
    }
   ],
   "source": [
    "# load all labelled sentences\n",
    "constructor_sents = get_labelled_data('./labelled_data/labelled_constructor_sentences.json')[:321]\n",
    "# exception_sents = get_labelled_data('./labelled_exception_sentences.json')[:]\n",
    "method_sents = get_labelled_data('./labelled_data/labelled_method_sentences.json')[:377]\n",
    "return_sents = get_labelled_data('./labelled_data/labelled_return_sentences.json')[:353]\n",
    "parameter_sents = get_labelled_data('./labelled_data/labelled_parameter_sentences.json')[:336]\n",
    "\n",
    "print(get_label_counts(constructor_sents))\n",
    "# print(get_label_counts(exception_sents))\n",
    "print(get_label_counts(method_sents))\n",
    "print(get_label_counts(return_sents))\n",
    "print(get_label_counts(parameter_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ROOT                          \n",
      "                 |                             \n",
      "                 S                            \n",
      "       __________|__________________________   \n",
      "      |                     VP              | \n",
      "      |                 ____|___            |  \n",
      "      |                |        PP          | \n",
      "      |                |     ___|_____      |  \n",
      "      |                |    |         S     | \n",
      "      |                |    |         |     |  \n",
      "      NP               |    |         VP    | \n",
      "  ____|__________      |    |         |     |  \n",
      " DT   JJ    JJ   NN   VBZ   IN       VBG    . \n",
      " |    |     |    |     |    |         |     |  \n",
      "The quick brown fox  sucks  at     jumping  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POS and dependency parsing\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser(url='http://localhost:9010')\n",
    "next(parser.raw_parse('The quick brown fox sucks at jumping.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_counts_doccano_file(path):\n",
    "    counts = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            obj = ujson.loads(line)\n",
    "            if len(obj['labels']) > 0:\n",
    "                for label in obj['labels']:\n",
    "                    if label not in counts:\n",
    "                        counts[label] = 0\n",
    "                    counts[label] += 1\n",
    "                    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ambiguous': 291, 'not null': 26, 'range limitation': 19}\n",
      "{'ambiguous': 242, 'not null': 73, 'range limitation': 46, 'type restriction': 1}\n"
     ]
    }
   ],
   "source": [
    "print(get_label_counts_doccano_file('./labelled_data/new_parameter_full.jsonl'))\n",
    "print(get_label_counts_doccano_file('./labelled_data/new_exception_full.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "# calculate how many exception sentences have more than 1 label\n",
    "with open('./labelled_data/new_exception_full.jsonl') as f:\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        if len(obj['labels']) > 1:\n",
    "            c += 1\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291152\n"
     ]
    }
   ],
   "source": [
    "with open('./repos.txt') as f:\n",
    "    s = set()\n",
    "    for line in f:\n",
    "        s.add(line)\n",
    "        \n",
    "    print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
