{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['through', 'shouldn', 'couldn', 'has', 'at', 'too', 'couldn', 'while', 'for', 'any', 'aren', 'an', 're', 'in', 'up', 'whom', 'as', 'yours', 'out', 'do', 'mustn', 'needn', 'myself', 've', 'didn', 'had', 'further', 'other', 'these', 'hasn', 'shan', 'hadn', 'over', 'but', 'just', 'until', 'are', 'which', 'above', 'ma', 'will', 'weren', 'because', 'his', 'won', 'if', 'during', 'doing', 'they', 'so', 'haven', 'mightn', 'itself', 'them', 'yourself', 'have', 'of', 'yourselves', 'this', 'before', 'below', 'more', 'there', 'under', 'did', 'we', 'the', 'wasn', 'few', 'mightn', 'what', 'hasn', 'should', 'most', 'didn', 'ours', 'and', 'our', 'himself', 'she', 'been', 'you', 'once', 'was', 'why', 'against', 'weren', 'theirs', 'their', 'such', 'after', 'its', 'now', 'won', 'when', 'shouldn', 'wouldn', 'to', 'you', 'shan', 'll', 'doesn', 'me', 'him', 'herself', 'nor', 'very', 'her', 'between', 'with', 'be', 'here', 'should', 'down', 'were', 'hadn', 'into', 'on', 'can', 'you', 'themselves', 'no', 'aren', 'you', 'all', 'it', 'those', 'hers', 'or', 'where', 'some', 'being', 'ourselves', 'that', 'mustn', 'than', 'wasn', 'by', 'don', 'about', 'only', 'how', 'own', 'it', 'isn', 'isn', 'both', 'she', 'each', 'then', 'don', 'ain', 'your', 'is', 'not', 'my', 'same', 'am', 'needn', 'does', 'off', 'from', 'having', 'doesn', 'haven', 'he', 'that', 'you', 'wouldn', 'again', 'who']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ujson\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from itertools import repeat\n",
    "from scipy import spatial\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import functools\n",
    "import operator\n",
    "from random import sample\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = [s.split(\"'\")[0] for s in stopwords]\n",
    "custom_stopwords = [s for s in custom_stopwords if len(s) > 1]\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input paths\n",
    "tfidf_path = './output/tfidf_results_new.jsonl'\n",
    "w2v_path = './output/word2vec_results_new.jsonl'\n",
    "bm25_path = './output/bm25_results_new.jsonl'\n",
    "combo_path = './output/combined_sim_results_new.jsonl'\n",
    "\n",
    "# output paths\n",
    "tfidf_sample_path = './output/tfidf_sample.jsonl'\n",
    "w2v_sample_path = './output/w2v_sample.jsonl'\n",
    "bm25_sample_path = './output/bm25_sample.jsonl'\n",
    "combo_sample_path = './output/combo_sample_path.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove all code blocks\n",
    "    doc = re.sub(r'```([^```]*)```', '', doc)\n",
    "            \n",
    "    # remove urls\n",
    "    doc = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',             \n",
    "        '', doc)\n",
    "    \n",
    "    # remove line break characters\n",
    "    doc = re.sub(r'[\\r\\n]', ' ', doc)\n",
    "    \n",
    "    # remove apostrophes/suffixes\n",
    "    doc = re.sub(r\"'\\w |\\w' \", ' ', doc)\n",
    "    \n",
    "    # remove numbers\n",
    "    doc = re.sub(r'(\\d\\.?)+', ' ', doc)\n",
    "        \n",
    "    # replace all punctuation except for full stop with space\n",
    "    doc = re.sub(r'[^A-Za-z\\.]', ' ', doc)\n",
    "        \n",
    "    # normalise full stops\n",
    "    doc = re.sub(r'\\s\\.\\.+', '.', doc)\n",
    "    \n",
    "    # remove more than 1 whitespace\n",
    "    doc = re.sub('\\s\\s+', ' ', doc)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    docs = doc.split('. ')\n",
    "    docs = [t for t in docs if t != '']\n",
    "    return docs\n",
    "\n",
    "def doc_tokenize(doc):\n",
    "    doc = re.sub('\\.', ' ', doc) # remove full stops\n",
    "    \n",
    "    tokens = [t.lower() for t in doc.split() if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub('\\.', ' ', sentence) # remove full stops\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    tokens = [t.lower() for t in tokens if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 627450\n",
      "Number of comments: 1855870\n"
     ]
    }
   ],
   "source": [
    "# load the extracted java-related GitHub data\n",
    "def load_github_issues():\n",
    "    with open('./output/issue-comments-revised.jsonl') as issue_comments_f:\n",
    "        issue_list = []\n",
    "        comments_list = []\n",
    "\n",
    "        for line in issue_comments_f:\n",
    "            obj = ujson.loads(line)\n",
    "\n",
    "            comments_list.append({\n",
    "                'body': obj['body'],\n",
    "                'repo_name': obj['repo_name'],\n",
    "                'html_url': obj['html_url'],\n",
    "                'issue_id': obj['issue']['id']\n",
    "            })\n",
    "\n",
    "            issue = obj['issue']\n",
    "            issue['repo_name'] = obj['repo_name']\n",
    "            issue_list.append(issue)\n",
    "\n",
    "        issues_df = pd.DataFrame(issue_list)\n",
    "        issues_df = issues_df.drop_duplicates(subset=['id'])\n",
    "        comments_df = pd.DataFrame(comments_list)\n",
    "        \n",
    "        return (issues_df, comments_df)\n",
    "\n",
    "\n",
    "issues_df, comments_df = load_github_issues()\n",
    "print(\"Number of issues: {}\".format(len(issues_df.index)))\n",
    "print(\"Number of comments: {}\".format(len(comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_issues_df = issues_df.dropna(subset=['body'])\n",
    "code_issues_df = code_issues_df[code_issues_df['body'].str.contains('```')]\n",
    "block_comments_df = comments_df[comments_df['issue_id'].isin(code_issues_df['id'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed all comments...\n",
      "Tokenized all paragraphs...\n",
      "Completed sentence tokenization...\n",
      "Tokenized all sentences...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence_list(sent_list):\n",
    "    return [tokenize(x) for x in sent_list]\n",
    "\n",
    "def calculate_preprocessed_comment_sentences(df):\n",
    "    df['preprocessed_comments'] = df['body'].map(lambda x: preprocess(x))\n",
    "    print('Preprocessed all comments...')\n",
    "    \n",
    "    df['tokenised_para'] = df['preprocessed_comments'].map(lambda x: doc_tokenize(x))\n",
    "    print('Tokenized all paragraphs...')\n",
    "    \n",
    "    df['sentences'] = df['preprocessed_comments'].map(lambda x: sent_tokenize(x))\n",
    "    print('Completed sentence tokenization...')\n",
    "    \n",
    "    df['tokenised_sentences'] = df['sentences'].map(lambda x: tokenize_sentence_list(x))\n",
    "    print('Tokenized all sentences...')\n",
    "                            \n",
    "calculate_preprocessed_comment_sentences(block_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all comments that contain non-empty preprocessed sentences\n",
    "preprocessed_comments_df = block_comments_df[block_comments_df.astype(str)['tokenised_sentences'] != '[]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elasticsearch 5.4.1\\r\\nRollover API problem\\r\\...</td>\n",
       "      <td>https://github.com/elastic/elasticsearch/issue...</td>\n",
       "      <td>264716524</td>\n",
       "      <td>26976</td>\n",
       "      <td>elastic/elasticsearch</td>\n",
       "      <td>Alias [test-schema-active-logs] has more than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A snackbar shows and then gets hidden by the b...</td>\n",
       "      <td>https://github.com/ritvikkar/MovieFinder/issue...</td>\n",
       "      <td>285146015</td>\n",
       "      <td>43</td>\n",
       "      <td>ritvikkar/MovieFinder</td>\n",
       "      <td>We need to make offline mode look better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/jooby-project/jooby/blob/ma...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/965</td>\n",
       "      <td>284439800</td>\n",
       "      <td>965</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>2nd thymeleaf code snippet (in the documentati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When updating Spotless from `3.6.0` to `3.7.0`...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/182</td>\n",
       "      <td>285279535</td>\n",
       "      <td>182</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Unable to store input properties... when upgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://pastebin.com/bT3JGcjN</td>\n",
       "      <td>https://github.com/Alex-the-666/Ice_and_Fire/i...</td>\n",
       "      <td>258569936</td>\n",
       "      <td>321</td>\n",
       "      <td>Alex-the-666/Ice_and_Fire</td>\n",
       "      <td>Hippogryph loot table might be broken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  Elasticsearch 5.4.1\\r\\nRollover API problem\\r\\...   \n",
       "1  A snackbar shows and then gets hidden by the b...   \n",
       "2  https://github.com/jooby-project/jooby/blob/ma...   \n",
       "3  When updating Spotless from `3.6.0` to `3.7.0`...   \n",
       "4                      https://pastebin.com/bT3JGcjN   \n",
       "\n",
       "                                            html_url         id  number  \\\n",
       "0  https://github.com/elastic/elasticsearch/issue...  264716524   26976   \n",
       "1  https://github.com/ritvikkar/MovieFinder/issue...  285146015      43   \n",
       "2  https://github.com/jooby-project/jooby/issues/965  284439800     965   \n",
       "3    https://github.com/diffplug/spotless/issues/182  285279535     182   \n",
       "4  https://github.com/Alex-the-666/Ice_and_Fire/i...  258569936     321   \n",
       "\n",
       "                   repo_name  \\\n",
       "0      elastic/elasticsearch   \n",
       "1      ritvikkar/MovieFinder   \n",
       "2        jooby-project/jooby   \n",
       "3          diffplug/spotless   \n",
       "4  Alex-the-666/Ice_and_Fire   \n",
       "\n",
       "                                               title  \n",
       "0  Alias [test-schema-active-logs] has more than ...  \n",
       "1           We need to make offline mode look better  \n",
       "2  2nd thymeleaf code snippet (in the documentati...  \n",
       "3  Unable to store input properties... when upgra...  \n",
       "4              Hippogryph loot table might be broken  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>preprocessed_comments</th>\n",
       "      <th>tokenised_para</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just an example to see how to use the  `thymel...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/...</td>\n",
       "      <td>284439800</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>Just an example to see how to use the thymelea...</td>\n",
       "      <td>[example, see, use, thymeleaf, api, need, test...</td>\n",
       "      <td>[Just an example to see how to use the thymele...</td>\n",
       "      <td>[[example, see, use, thymeleaf, api, need, tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>nedtwigg Would it be practical for you to upgr...</td>\n",
       "      <td>[nedtwigg, would, practical, upgrade, propriet...</td>\n",
       "      <td>[nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>[[nedtwigg, would, practical, upgrade, proprie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nope - 4.4.1 brings a different, unrelated pro...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Nope brings a different unrelated problem</td>\n",
       "      <td>[nope, brings, different, unrelated, problem]</td>\n",
       "      <td>[Nope brings a different unrelated problem]</td>\n",
       "      <td>[[nope, brings, different, unrelated, problem]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>`Objects.requireNonNull` should also be suppor...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47#iss...</td>\n",
       "      <td>268767363</td>\n",
       "      <td>uber/NullAway</td>\n",
       "      <td>Objects.requireNonNull should also be supported.</td>\n",
       "      <td>[objects, requirenonnull, also, supported]</td>\n",
       "      <td>[Objects.requireNonNull should also be support...</td>\n",
       "      <td>[[objects, requirenonnull, also, supported]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Darn, that's a shame. :&lt;\\r\\n\\r\\nNothing comes ...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Darn that a shame. Nothing comes to mind yet b...</td>\n",
       "      <td>[darn, shame, nothing, comes, mind, yet, might...</td>\n",
       "      <td>[Darn that a shame, Nothing comes to mind yet ...</td>\n",
       "      <td>[[darn, shame], [nothing, comes, mind, yet, mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   Just an example to see how to use the  `thymel...   \n",
       "3   @nedtwigg Would it be practical for you to upg...   \n",
       "7   Nope - 4.4.1 brings a different, unrelated pro...   \n",
       "10  `Objects.requireNonNull` should also be suppor...   \n",
       "14  Darn, that's a shame. :<\\r\\n\\r\\nNothing comes ...   \n",
       "\n",
       "                                             html_url   issue_id  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/...  284439800   \n",
       "3   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "7   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "10  https://github.com/uber/NullAway/issues/47#iss...  268767363   \n",
       "14  https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "\n",
       "              repo_name                              preprocessed_comments  \\\n",
       "2   jooby-project/jooby  Just an example to see how to use the thymelea...   \n",
       "3     diffplug/spotless  nedtwigg Would it be practical for you to upgr...   \n",
       "7     diffplug/spotless          Nope brings a different unrelated problem   \n",
       "10        uber/NullAway   Objects.requireNonNull should also be supported.   \n",
       "14    diffplug/spotless  Darn that a shame. Nothing comes to mind yet b...   \n",
       "\n",
       "                                       tokenised_para  \\\n",
       "2   [example, see, use, thymeleaf, api, need, test...   \n",
       "3   [nedtwigg, would, practical, upgrade, propriet...   \n",
       "7       [nope, brings, different, unrelated, problem]   \n",
       "10         [objects, requirenonnull, also, supported]   \n",
       "14  [darn, shame, nothing, comes, mind, yet, might...   \n",
       "\n",
       "                                            sentences  \\\n",
       "2   [Just an example to see how to use the thymele...   \n",
       "3   [nedtwigg Would it be practical for you to upg...   \n",
       "7         [Nope brings a different unrelated problem]   \n",
       "10  [Objects.requireNonNull should also be support...   \n",
       "14  [Darn that a shame, Nothing comes to mind yet ...   \n",
       "\n",
       "                                  tokenised_sentences  \n",
       "2   [[example, see, use, thymeleaf, api, need, tes...  \n",
       "3   [[nedtwigg, would, practical, upgrade, proprie...  \n",
       "7     [[nope, brings, different, unrelated, problem]]  \n",
       "10       [[objects, requirenonnull, also, supported]]  \n",
       "14  [[darn, shame], [nothing, comes, mind, yet, mi...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_index_to_df_index = {}\n",
    "with open('./output/associated-sentence-df-index-new.txt') as f:\n",
    "    indices = f.readlines()\n",
    "    indices = [int(x) for x in indices]\n",
    "\n",
    "    for index, df_index in enumerate(indices):\n",
    "        sentence_index_to_df_index[index] = df_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 19101\n",
      "Total number of scores: 2430521\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "with open(tfidf_path) as f, open(tfidf_sample_path, 'w+') as f_out:\n",
    "    results = []\n",
    "    c = 0\n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        obj['tfidf_sim_scores'] = \\\n",
    "            sorted(obj['tfidf_sim_scores'], key=lambda x: x['score'], reverse=True)\n",
    "        c += len(obj['tfidf_sim_scores'])\n",
    "        obj['tfidf_sim_scores'] = obj['tfidf_sim_scores'][:3] # limit to 3 results per caveat\n",
    "        results.append(obj)\n",
    "\n",
    "    print('Number of results: {}'.format(len(results)))\n",
    "    print('Total number of scores: {}'.format(c))\n",
    "    to_label = sample(results, 384)\n",
    "    \n",
    "    out_str = ''\n",
    "    for obj in to_label:\n",
    "        for res in obj['tfidf_sim_scores']:\n",
    "            row = preprocessed_comments_df.loc[sentence_index_to_df_index[res['sentence_index']], :]\n",
    "            out_str += ujson.dumps({\n",
    "                'score' : res['score'],\n",
    "                'caveat': obj['caveat_sentence'],\n",
    "                'api': obj['api'],\n",
    "                'class': obj['simple_class_name'],\n",
    "                'comment': row['body'],\n",
    "                'html_url': row['html_url']\n",
    "            }) + '\\n'\n",
    "            \n",
    "    f_out.write(out_str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 2000\n",
      "Total number of scores: 433237\n"
     ]
    }
   ],
   "source": [
    "# W2V\n",
    "with open(w2v_path) as f, open(w2v_sample_path, 'w+') as f_out:\n",
    "    results = []\n",
    "    out_str = ''\n",
    "    c = 0\n",
    "    \n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        obj['w2v_results'] = [x for x in obj['w2v_results'] if x['score'] != 'nan']\n",
    "        c += len(obj['w2v_results'])\n",
    "        \n",
    "        for score in obj['w2v_results']:\n",
    "            score['score'] = float(score['score'])\n",
    "        \n",
    "        obj['w2v_results'] = sorted(obj['w2v_results'], key=lambda x: x['score'], reverse=True)\n",
    "        obj['w2v_results'] = obj['w2v_results'][:3] # limit to 3 results per caveat\n",
    "        results.append(obj)\n",
    "    \n",
    "    print('Number of results: {}'.format(len(results)))\n",
    "    print('Total number of scores: {}'.format(c))\n",
    "    to_label = sample(results, 384)\n",
    "    \n",
    "    for obj in to_label:\n",
    "        for res in obj['w2v_results']:\n",
    "            issue = issues_df[issues_df['id'] == res['issue_number']]\n",
    "            out_str += ujson.dumps({\n",
    "                'score': res['score'],\n",
    "                'comment': res['comment'],\n",
    "                'class': obj['simple_class_name'],\n",
    "                'api': obj['api'],\n",
    "                'caveat': obj['caveat_sentence'],\n",
    "                'html_url': issue.iloc[0]['html_url']\n",
    "            }) + '\\n'\n",
    "            \n",
    "    f_out.write(out_str.strip())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 2000\n",
      "Total number of scores: 677202\n"
     ]
    }
   ],
   "source": [
    "# BM25\n",
    "with open(bm25_path) as f, open(bm25_sample_path, 'w+') as f_out:\n",
    "    results = []\n",
    "    out_str = ''\n",
    "    c = 0\n",
    "\n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        obj['bm25_results'] = [x for x in obj['bm25_results'] if x['score'] != 'nan']\n",
    "        c += len(obj['bm25_results'])\n",
    "        \n",
    "        for score in obj['bm25_results']:\n",
    "            score['score'] = float(score['score'])\n",
    "        obj['bm25_results'] = sorted(obj['bm25_results'], key=lambda x: x['score'], reverse=True)\n",
    "        obj['bm25_results'] = obj['bm25_results'][:3] # limit to 3 results per caveat\n",
    "        results.append(obj)\n",
    "    \n",
    "    print('Number of results: {}'.format(len(results)))\n",
    "    print('Total number of scores: {}'.format(c))\n",
    "    to_label = sample(results, 384)\n",
    "    \n",
    "    for obj in to_label:\n",
    "        for res in obj['bm25_results']:\n",
    "            issue = issues_df[issues_df['id'] == res['issue_number']]\n",
    "            out_str += ujson.dumps({\n",
    "                'score': res['score'],\n",
    "                'comment': res['comment'],\n",
    "                'class': obj['simple_class_name'],\n",
    "                'api': obj['api'],\n",
    "                'caveat': obj['caveat_sentence'],\n",
    "                'html_url': issue.iloc[0]['html_url']\n",
    "            }) + '\\n'\n",
    "            \n",
    "    f_out.write(out_str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 2000\n",
      "Total number of scores: 681187\n"
     ]
    }
   ],
   "source": [
    "# W2V + BM25 combination\n",
    "with open(combo_path) as f, open(combo_sample_path, 'w+') as f_out:\n",
    "    results = []\n",
    "    out_str = ''\n",
    "    c = 0\n",
    "\n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        obj['combination_results'] = [x for x in obj['combination_results'] if x['score'] != 'nan']\n",
    "        c += len(obj['combination_results'])   \n",
    "    \n",
    "        for score in obj['combination_results']:\n",
    "            score['score'] = float(score['score'])\n",
    "            \n",
    "        obj['combination_results'] = \\\n",
    "            sorted(obj['combination_results'], key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "        obj['combination_results'] = obj['combination_results'][:3] # limit to 3 results per caveat\n",
    "        results.append(obj)\n",
    "    \n",
    "    print('Number of results: {}'.format(len(results)))\n",
    "    print('Total number of scores: {}'.format(c))\n",
    "    to_label = sample(results, 384)\n",
    "    \n",
    "    for obj in to_label:\n",
    "        for res in obj['combination_results']:\n",
    "            issue = issues_df[issues_df['id'] == res['issue_number']]\n",
    "            out_str += ujson.dumps({\n",
    "                'score': res['score'],\n",
    "                'comment': res['comment'],\n",
    "                'class': obj['simple_class_name'],\n",
    "                'api': obj['api'],\n",
    "                'caveat': obj['caveat_sentence'],\n",
    "                'html_url': issue.iloc[0]['html_url']\n",
    "            }) + '\\n'\n",
    "            \n",
    "    f_out.write(out_str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to doccano format for labelling\n",
    "def convert_to_doccano(path, output_path):\n",
    "    with open(path) as f, open(output_path, 'w+') as f_out:\n",
    "        for line in f:\n",
    "            obj = ujson.loads(line)\n",
    "            obj['labels'] = ['not-relevant']\n",
    "            obj['text'] = 'Class: ' + obj['class'] + '\\nAPI: ' + obj['api'] + '\\n--------\\nCaveat: ' \\\n",
    "                + obj['caveat'] + '\\n--------\\nComment: ' + re.sub(r'```([^```]*)```', '', obj['comment'])\n",
    "            f_out.write(ujson.dumps(obj) + '\\n')\n",
    "\n",
    "convert_to_doccano(tfidf_sample_path, './output/tfidf_to_label.jsonl')\n",
    "convert_to_doccano(w2v_sample_path, './output/w2v_to_label.jsonl')\n",
    "convert_to_doccano(bm25_sample_path, './output/bm25_to_label.jsonl')\n",
    "convert_to_doccano(combo_sample_path, './output/combo_to_label.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_modified_labelled_doccano(labelled_path, metadata_path, original_path, output_path):\n",
    "    with open(labelled_path) as f, open(original_path) as f2, open(metadata_path) as f3:\n",
    "        labelled_objs = []\n",
    "        label_id_to_name = {}\n",
    "        \n",
    "        metadata = ujson.load(f3)\n",
    "        for obj in metadata:\n",
    "            label_id_to_name[obj['id']] = obj['text']\n",
    "        \n",
    "        for line in f:\n",
    "            labelled_objs.append(ujson.loads(line))\n",
    "        \n",
    "        with open(output_path, 'w+') as f_out:\n",
    "            for i, line in enumerate(f2):\n",
    "                obj = ujson.loads(line)\n",
    "                del obj['labels']\n",
    "                obj['label'] = label_id_to_name[labelled_objs[i]['annotations'][0]['label']]\n",
    "                \n",
    "                f_out.write(ujson.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_modified_labelled_doccano('./output/labelled_combo.jsonl',\n",
    "                               './output/combo_metadata.json',\n",
    "                               './output/combo_to_label.jsonl',\n",
    "                               './labelled_data/combo.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
