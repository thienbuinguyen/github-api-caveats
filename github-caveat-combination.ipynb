{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ve', 'yours', 'or', 'until', 'hasn', 'wouldn', 'after', 'if', 'he', 'into', 'shouldn', 'as', 'will', 'before', 'wouldn', 'than', 'aren', 'on', 'few', 'whom', 'doing', 'your', 'but', 'weren', 'our', 'such', 'you', 'won', 'not', 'having', 'yourselves', 'shan', 'of', 'through', 'ourselves', 'those', 'can', 'very', 'down', 'out', 'most', 'ain', 'what', 'myself', 'mightn', 'once', 'so', 'more', 'does', 'while', 'needn', 'them', 'him', 'its', 'against', 'the', 'when', 'should', 'was', 'about', 'isn', 'hadn', 'she', 'this', 'same', 'in', 'for', 'yourself', 'll', 'where', 'did', 'again', 'any', 'their', 'my', 'then', 'me', 'his', 'itself', 'there', 'no', 'from', 'hadn', 'needn', 'nor', 'you', 'all', 'theirs', 'shan', 'why', 'doesn', 'who', 'over', 'how', 'ours', 'both', 'up', 'mustn', 'off', 'she', 'above', 'wasn', 'were', 'haven', 'an', 'weren', 'being', 'below', 'don', 'himself', 'they', 'you', 'that', 'had', 'own', 'you', 'isn', 'other', 'with', 'be', 'mustn', 'is', 'been', 'just', 'won', 'that', 'some', 'now', 'under', 'herself', 'didn', 'aren', 'hasn', 'which', 'doesn', 'further', 'at', 'ma', 'has', 'didn', 'to', 'couldn', 'we', 'these', 'here', 'during', 'it', 'it', 'and', 'don', 'you', 'am', 'her', 'are', 'hers', 'should', 'do', 'wasn', 'because', 'between', 're', 'have', 'by', 'couldn', 'shouldn', 'only', 'mightn', 'each', 'haven', 'too', 'themselves']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ujson\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from itertools import repeat\n",
    "from scipy import spatial\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import functools\n",
    "import operator\n",
    "from random import sample\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = [s.split(\"'\")[0] for s in stopwords]\n",
    "custom_stopwords = [s for s in custom_stopwords if len(s) > 1]\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " \"aren't\",\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " \"didn't\",\n",
       " \"doesn't\",\n",
       " \"don't\",\n",
       " \"hadn't\",\n",
       " \"hasn't\",\n",
       " \"haven't\",\n",
       " 'i',\n",
       " \"isn't\",\n",
       " \"it's\",\n",
       " 'm',\n",
       " \"mightn't\",\n",
       " \"mustn't\",\n",
       " \"needn't\",\n",
       " 'o',\n",
       " 's',\n",
       " \"shan't\",\n",
       " \"she's\",\n",
       " \"should've\",\n",
       " \"shouldn't\",\n",
       " 't',\n",
       " \"that'll\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(stopwords)\n",
    "b = set(custom_stopwords)\n",
    "a.difference(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 627450\n",
      "Number of comments: 1855870\n"
     ]
    }
   ],
   "source": [
    "# load the extracted java-related GitHub data\n",
    "def load_github_issues():\n",
    "    with open('./output/issue-comments-revised.jsonl') as issue_comments_f:\n",
    "        issue_list = []\n",
    "        comments_list = []\n",
    "\n",
    "        for line in issue_comments_f:\n",
    "            obj = ujson.loads(line)\n",
    "\n",
    "            comments_list.append({\n",
    "                'body': obj['body'],\n",
    "                'repo_name': obj['repo_name'],\n",
    "                'html_url': obj['html_url'],\n",
    "                'issue_id': obj['issue']['id']\n",
    "            })\n",
    "\n",
    "            issue = obj['issue']\n",
    "            issue['repo_name'] = obj['repo_name']\n",
    "            issue_list.append(issue)\n",
    "\n",
    "        issues_df = pd.DataFrame(issue_list)\n",
    "        issues_df = issues_df.drop_duplicates(subset=['id'])\n",
    "        comments_df = pd.DataFrame(comments_list)\n",
    "        \n",
    "        return (issues_df, comments_df)\n",
    "\n",
    "\n",
    "issues_df, comments_df = load_github_issues()\n",
    "print(\"Number of issues: {}\".format(len(issues_df.index)))\n",
    "print(\"Number of comments: {}\".format(len(comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues with code block/s: 85318\n"
     ]
    }
   ],
   "source": [
    "# find the issues that contain code blocks\n",
    "code_issues_df = issues_df.dropna(subset=['body'])\n",
    "code_issues_df = code_issues_df[code_issues_df['body'].str.contains('```')]\n",
    "print(\"Number of issues with code block/s: {}\".format(len(code_issues_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/jooby-project/jooby/blob/ma...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/965</td>\n",
       "      <td>284439800</td>\n",
       "      <td>965</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>2nd thymeleaf code snippet (in the documentati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When updating Spotless from `3.6.0` to `3.7.0`...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/182</td>\n",
       "      <td>285279535</td>\n",
       "      <td>182</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Unable to store input properties... when upgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I'm using Google Guava [Preconditions](https:/...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47</td>\n",
       "      <td>268767363</td>\n",
       "      <td>47</td>\n",
       "      <td>uber/NullAway</td>\n",
       "      <td>NullAway doesn't recognize Guava Preconditions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&gt; Citing added javadoc:\\r\\n\\r\\nImmutables appl...</td>\n",
       "      <td>https://github.com/immutables/immutables/issue...</td>\n",
       "      <td>285281793</td>\n",
       "      <td>740</td>\n",
       "      <td>immutables/immutables</td>\n",
       "      <td>Style-level fence for annotation classpath aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Hi, i try to build ffmpeg for windows 32 bit \\...</td>\n",
       "      <td>https://github.com/bytedeco/javacpp-presets/is...</td>\n",
       "      <td>285279281</td>\n",
       "      <td>503</td>\n",
       "      <td>bytedeco/javacpp-presets</td>\n",
       "      <td>Build for windows-x86 not working</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   https://github.com/jooby-project/jooby/blob/ma...   \n",
       "3   When updating Spotless from `3.6.0` to `3.7.0`...   \n",
       "10  I'm using Google Guava [Preconditions](https:/...   \n",
       "19  > Citing added javadoc:\\r\\n\\r\\nImmutables appl...   \n",
       "33  Hi, i try to build ffmpeg for windows 32 bit \\...   \n",
       "\n",
       "                                             html_url         id  number  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/965  284439800     965   \n",
       "3     https://github.com/diffplug/spotless/issues/182  285279535     182   \n",
       "10         https://github.com/uber/NullAway/issues/47  268767363      47   \n",
       "19  https://github.com/immutables/immutables/issue...  285281793     740   \n",
       "33  https://github.com/bytedeco/javacpp-presets/is...  285279281     503   \n",
       "\n",
       "                   repo_name  \\\n",
       "2        jooby-project/jooby   \n",
       "3          diffplug/spotless   \n",
       "10             uber/NullAway   \n",
       "19     immutables/immutables   \n",
       "33  bytedeco/javacpp-presets   \n",
       "\n",
       "                                                title  \n",
       "2   2nd thymeleaf code snippet (in the documentati...  \n",
       "3   Unable to store input properties... when upgra...  \n",
       "10  NullAway doesn't recognize Guava Preconditions...  \n",
       "19  Style-level fence for annotation classpath aut...  \n",
       "33                  Build for windows-x86 not working  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments for issues with code block/s: 290019\n"
     ]
    }
   ],
   "source": [
    "block_comments_df = comments_df[comments_df['issue_id'].isin(code_issues_df['id'])].copy()\n",
    "print('Number of comments for issues with code block/s: {}'.format(len(block_comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove all code blocks\n",
    "    doc = re.sub(r'```([^```]*)```', '', doc)\n",
    "            \n",
    "    # remove urls\n",
    "    doc = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',             \n",
    "        '', doc)\n",
    "    \n",
    "    # remove line break characters\n",
    "    doc = re.sub(r'[\\r\\n]', ' ', doc)\n",
    "    \n",
    "    # remove apostrophes/suffixes\n",
    "    doc = re.sub(r\"'\\w |\\w' \", ' ', doc)\n",
    "    \n",
    "    # remove numbers\n",
    "    doc = re.sub(r'(\\d\\.?)+', ' ', doc)\n",
    "        \n",
    "    # replace all punctuation except for full stop with space\n",
    "    doc = re.sub(r'[^A-Za-z\\.]', ' ', doc)\n",
    "        \n",
    "    # normalise full stops\n",
    "    doc = re.sub(r'\\s\\.\\.+', '.', doc)\n",
    "    \n",
    "    # remove more than 1 whitespace\n",
    "    doc = re.sub('\\s\\s+', ' ', doc)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    docs = doc.split('. ')\n",
    "    docs = [t for t in docs if t != '']\n",
    "    return docs\n",
    "\n",
    "def doc_tokenize(doc):\n",
    "    doc = re.sub('\\.', ' ', doc) # remove full stops\n",
    "    \n",
    "    tokens = [t.lower() for t in doc.split() if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub('\\.', ' ', sentence) # remove full stops\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    tokens = [t.lower() for t in tokens if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caveat sentences: 73831\n"
     ]
    }
   ],
   "source": [
    "# load the non-dreprecated java doc caveat sentences\n",
    "def load_caveats():\n",
    "    caveat_files_dir = './output/java_12_spec_caveat_sentences_revised/'\n",
    "    caveats_list = []\n",
    "\n",
    "    files = glob.glob(caveat_files_dir + '*.json')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            arr = ujson.load(f)\n",
    "            full_class_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            simple_class_name = full_class_name.split('.')[-1]\n",
    "            for caveat in arr:\n",
    "                if not caveat['deprecated'] and 'name' in caveat:\n",
    "                    for sentence in caveat['sentences']:\n",
    "                        caveats_list.append({\n",
    "                            'simple_class_name': simple_class_name,\n",
    "                            'full_class_name': full_class_name,\n",
    "                            'api': caveat['name'],\n",
    "                            'sentence': sentence,\n",
    "                            'type': 'body'\n",
    "                        })\n",
    "\n",
    "                    # add all misc level sentences\n",
    "                    for misc_obj in caveat['caveat_misc']:\n",
    "                        if misc_obj['name'] in ['Parameters:', 'Throws:']:\n",
    "                            for obj in misc_obj['list']:\n",
    "                                for misc_sentence in obj['sentences']:\n",
    "                                    sentence = ''\n",
    "                                    if misc_obj['name'] == 'Parameters:':\n",
    "                                        sentence = obj['parameter'] + ' ' + misc_sentence\n",
    "                                    else:\n",
    "                                        sentence = obj['exception'] + ' ' + misc_sentence\n",
    "                                    caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': sentence,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "                        else:\n",
    "                            for s in misc_obj['list']:\n",
    "                                caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': s,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "    return pd.DataFrame(caveats_list)\n",
    "\n",
    "caveats = load_caveats()\n",
    "print('Number of caveat sentences: {}'.format(len(caveats.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "caveats['preprocessed'] = caveats['sentence'].apply(lambda x: preprocess(x))\n",
    "caveats['tokens'] = caveats['preprocessed'].map(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api</th>\n",
       "      <th>full_class_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>simple_class_name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This statistic is reset when the thread conten...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>the approximate accumulated elapsed time in mi...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>UnsupportedOperationException if the Java virt...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getWaitedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              api                  full_class_name  \\\n",
       "0  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "1  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "2  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "3  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "4   getWaitedTime  java.lang.management.ThreadInfo   \n",
       "\n",
       "                                            sentence simple_class_name  type  \n",
       "0  This method returns -1 if thread contention mo...        ThreadInfo  body  \n",
       "1  This statistic is reset when the thread conten...        ThreadInfo  body  \n",
       "2  the approximate accumulated elapsed time in mi...        ThreadInfo  misc  \n",
       "3  UnsupportedOperationException if the Java virt...        ThreadInfo  misc  \n",
       "4  This method returns -1 if thread contention mo...        ThreadInfo  body  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caveats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just an example to see how to use the  `thymel...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/...</td>\n",
       "      <td>284439800</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nope - 4.4.1 brings a different, unrelated pro...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>`Objects.requireNonNull` should also be suppor...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47#iss...</td>\n",
       "      <td>268767363</td>\n",
       "      <td>uber/NullAway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Darn, that's a shame. :&lt;\\r\\n\\r\\nNothing comes ...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   Just an example to see how to use the  `thymel...   \n",
       "3   @nedtwigg Would it be practical for you to upg...   \n",
       "7   Nope - 4.4.1 brings a different, unrelated pro...   \n",
       "10  `Objects.requireNonNull` should also be suppor...   \n",
       "14  Darn, that's a shame. :<\\r\\n\\r\\nNothing comes ...   \n",
       "\n",
       "                                             html_url   issue_id  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/...  284439800   \n",
       "3   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "7   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "10  https://github.com/uber/NullAway/issues/47#iss...  268767363   \n",
       "14  https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "\n",
       "              repo_name  \n",
       "2   jooby-project/jooby  \n",
       "3     diffplug/spotless  \n",
       "7     diffplug/spotless  \n",
       "10        uber/NullAway  \n",
       "14    diffplug/spotless  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed all comments...\n",
      "Tokenized all paragraphs...\n",
      "Completed sentence tokenization...\n",
      "Tokenized all sentences...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence_list(sent_list):\n",
    "    return [tokenize(x) for x in sent_list]\n",
    "\n",
    "def calculate_preprocessed_comment_sentences(df):\n",
    "    df['preprocessed_comments'] = df['body'].map(lambda x: preprocess(x))\n",
    "    print('Preprocessed all comments...')\n",
    "    \n",
    "    df['tokenised_para'] = df['preprocessed_comments'].map(lambda x: doc_tokenize(x))\n",
    "    print('Tokenized all paragraphs...')\n",
    "    \n",
    "    df['sentences'] = df['preprocessed_comments'].map(lambda x: sent_tokenize(x))\n",
    "    print('Completed sentence tokenization...')\n",
    "    \n",
    "    df['tokenised_sentences'] = df['sentences'].map(lambda x: tokenize_sentence_list(x))\n",
    "    print('Tokenized all sentences...')\n",
    "\n",
    "\n",
    "calculate_preprocessed_comment_sentences(block_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>preprocessed_comments</th>\n",
       "      <th>tokenised_para</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just an example to see how to use the  `thymel...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/...</td>\n",
       "      <td>284439800</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>Just an example to see how to use the thymelea...</td>\n",
       "      <td>[example, see, use, thymeleaf, api, need, test...</td>\n",
       "      <td>[Just an example to see how to use the thymele...</td>\n",
       "      <td>[[example, see, use, thymeleaf, api, need, tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>nedtwigg Would it be practical for you to upgr...</td>\n",
       "      <td>[nedtwigg, would, practical, upgrade, propriet...</td>\n",
       "      <td>[nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>[[nedtwigg, would, practical, upgrade, proprie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nope - 4.4.1 brings a different, unrelated pro...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Nope brings a different unrelated problem</td>\n",
       "      <td>[nope, brings, different, unrelated, problem]</td>\n",
       "      <td>[Nope brings a different unrelated problem]</td>\n",
       "      <td>[[nope, brings, different, unrelated, problem]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>`Objects.requireNonNull` should also be suppor...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47#iss...</td>\n",
       "      <td>268767363</td>\n",
       "      <td>uber/NullAway</td>\n",
       "      <td>Objects.requireNonNull should also be supported.</td>\n",
       "      <td>[objects, requirenonnull, also, supported]</td>\n",
       "      <td>[Objects.requireNonNull should also be support...</td>\n",
       "      <td>[[objects, requirenonnull, also, supported]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Darn, that's a shame. :&lt;\\r\\n\\r\\nNothing comes ...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Darn that a shame. Nothing comes to mind yet b...</td>\n",
       "      <td>[darn, shame, nothing, comes, mind, yet, might...</td>\n",
       "      <td>[Darn that a shame, Nothing comes to mind yet ...</td>\n",
       "      <td>[[darn, shame], [nothing, comes, mind, yet, mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   Just an example to see how to use the  `thymel...   \n",
       "3   @nedtwigg Would it be practical for you to upg...   \n",
       "7   Nope - 4.4.1 brings a different, unrelated pro...   \n",
       "10  `Objects.requireNonNull` should also be suppor...   \n",
       "14  Darn, that's a shame. :<\\r\\n\\r\\nNothing comes ...   \n",
       "\n",
       "                                             html_url   issue_id  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/...  284439800   \n",
       "3   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "7   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "10  https://github.com/uber/NullAway/issues/47#iss...  268767363   \n",
       "14  https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "\n",
       "              repo_name                              preprocessed_comments  \\\n",
       "2   jooby-project/jooby  Just an example to see how to use the thymelea...   \n",
       "3     diffplug/spotless  nedtwigg Would it be practical for you to upgr...   \n",
       "7     diffplug/spotless          Nope brings a different unrelated problem   \n",
       "10        uber/NullAway   Objects.requireNonNull should also be supported.   \n",
       "14    diffplug/spotless  Darn that a shame. Nothing comes to mind yet b...   \n",
       "\n",
       "                                       tokenised_para  \\\n",
       "2   [example, see, use, thymeleaf, api, need, test...   \n",
       "3   [nedtwigg, would, practical, upgrade, propriet...   \n",
       "7       [nope, brings, different, unrelated, problem]   \n",
       "10         [objects, requirenonnull, also, supported]   \n",
       "14  [darn, shame, nothing, comes, mind, yet, might...   \n",
       "\n",
       "                                            sentences  \\\n",
       "2   [Just an example to see how to use the thymele...   \n",
       "3   [nedtwigg Would it be practical for you to upg...   \n",
       "7         [Nope brings a different unrelated problem]   \n",
       "10  [Objects.requireNonNull should also be support...   \n",
       "14  [Darn that a shame, Nothing comes to mind yet ...   \n",
       "\n",
       "                                  tokenised_sentences  \n",
       "2   [[example, see, use, thymeleaf, api, need, tes...  \n",
       "3   [[nedtwigg, would, practical, upgrade, proprie...  \n",
       "7     [[nope, brings, different, unrelated, problem]]  \n",
       "10       [[objects, requirenonnull, also, supported]]  \n",
       "14  [[darn, shame], [nothing, comes, mind, yet, mi...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Having looked into this issue some more I think my original approach is the only one that works and turning off class signature merging when it either not needed or deliberately excluded is the only sensible way to do this barring some sort of mapping being created for formal type parameters in the base interfaces. The purpose behind merging the signatures in the first place was to allow decompilation to succeed there was no need for the signatures to make sense in the traditional way since they were only used in the direct context of the class and then only by the decompiler in order to compute formal types for otherwise generic methods. The issue basically boils down to there being no intrinsic intent or purpose associated with a particular formal type parameter and though it may be easy for a human to understand the relationships the computer doesn have the necessary contextual information to make informed decisions. Consider a class Foo V which implements Callable V . A Mixin is added which implements Comparable T and the relevant compareTo T other is added by the mixin. The new signature of the class after merging is Foo V T Callable V Comparable T . This is is the situation which breaks Kotlin though it provides a very useful context for the decompiler. The formal parameters V and T can be merged because they have the same bound however we don know that this merging is sane it not in this case but it might be in others . This means that there are two options Indicate in some way which formal type parameters can be merged this preserves both use cases but puts a burden on the developer to decorage all situations where merging is possible and risk things breaking in situations where the mappings get out of sync. Only enable signature merging when the decompiler is active this is more robust and possibly saves some wasted effort as well in situations where the signature merging is redundant.\n",
      "I originally thought this issue was specific to but it is not. The problem occurs in but does not log in dotcms.log file. It logs in catalina.out. There was some discussion about this issue in Slack. After some debugging I can safely say that as the stack indicates this is happening as the Felix framework is starting bundles not when it is stopping them. It seems that something is not in the proper state and when they validate the state as they are dispatching an event to it it throws an exception. This only happens when deploying and undeploying fragments. Other plugins do not show this behavior. For me these errors were not sporadic they happened every time with fragment plugins. Conditions from EventDispatcher breakpoint on call to invokeServiceListenerCallback bundle.toString .compareTo org.apache.felix.framework filter.toString .compareTo objectClass javax.servlet.http.HttpServlet http.felix.dispatcher event.toString .compareTo org.osgi.framework.ServiceEvent source javax.servlet.http.HttpServlet\n",
      "Hello as you mentioned above Picards assumes records sorted lexicographically so records will be sorted using String.compareTo comparator. However samtools sorts records in such way Records that has any number starting from current position are less than records that has letter at this position. If there are records which both have numbers starting from current positions they will be compared by these number values. image If records both have letters instead of numbers at the current position they will be compared lexicographically but only until next numbers. image image In order to support such sorting SAMRecordQueryNameComparator should be rewritten. So does picard need this type of sorting\n",
      "The thing that I reacted to the most was that you re returning a UnitConverter as a response to a getConverter method on a Prefix object. I would expect a PrefixConverter Maybe this is an indication that the naming for the UnitConverter class is wrong. I checked its definition and it doesn seem to be related to Unit at all. While we re at it I had wished there was a compareTo method on the Unit Converter class so I could figure which Converter Unit Prefix was larger e.g. while trying to implement addition .\n",
      "This is a bug in OriginTrackedPropertiesLoader . If I add withspace with to the test properties.properties used by OriginTrackedPropertiesLoaderTests then compareToJavaProperties fails.\n",
      "Hi I am facing the same issue. Below is my code. AutoValue public abstract class PaymentHistoryViewModelNew implements Parcelable public abstract String paidAmount public abstract String numberOfPayments public abstract List PaymentItemViewModelNew paymentViewModels public static Builder builder return new AutoValue PaymentHistoryViewModelNew.Builder public static PaymentHistoryViewModelNew from PaymentHistoryResponse paymentHistoryResponse String paidAmount List PaymentItemViewModelNew list new ArrayList if paymentHistoryResponse.payments null Collections.sort paymentHistoryResponse.payments new Comparator Payment Override public int compare Payment thisPayment Payment otherPayment Date thisPaymentDate DateHelper.parseDate thisPayment.date DateHelper.DATE API FORMAT Date otherPaymentDate DateHelper.parseDate otherPayment.date DateHelper.DATE API FORMAT if thisPaymentDate null otherPaymentDate null return otherPaymentDate.compareTo thisPaymentDate return for Payment payment paymentHistoryResponse.payments list.add PaymentItemViewModelNew.builder .build .from payment return builder .numberOfPayments NumberUtils.formatTimes list.size .paidAmount paidAmount .paymentViewModels list .build AutoValue.Builder public abstract static class Builder public abstract Builder paidAmount String amount public abstract Builder numberOfPayments String amount public abstract Builder paymentViewModels List PaymentItemViewModelNew list public abstract PaymentHistoryViewModelNew build\n",
      "I think this problem could be solve if we handle compareTo function same as hashCode . If it is exist call that else compares as expected a b b a If you think the same then I can make a pull request for that change\n",
      "Looking at the part of the code that was changed for the fix I am wondering if it would not be better for the class TransformedHeader to implement Comparable sort instances first by Parsed index and then by Parsed field and let getFieldSequence method return a SortedSet instead of a List The implementation will become much simpler and will avoid any similar bugs in the future given that SortedSet guarantees correct order as long as Comparable compareTo has been implemented correctly.\n",
      "Hi will share few useful information on lambda expression in java use cases of java on iterators Java lambda expression are mainly used to execute a block of code later once or multiple times. In Boolean expressions we can use lambda expression. Example List list list.isEmpty Using lambda expression we can Create a new objects. Example new pet dog we can also used as consuming from an object. Example Pet p System.out.println p.getAgeMonths A lambda expression used to Select or extract from an object. Example String s s.length It produce a single value by performing computation on two values. Example int a int b a b we can Compare two objects using lambda expression. Example Pet p Pet p p getAgeMonths .compareTo p getAgeMonths\n",
      "I want to do this right or not at all. Since I haven found a good way to do this right in a very long time I ll go ahead and close this issue. The best way to test compareTo IMO is by using some form of property based testing.\n",
      "I want to do this right or not at all. Since I haven found a good way to do this right in a very long time I ll go ahead and close this issue. The best way to test compareTo IMO is by using some form of property based testing. Sorry about this.\n",
      "Sample code to recreate issue package com.acme.neo j import java.util. import java.util.concurrent.ExecutorService import java.util.concurrent.TimeUnit import org.neo j.driver.v import org.neo j.driver.v exceptions.Neo jException import static java.util.concurrent.Executors.newFixedThreadPool public class ConcurrentRelationships public static int NUM PERSONS public static int NUM RELATIONSHIPS public static int NUM THREADS public static void main String... args Driver driver GraphDatabase.driver bolt localhost AuthTokens.none Generate and insert group of people List String people new ArrayList try Session session driver.session for int i i NUM PERSONS i people.add person i String nodeStatement MERGE p Person name person i session.run nodeStatement .consume System.out.println Adding person i Generate relationships List Map String Object relationships new ArrayList for int i i NUM RELATIONSHIPS int personA int Math.round Math.random NUM PERSONS int personB int Math.round Math.random NUM PERSONS if personA personB Map String Object relationship new HashMap relationship.put personA people.get personA relationship.put personB people.get personB relationships.add relationship i String relStatementAB WITH personA as personA personB as personB MATCH target Person name personB MATCH source Person name personA SET target. lock true SET source. lock true WITH source target CREATE source rel KNOWS target String relStatementBA WITH personA as personA personB as personB MATCH source Person name personA MATCH target Person name personB SET source. lock true SET target. lock true WITH source target CREATE source rel KNOWS target Insert relationships concurrently ExecutorService executor newFixedThreadPool NUM THREADS try for Map String Object entry relationships executor.submit try Session session driver.session if entry.get personA .toString .compareTo entry.get personB .toString personA name personB name session.run relStatementAB entry else session.run relStatementBA entry System.out.println entry.get personA KNOWS entry.get personB catch Neo jException n je System.err.println n je.getMessage executor.shutdown executor.awaitTermination Long.MAX VALUE TimeUnit.NANOSECONDS catch InterruptedException e e.printStackTrace finally if executor.isShutdown executor.shutdownNow System.out.println Done driver.close concurrentRelationships.zip\n",
      "The behavior change was introduced in ce a b d af eaf fd ad e d when we started to check that start end . The obvious fix would be to use BigDecimal comparator internally but this is actually a breaking change since the default comparison strategy is to use BigDecimal.equals for equal and not BigDecimal.compareTo . Anyway I think it is better to change the comparison strategy to be based on BigDecimal.compareTo except for isEqualTo that should honor BigDecimal.equals as users have isEqualByComparingTo to use BigDecimal.compareTo .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking about proper version number comparison. Let think about situation when version numbers excluding update number are the same and a. both versions have some update number it is clear we just compare update numbers. b. CVE doesn specify the update number but the dependency does We probably should consider all the update versions as affected i.e. we have update version . c. CVE specifies update number but the dependency does not have any Maybe we should consider this as update version so we match those as affected. Is this the desired behavior If so it is getting a bit tricky. The comparison cannot be performed in compareTo method as we treat differently versions of dependency and versions in CVE. Tricky doesn mean impossible though. It can be pretty easy but it would require some review of usage of compareTo. I can probably do it. But first I like to check if we agree on this approach.\n",
      "it should specify it does a string.compareTo or equivalent.\n",
      "compareTo is a proper comparison method in Java. Do you really think that we should go so deep in the spec Sorry but it sound like next time you will want us to specify that equals method must be used instead of .\n",
      "afaik operator diamond is currently unused. operator spaceship does a comparison does a compareTo on comparables. you can use them in your own code by defining the methods\n",
      "As ordinal is commonly used in compareTo based on JavaDoc it seems safe to rely on that for now. A similar question could of course arise to several similar methods in ComparableQuantity except isEquivalentTo which has a special purpose where simple equals is often insufficient to make a true comparison.\n",
      "I not sure to understand both compareTo and isGreaterThanOrEqualTo are based on ordinal . So I still missing the plus value of isGreaterThanOrEqualTo\n",
      "Does updating the Java version help at all or updating to Java It looks like Zulu builds of OpenJDK may use a different numbering scheme and are at u for the latest Java version. I not sure but the check with ver .compareTo might be incorrect if the version is . Still if it true that OpenGL or higher with the FBO extension is required and your graphics drivers don satisfy that requirement what should this application do when it catches that Throwable A GUI window for a message would need to use AWT Swing instead of the existing LWJGL user interface because LWJGL would keep throwing that Throwable. If the application is a jar file and was double clicked it won have a terminal to print to by default on Windows double clicking uses javaw.exe which does not pop up a cmd.exe instance while java.exe would . It sounds like this isn an issue with this repo code but rather with driver support on your machine or compatibility bugs in the JDK. There also the possibility that OpenJ an alternative compiler open sourced by IBM might not have the same compatibility bug I just ran this app on Windows with OpenJ version and with the older IBM J version and it seems to run fine. Maybe try some other Java versions if this is still an issue In particular if OpenJ doesn have this issue on Windows IBM has said they will support Java for a long time and that may include older Java on older hardware that might be good to know so any bundles of this application could include that JDK instead of OpenJDK. The Adopt OpenJDK project makes OpenJ easy to install now and that wasn always the case. If any form of Java works this project can probably distribute a minimized JRE using jlink which would save some download time and would guarantee a specific Java version.\n",
      "If you really want to do that why not just adding a BigDecimal test in the existing compareTo method\n",
      "Looks like Forge replaces the KeyBinding class with one that doesn have a compareTo method. Is using LiteLoader with Forge something that happens alot\n",
      "Okay so I think the basic reason why it getting large is due to fire e.g. on netherrack constantly updating itself and that TreeSet uses compareTo to check for duplicates rather than equals and hashcode . The remove methods for BlockUpdateEntry will never remove anything because they will never compareTo\n",
      "Perhaps but digging into the spec I see p Note that when determining equality of dates these semantics imply that if either date time has unspecified components the result of the comparison will be unknown null . In the above situation since the right interval is more fully specified than the left it seems appropriate that compareTo. returns null . Jumping up a level to the calling IncludedInEvaluator.included. the spec p says Each of these operators including IncludedIn returns true if the intervals X and Y are in the given relationship to each other. If either or both arguments are null the result is null. Otherwise the result is false. . So this leads me to believe that lines Are not correctly dealing with the fact than LessOrEqualEvaluator.lessOrEqual rightStart leftStart can and in this instance does return null . Throwing an NPE isn correct behavior. I believe that a correct implementation would be Of course in this example still it begs the question of why the CQL PeriodToIntervalOfDT and Assessment Period Three are returning incompatible Intervals.\n",
      "damir I see you are right regarding and I can dropp those checks it does not matter if the exception has been thrown because of false if statement or because of failed cast regarding I think the NaN checks are neccesary. With the external class for the NaN representation stored in a static field I could eleminate all the if this NaN checks but I still have to check if other subtrahend multiplicant etc. is NaN because ta j supports the NaN value. That means for example numOf .minus NaN should result in NaN and not in a ClassCastException or whatever error the delegate will produce. Furthermore we have to ensure symmetry and transivity of the equals o and compareTo o functions for example NaN.equals numOf numOf .equals NaN\n",
      "Looks to me like compareTo. isn handling this right because Instants left right are identical and they re being used for the other comparisons.\n"
     ]
    }
   ],
   "source": [
    "for i in block_comments_df.index:\n",
    "    sent = block_comments_df.loc[i, 'preprocessed_comments']\n",
    "    \n",
    "    if 'compareTo' in sent:\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290019\n",
      "286010\n"
     ]
    }
   ],
   "source": [
    "# Get all comments that contain non-empty preprocessed sentences\n",
    "preprocessed_comments_df = block_comments_df[block_comments_df.astype(str)['tokenised_sentences'] != '[]'].copy()\n",
    "preprocessed_comments_df['original_index'] = preprocessed_comments_df.index\n",
    "print(len(block_comments_df.index))\n",
    "print(len(preprocessed_comments_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed relevant comment map for all APIs...\n",
      "Total of 213 ambiguous APIs found...\n",
      "Created mappings for all APIs to relevant comments...\n",
      "Relevant comments for each caveat calculated in 65.28461500000003 s\n"
     ]
    }
   ],
   "source": [
    "# Determine which comments are relevant for each caveat\n",
    "# Note: apply a class-name-must-also-appear-in-text restriction on apis that are found \n",
    "# in at least <ambigious_cutoff> comments to reduce computation later in bm25/w2v\n",
    "\n",
    "class_and_apis = set()\n",
    "relevant_comments_dict = {} # map apis to number of relevant comments\n",
    "ambigious_cutoff = 1000 # number of comments before an api is considered ambiguous\n",
    "\n",
    "for i in caveats.index:\n",
    "    pair = (re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower()), caveats.loc[i, 'api'].lower())\n",
    "    class_and_apis.add(pair)\n",
    "\n",
    "classes = set([a for a, b in class_and_apis])\n",
    "apis = set([b for a, b in class_and_apis])\n",
    "\n",
    "start = time.clock()\n",
    "for i in preprocessed_comments_df.index:\n",
    "    tokens = preprocessed_comments_df.loc[i, 'tokenised_para']\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in apis:\n",
    "            if not token in relevant_comments_dict:\n",
    "                relevant_comments_dict[token] = set()\n",
    "            relevant_comments_dict[token].add(i)\n",
    "print(\"Completed relevant comment map for all APIs...\")\n",
    "\n",
    "ambiguous_apis = {} # map apis to list of possible classes\n",
    "for api in relevant_comments_dict:\n",
    "    if len(relevant_comments_dict[api]) > ambigious_cutoff:\n",
    "        for c in classes:\n",
    "            if (c, api) in class_and_apis:\n",
    "                if not api in ambiguous_apis:\n",
    "                    ambiguous_apis[api] = set()\n",
    "                ambiguous_apis[api].add(c)\n",
    "print(\"Total of {} ambiguous APIs found...\".format(len(ambiguous_apis)))\n",
    "                \n",
    "relevant_comments_dict = {} # map api to set of relevant comment indices\n",
    "restricted_relevant_comments_dict = {} # map <class, api> pairs to set of relevant comment indices\n",
    "         \n",
    "for i in preprocessed_comments_df.index:\n",
    "    tokens = preprocessed_comments_df.loc[i, 'tokenised_para']\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in apis:\n",
    "            if token in ambiguous_apis:\n",
    "                for c in ambiguous_apis[token]:\n",
    "                    if c in tokens:\n",
    "                        if not (c, token) in restricted_relevant_comments_dict:\n",
    "                            restricted_relevant_comments_dict[(c, token)] = set()\n",
    "                        restricted_relevant_comments_dict[(c, token)].add(i)\n",
    "            else:\n",
    "                if not token in relevant_comments_dict:\n",
    "                    relevant_comments_dict[token] = set()\n",
    "                relevant_comments_dict[token].add(i)\n",
    "            \n",
    "print(\"Created mappings for all APIs to relevant comments...\")\n",
    "            \n",
    "end =time.clock()\n",
    "training_time=end-start\n",
    "print('Relevant comments for each caveat calculated in ' + str(training_time)+ ' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21932\n",
      "11215\n"
     ]
    }
   ],
   "source": [
    "print(len(class_and_apis))\n",
    "print(len(apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692933\n"
     ]
    }
   ],
   "source": [
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit number of comments considered relevant\n",
    "for key in restricted_relevant_comments_dict:\n",
    "    if len(restricted_relevant_comments_dict[key]) >= ambigious_cutoff:\n",
    "        restricted_relevant_comments_dict[key] = \\\n",
    "            sample(restricted_relevant_comments_dict[key], ambigious_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the comment sentences to file, alongside relevant info to retrieve df row later\n",
    "with open('./output/issue-comment-sentences-new.txt', 'w+') as f_out_sents, \\\n",
    "    open('./output/comment_index_to_sentence_index.jsonl', 'w+') as f_out_index:\n",
    "        \n",
    "    sent_str = ''\n",
    "    index_str = ''\n",
    "    \n",
    "    c = 0\n",
    "    for i in preprocessed_comments_df.index:\n",
    "        sentences = preprocessed_comments_df.loc[i, 'tokenised_sentences']\n",
    "        sentence_indices = []\n",
    "        for sentence in sentences:\n",
    "            sent_str += ' '.join(sentence) + '\\n'\n",
    "            sentence_indices.append(c)\n",
    "            c += 1\n",
    "            \n",
    "        index_str += ujson.dumps({'comment_index': i, 'sentence_indices': sentence_indices}) + '\\n'\n",
    "    \n",
    "    f_out_sents.write(sent_str)\n",
    "    f_out_index.write(index_str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training and cost 69.31662900000083 s\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "start = time.clock()\n",
    "model = word2vec.Word2Vec(PathLineSentences('./output/issue-comment-sentences-new.txt'), size=100, window=5, min_count=5, workers=cores-1, iter=1, sg=1)\n",
    "end =time.clock()\n",
    "training_time=end-start\n",
    "print('end training and cost ' + str(training_time)+ ' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec model\n",
    "model.save('./output/word2vec-new.model')\n",
    "model.wv.save_word2vec_format('./output/word2vec-new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading sentences from file...\n",
      "IDF computation time: 4.449582000000021s\n"
     ]
    }
   ],
   "source": [
    "# calculate idf\n",
    "idf = {}\n",
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    start = time.clock()\n",
    "    lines = f.readlines()\n",
    "    print('Finished reading sentences from file...')    \n",
    "    vocab = list(Word2Vec.load('./output/word2vec-new.model').wv.vocab.keys())\n",
    "    N = len(lines)\n",
    "    docs = [sentence.split() for sentence in lines]\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            if word not in idf:\n",
    "                idf[word] = 1\n",
    "            else:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word in idf:\n",
    "        idf[word] = math.log(N / float(idf[word] + 1))\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('IDF computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average document length: 10.688945395875214\n"
     ]
    }
   ],
   "source": [
    "s_avg = 0 # avg doc length\n",
    "with open('./output/issue-comment-sentences-new.txt','r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    doc_lengths = [len(line.split()) for line in lines]\n",
    "    s_avg = sum(doc_lengths) / len(doc_lengths)\n",
    "    print(\"average document length: {}\".format(s_avg)) \n",
    "\n",
    "# Calculate combination scores of word2vec and bm25\n",
    "def bm25(doc, s2, idf):\n",
    "    score = 0\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "\n",
    "    for w in doc:\n",
    "        idf_s = idf.get(w, 1)\n",
    "        bm25_ra = s2.count(w) * (k1 + 1)\n",
    "        bm25_rb = s2.count(w) + k1 * (1 - b + b * len(s2) / s_avg)\n",
    "        score += idf_s * (bm25_ra / bm25_rb)\n",
    "    return score\n",
    "\n",
    "def compute(s1, s2, voc):   \n",
    "    v2 = np.array([voc[s] for s in s2 if s in voc])\n",
    "    v2 = v2.sum(axis=0)\n",
    "\n",
    "    v1 = np.array([voc[s] for s in s1 if s in voc])\n",
    "    v1 = v1.sum(axis=0)\n",
    "    \n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "def cosine(sentences, s2, voc):\n",
    "    s1_df_score = pd.Series(sentences)\n",
    "    s1_df_score = s1_df_score.map(lambda x: compute(x, s2, voc))\n",
    "    \n",
    "    s1_df_score.dropna(inplace=True)\n",
    "    return s1_df_score.sort_values(ascending=False).head(1)\n",
    "        \n",
    "def load_voc(file_voc):\n",
    "    vector_file = codecs.open(file_voc, 'r', encoding='utf-8')\n",
    "    line = vector_file.readline()\n",
    "    voc_size, vec_dim = map(int, line.split(' '))\n",
    "    embedding = dict()\n",
    "    line = vector_file.readline()\n",
    "    while line:\n",
    "        items = line.split(' ')\n",
    "        item = items[0]\n",
    "        vec = np.array(items[1:], dtype='float32')\n",
    "        embedding[item] = vec\n",
    "        line = vector_file.readline()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF score computation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_tf_idf_query_similarity(vectorizer, vecs, query):\n",
    "    \"\"\"\n",
    "    vectorizer: TfIdfVectorizer model\n",
    "    docs_tfidf: tfidf vectors for all docs\n",
    "    query: query doc\n",
    "\n",
    "    return: cosine similarity between query and all docs\n",
    "    \"\"\"\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    sim_scores = cosine_similarity(query_tfidf, vecs).flatten()\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed TF-IDF vectors for all documents...\n"
     ]
    }
   ],
   "source": [
    "vocab = load_voc('./output/word2vec-new.txt')\n",
    "tf_idf_vectorizer = None\n",
    "tf_idf_vectors = None\n",
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    docs = f.readlines()\n",
    "    \n",
    "    tf_idf_vectorizer = TfidfVectorizer(lowercase=None)\n",
    "    tf_idf_vectors = tf_idf_vectorizer.fit_transform(docs)\n",
    "    print(\"Computed TF-IDF vectors for all documents...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to map issue sentences to metadata\n",
    "comment_index_to_sentence_indices = {}\n",
    "sentence_index_to_comment_index = {}\n",
    "with open('./output/comment_index_to_sentence_index.jsonl') as f:\n",
    "    for line in f:\n",
    "        d = ujson.loads(line)\n",
    "        comment_index_to_sentence_indices[d['comment_index']] = d['sentence_indices']\n",
    "        \n",
    "        for index in d['sentence_indices']:\n",
    "            sentence_index_to_comment_index[index] = d['comment_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for TF-IDF vectors computation time: 88.99516799999998s\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "\n",
    "with open('./output/tfidf_results.jsonl', 'w+') as f:\n",
    "    complete_tfidf_sim_results = ''\n",
    "    c = 0\n",
    "    # calculate scores for each caveat sentence\n",
    "    for i in caveats.index:       \n",
    "        api = caveats.loc[i,'api'].lower()\n",
    "        class_name = re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower())\n",
    "        pair = (class_name, api)\n",
    "        has_relevant_comments = False\n",
    "        comment_indices = []\n",
    "        \n",
    "        if api in relevant_comments_dict:\n",
    "            comment_indices = relevant_comments_dict[api]\n",
    "            has_relevant_comments = True\n",
    "        elif pair in restricted_relevant_comments_dict:\n",
    "            comment_indices = restricted_relevant_comments_dict[pair]\n",
    "            has_relevant_comments = True\n",
    "    \n",
    "        comment_indices = list(comment_indices)\n",
    "        \n",
    "        if has_relevant_comments:\n",
    "            indices = []\n",
    "            for index in comment_indices:\n",
    "                if index in comment_index_to_sentence_indices:\n",
    "                    indices += comment_index_to_sentence_indices[index]\n",
    "\n",
    "            if len(indices) > 0:\n",
    "                relevant_vecs = tf_idf_vectors[indices,:]\n",
    "                sim_scores = get_tf_idf_query_similarity(tf_idf_vectorizer, relevant_vecs, ' '.join(caveats.loc[i, 'tokens']))\n",
    "\n",
    "                scores = {}\n",
    "                for j, score in enumerate(sim_scores):\n",
    "                    if score > 0:\n",
    "                        comment_id = sentence_index_to_comment_index[indices[j]]\n",
    "                        if not comment_id in scores or scores[comment_id]['score'] < score:\n",
    "                            scores[comment_id] = {\n",
    "                                'score': score,\n",
    "                                'comment_id': comment_id\n",
    "                            }\n",
    "\n",
    "                if len(scores) > 0:\n",
    "                    scores = [scores[key] for key in scores]\n",
    "                    complete_tfidf_sim_results += ujson.dumps({\n",
    "                        'caveat_id': i,\n",
    "                        'scores': scores\n",
    "                    }) + '\\n'\n",
    "\n",
    "                    c += 1\n",
    "\n",
    "                    if c >= 2000:\n",
    "                        f.write(complete_tfidf_sim_results)\n",
    "\n",
    "                        c = 0\n",
    "                        complete_tfidf_sim_results = ''\n",
    "\n",
    "    if len(complete_tfidf_sim_results) > 0:\n",
    "        f.write(complete_tfidf_sim_results)\n",
    "           \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('Cosine similarity for TF-IDF vectors computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity computation time: 8282.487697999997s\n"
     ]
    }
   ],
   "source": [
    "with open('./output/combined_sim_results.jsonl', 'w+') as f_combo_out, \\\n",
    "    open('./output/word2vec_results.jsonl', 'w+') as f_w2v_out, \\\n",
    "    open('./output/bm25_results.jsonl', 'w+') as f_bm25_out, \\\n",
    "    open('./output/ir_error_log.jsonl', 'w+') as f_err:\n",
    "        \n",
    "    start = time.clock()\n",
    "    complete_combined_results = ''\n",
    "    complete_bm25_results = ''\n",
    "    complete_word2vec_results = ''\n",
    "    errors = ''\n",
    "    c=0\n",
    "    \n",
    "    for i in caveats.index:\n",
    "        api = caveats.loc[i,'api'].lower()\n",
    "        class_name = re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower())\n",
    "        pair = (class_name, api)\n",
    "        has_relevant_comments = False\n",
    "        comment_indices = []\n",
    "        \n",
    "        if api in relevant_comments_dict:\n",
    "            comment_indices = relevant_comments_dict[api]\n",
    "            has_relevant_comments = True\n",
    "        elif pair in restricted_relevant_comments_dict:\n",
    "            comment_indices = restricted_relevant_comments_dict[pair]\n",
    "            has_relevant_comments = True\n",
    "            \n",
    "        comment_indices = list(comment_indices)\n",
    "        \n",
    "        if has_relevant_comments:\n",
    "            try:\n",
    "                caveat_sent = caveats.loc[i,'tokens']\n",
    "                combined_sim_results = []\n",
    "                bm25_results = []\n",
    "                word2vec_results = []\n",
    "\n",
    "                # retrieve issue comment sentences that are relevant\n",
    "                relevant_comments_df = preprocessed_comments_df[preprocessed_comments_df.index.isin(comment_indices)]\n",
    "                sim_w2v = relevant_comments_df['tokenised_sentences'].apply(cosine, s2=caveat_sent, voc=vocab)\n",
    "                sim_bm25 = relevant_comments_df['tokenised_para'].apply(bm25, s2=caveat_sent, idf=idf)\n",
    "\n",
    "                sim_bm25 = (sim_bm25 - sim_bm25.min()) / (sim_bm25.max() - sim_bm25.min())\n",
    "                sim_bm25 = pd.to_numeric(sim_bm25, downcast='float')\n",
    "                sim_w2v = pd.to_numeric(sim_w2v[0], downcast='float')\n",
    "\n",
    "                if len(sim_bm25) != 0 or len(sim_w2v) != 0:\n",
    "                    # word2vec cosine similarity\n",
    "                    for j in sim_w2v.index:\n",
    "                        if not np.isnan(sim_w2v[j]):\n",
    "                            word2vec_results.append({\n",
    "                                'score': float(sim_w2v[j]),\n",
    "                                'comment_id': int(relevant_comments_df.loc[j, 'original_index'])\n",
    "                            })\n",
    "\n",
    "                    # bm25 score\n",
    "                    for j in sim_bm25.index:\n",
    "                        if not np.isnan(sim_bm25[j]):\n",
    "                            bm25_results.append({\n",
    "                                'score': float(sim_bm25[j]),\n",
    "                                'comment_id': int(relevant_comments_df.loc[j, 'original_index'])\n",
    "                            })\n",
    "                    # calculate combination similarity score\n",
    "                    combined_sim = 0.5 * sim_bm25.add(0.5 * sim_w2v, fill_value=0)\n",
    "                    for j in combined_sim.index:\n",
    "                        if not np.isnan(combined_sim[j]):\n",
    "                            combined_sim_results.append({\n",
    "                                'score': float(combined_sim[j]),\n",
    "                                'comment_id': int(relevant_comments_df.loc[j, 'original_index'])\n",
    "                            })\n",
    "                    # Write results to relevant files\n",
    "                    if len(word2vec_results) > 0:\n",
    "                        complete_word2vec_results += ujson.dumps({\n",
    "                            'caveat_id': i,\n",
    "                            'scores': word2vec_results\n",
    "                        }) + '\\n'\n",
    "                        \n",
    "                    if len(bm25_results) > 0:\n",
    "                        complete_bm25_results += ujson.dumps({\n",
    "                            'caveat_id': i,\n",
    "                            'scores': bm25_results\n",
    "                        }) + '\\n'\n",
    "\n",
    "                    if len(combined_sim_results) > 0:\n",
    "                        complete_combined_results += ujson.dumps({\n",
    "                            'caveat_id': i,\n",
    "                            'scores': combined_sim_results\n",
    "                        }) + '\\n'\n",
    "\n",
    "                    c+=1\n",
    "\n",
    "                    # write buffered results to file\n",
    "                    if c >= 2000:\n",
    "                        c = 0\n",
    "\n",
    "                        f_combo_out.write(complete_combined_results)\n",
    "                        f_bm25_out.write(complete_bm25_results)\n",
    "                        f_w2v_out.write(complete_word2vec_results)\n",
    "\n",
    "                        # reset output strings\n",
    "                        complete_combined_results = ''\n",
    "                        complete_bm25_results = ''\n",
    "                        complete_word2vec_results = ''\n",
    "\n",
    "            except Exception as e:\n",
    "                errors += ujson.dumps({'caveat_index': i, 'error': e}) + '\\n'\n",
    "    \n",
    "    # write any buffered results remaining\n",
    "    if len(complete_combined_results) > 0:\n",
    "        f_combo_out.write(complete_combined_results.strip())\n",
    "    if len(complete_bm25_results) > 0:\n",
    "        f_bm25_out.write(complete_bm25_results.strip())\n",
    "    if len(complete_word2vec_results) > 0:\n",
    "        f_w2v_out.write(complete_word2vec_results.strip())\n",
    "        \n",
    "    # write error log\n",
    "    f_err.write(errors)\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('Similarity computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_label = set()\n",
    "\n",
    "\n",
    "def output_label_ready_file(results_path, doccano_path, output_complete_path, ids_to_label):\n",
    "    with open(results_path) as f, open(doccano_path, 'w+') as f_out_docanno, \\\n",
    "            open(output_complete_path, 'w+') as f_out:\n",
    "        \n",
    "        c = 0\n",
    "        results = []\n",
    "        for line in f:\n",
    "            obj = ujson.loads(line)\n",
    "            c += len(obj['scores'])\n",
    "\n",
    "            obj['scores'] = sorted(obj['scores'], key=lambda x: x['score'], reverse=True)\n",
    "            obj['scores'] = obj['scores'][:3] # limit to 3 results per caveat\n",
    "            results.append(obj)\n",
    "\n",
    "        print('Number of results: {}'.format(len(results)))\n",
    "        print('Total number of scores: {}'.format(c))\n",
    "        to_label = sample(results, 384)\n",
    "\n",
    "        for obj in to_label:\n",
    "            for res in obj['scores']:\n",
    "                comment_index = res['comment_id']\n",
    "                caveat_index = obj['caveat_id']\n",
    "                f_out.write(ujson.dumps({\n",
    "                    'score': res['score'],\n",
    "                    'comment': preprocessed_comments_df.loc[comment_index, 'body'],\n",
    "                    'class': caveats.loc[caveat_index, 'simple_class_name'],\n",
    "                    'api': caveats.loc[caveat_index,'api'],\n",
    "                    'caveat': caveats.loc[caveat_index, 'sentence'],\n",
    "                    'html_url': preprocessed_comments_df.loc[comment_index, 'html_url']\n",
    "                }) + '\\n')\n",
    "                \n",
    "                class_in_body = re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower()) in preprocessed_comments_df.loc[comment_index, 'tokenised_para']\n",
    "                f_out_docanno.write(ujson.dumps({\n",
    "                    'text': 'contains class: ' + str(class_in_body) + '\\nclass: ' +  caveats.loc[caveat_index, 'simple_class_name'] + '\\napi: ' + caveats.loc[caveat_index,'api'] + \\\n",
    "                        '\\n--------------------------------------\\ncaveat: ' + caveats.loc[caveat_index,'sentence'] + \\\n",
    "                        '\\n--------------------------------------\\ncomment: '+ re.sub(r'```([^```]*)```', '', preprocessed_comments_df.loc[comment_index, 'body']),\n",
    "                    'labels': ['non-relevant']\n",
    "                }) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 29028\n",
      "Total number of scores: 1621408\n"
     ]
    }
   ],
   "source": [
    "# input paths\n",
    "tfidf_path = './output/tfidf_results.jsonl'\n",
    "w2v_path = './output/word2vec_results.jsonl'\n",
    "bm25_path = './output/bm25_results.jsonl'\n",
    "combo_path = './output/combined_sim_results.jsonl'\n",
    "\n",
    "# doccano labelling paths\n",
    "tfidf_docanno_path = './output/tfidf_to_label.jsonl'\n",
    "w2v_docanno_path = './output/w2v_to_label.jsonl'\n",
    "bm25_docanno_path = './output/bm25_to_label.jsonl'\n",
    "combo_docanno_path = './output/combo_to_label.jsonl'\n",
    "\n",
    "# output paths\n",
    "tfidf_sample_path = './output/tfidf_sample.jsonl'\n",
    "w2v_sample_path = './output/w2v_sample.jsonl'\n",
    "bm25_sample_path = './output/bm25_sample.jsonl'\n",
    "combo_sample_path = './output/combo_sample.jsonl'\n",
    "\n",
    "output_label_ready_file(tfidf_path, tfidf_docanno_path, tfidf_sample_path)\n",
    "# output_label_ready_file(w2v_path, w2v_docanno_path, w2v_sample_path)\n",
    "# output_label_ready_file(bm25_path, bm25_docanno_path, bm25_sample_path)\n",
    "# output_label_ready_file(combo_path, combo_docanno_path, combo_sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'annotations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9c2ecf74d223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# write_labelled_doccano_relevance(w2v_labelled_path, w2v_sample_path, w2v_output_labelled_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# write_labelled_doccano_relevance(bm25_labelled_path, bm25_sample_path, bm25_output_labelled_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mwrite_labelled_doccano_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo_labelled_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo_sample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo_output_labelled_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-9c2ecf74d223>\u001b[0m in \u001b[0;36mwrite_labelled_doccano_relevance\u001b[0;34m(labelled_path, sample_path, output_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relevant'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'annotations'"
     ]
    }
   ],
   "source": [
    "tfidf_labelled_path = './output/labelled/tfidf.jsonl'\n",
    "w2v_labelled_path = './output/labelled/w2v.jsonl'\n",
    "bm25_labelled_path = './output/labelled/bm25.jsonl'\n",
    "combo_labelled_path = './output/labelled/combo.jsonl'\n",
    "\n",
    "tfidf_output_labelled_path = './labelled_data/tfidf.jsonl'\n",
    "w2v_output_labelled_path = './labelled_data/w2v.jsonl'\n",
    "bm25_output_labelled_path = './labelled_data/bm25.jsonl'\n",
    "combo_output_labelled_path = './labelled_data/combo.jsonl'\n",
    "\n",
    "def write_labelled_doccano_relevance(labelled_path, sample_path, output_path):\n",
    "    with open(labelled_path) as f, open(sample_path) as f2, open(output_path, 'w+') as f_out:\n",
    "        labelled = []\n",
    "        objs = []\n",
    "        \n",
    "        for line in f:\n",
    "            labelled.append(ujson.loads(line))\n",
    "        for line in f2:\n",
    "            objs.append(ujson.loads(line))\n",
    "            \n",
    "        for i, obj in enumerate(objs):\n",
    "            if len(labelled[i]['annotations']) == 0:\n",
    "                obj['label'] = 'relevant'\n",
    "            else:\n",
    "                obj['label'] = 'non-relevant'\n",
    "            f_out.write(ujson.dumps(obj) + '\\n')\n",
    "            \n",
    "# write_labelled_doccano_relevance(tfidf_labelled_path, tfidf_sample_path, tfidf_output_labelled_path)\n",
    "# write_labelled_doccano_relevance(w2v_labelled_path, w2v_sample_path, w2v_output_labelled_path)\n",
    "# write_labelled_doccano_relevance(bm25_labelled_path, bm25_sample_path, bm25_output_labelled_path)\n",
    "write_labelled_doccano_relevance(combo_labelled_path, combo_sample_path, combo_output_labelled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
