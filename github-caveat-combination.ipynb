{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['don', 'more', 'don', 'theirs', 'those', 'does', 'you', 'there', 'needn', 'herself', 'doesn', 'over', 'his', 'was', 'she', 'after', 'he', 'our', 'her', 'just', 'will', 'of', 'same', 'against', 'during', 'down', 'them', 'ourselves', 'below', 'into', 'some', 'hadn', 'shouldn', 'has', 'when', 'who', 'such', 'than', 'you', 'you', 'having', 'here', 'shan', 'whom', 'doesn', 'won', 'most', 'these', 'other', 'each', 'so', 'should', 'you', 'wasn', 'wasn', 'it', 'didn', 'its', 'any', 'this', 'between', 'shouldn', 'isn', 'do', 'did', 'aren', 'an', 'itself', 'how', 'too', 'for', 'off', 'couldn', 'is', 'she', 'at', 'which', 'all', 'what', 'it', 'myself', 'then', 'mightn', 'few', 'weren', 'that', 'can', 'only', 'while', 'had', 'being', 'no', 'under', 'himself', 'up', 'weren', 'about', 'hers', 'my', 'both', 'didn', 'be', 'on', 'll', 'hasn', 'through', 'that', 'yourselves', 'haven', 'why', 'are', 'if', 'your', 'from', 'in', 'by', 'couldn', 'ma', 'their', 'and', 'hadn', 'me', 're', 'now', 'we', 'they', 'aren', 'but', 'because', 'where', 'with', 'him', 'out', 'once', 'am', 'hasn', 'to', 'ain', 'needn', 'yourself', 'until', 'before', 'wouldn', 'mustn', 'wouldn', 'should', 'the', 'ours', 'further', 'were', 'mightn', 'shan', 'own', 'haven', 'again', 'you', 'as', 'have', 'doing', 'been', 've', 'not', 'yours', 'very', 'nor', 'themselves', 'isn', 'or', 'mustn', 'won', 'above']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ujson\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from itertools import repeat\n",
    "from scipy import spatial\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import functools\n",
    "import operator\n",
    "from random import sample\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = [s.split(\"'\")[0] for s in stopwords]\n",
    "custom_stopwords = [s for s in custom_stopwords if len(s) > 1]\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 627450\n",
      "Number of comments: 1855870\n"
     ]
    }
   ],
   "source": [
    "# load the extracted java-related GitHub data\n",
    "def load_github_issues():\n",
    "    with open('./output/issue-comments-revised.jsonl') as issue_comments_f:\n",
    "        issue_list = []\n",
    "        comments_list = []\n",
    "\n",
    "        for line in issue_comments_f:\n",
    "            obj = ujson.loads(line)\n",
    "\n",
    "            comments_list.append({\n",
    "                'body': obj['body'],\n",
    "                'repo_name': obj['repo_name'],\n",
    "                'html_url': obj['html_url'],\n",
    "                'issue_id': obj['issue']['id']\n",
    "            })\n",
    "\n",
    "            issue = obj['issue']\n",
    "            issue['repo_name'] = obj['repo_name']\n",
    "            issue_list.append(issue)\n",
    "\n",
    "        issues_df = pd.DataFrame(issue_list)\n",
    "        issues_df = issues_df.drop_duplicates(subset=['id'])\n",
    "        comments_df = pd.DataFrame(comments_list)\n",
    "        \n",
    "        return (issues_df, comments_df)\n",
    "\n",
    "\n",
    "issues_df, comments_df = load_github_issues()\n",
    "print(\"Number of issues: {}\".format(len(issues_df.index)))\n",
    "print(\"Number of comments: {}\".format(len(comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues with code block/s: 85318\n"
     ]
    }
   ],
   "source": [
    "# find the issues that contain code blocks\n",
    "code_issues_df = issues_df.dropna(subset=['body'])\n",
    "code_issues_df = code_issues_df[code_issues_df['body'].str.contains('```')]\n",
    "print(\"Number of issues with code block/s: {}\".format(len(code_issues_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/jooby-project/jooby/blob/ma...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/965</td>\n",
       "      <td>284439800</td>\n",
       "      <td>965</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>2nd thymeleaf code snippet (in the documentati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When updating Spotless from `3.6.0` to `3.7.0`...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/182</td>\n",
       "      <td>285279535</td>\n",
       "      <td>182</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Unable to store input properties... when upgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I'm using Google Guava [Preconditions](https:/...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47</td>\n",
       "      <td>268767363</td>\n",
       "      <td>47</td>\n",
       "      <td>uber/NullAway</td>\n",
       "      <td>NullAway doesn't recognize Guava Preconditions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&gt; Citing added javadoc:\\r\\n\\r\\nImmutables appl...</td>\n",
       "      <td>https://github.com/immutables/immutables/issue...</td>\n",
       "      <td>285281793</td>\n",
       "      <td>740</td>\n",
       "      <td>immutables/immutables</td>\n",
       "      <td>Style-level fence for annotation classpath aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Hi, i try to build ffmpeg for windows 32 bit \\...</td>\n",
       "      <td>https://github.com/bytedeco/javacpp-presets/is...</td>\n",
       "      <td>285279281</td>\n",
       "      <td>503</td>\n",
       "      <td>bytedeco/javacpp-presets</td>\n",
       "      <td>Build for windows-x86 not working</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   https://github.com/jooby-project/jooby/blob/ma...   \n",
       "3   When updating Spotless from `3.6.0` to `3.7.0`...   \n",
       "10  I'm using Google Guava [Preconditions](https:/...   \n",
       "19  > Citing added javadoc:\\r\\n\\r\\nImmutables appl...   \n",
       "33  Hi, i try to build ffmpeg for windows 32 bit \\...   \n",
       "\n",
       "                                             html_url         id  number  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/965  284439800     965   \n",
       "3     https://github.com/diffplug/spotless/issues/182  285279535     182   \n",
       "10         https://github.com/uber/NullAway/issues/47  268767363      47   \n",
       "19  https://github.com/immutables/immutables/issue...  285281793     740   \n",
       "33  https://github.com/bytedeco/javacpp-presets/is...  285279281     503   \n",
       "\n",
       "                   repo_name  \\\n",
       "2        jooby-project/jooby   \n",
       "3          diffplug/spotless   \n",
       "10             uber/NullAway   \n",
       "19     immutables/immutables   \n",
       "33  bytedeco/javacpp-presets   \n",
       "\n",
       "                                                title  \n",
       "2   2nd thymeleaf code snippet (in the documentati...  \n",
       "3   Unable to store input properties... when upgra...  \n",
       "10  NullAway doesn't recognize Guava Preconditions...  \n",
       "19  Style-level fence for annotation classpath aut...  \n",
       "33                  Build for windows-x86 not working  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments for issues with code block/s: 290019\n"
     ]
    }
   ],
   "source": [
    "block_comments_df = comments_df[comments_df['issue_id'].isin(code_issues_df['id'])].copy()\n",
    "print('Number of comments for issues with code block/s: {}'.format(len(block_comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove all code blocks\n",
    "    doc = re.sub(r'```([^```]*)```', '', doc)\n",
    "            \n",
    "    # remove urls\n",
    "    doc = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',             \n",
    "        '', doc)\n",
    "    \n",
    "    # remove line break characters\n",
    "    doc = re.sub(r'[\\r\\n]', ' ', doc)\n",
    "    \n",
    "    # remove apostrophes/suffixes\n",
    "    doc = re.sub(r\"'\\w |\\w' \", ' ', doc)\n",
    "    \n",
    "    # remove numbers\n",
    "    doc = re.sub(r'(\\d\\.?)+', ' ', doc)\n",
    "        \n",
    "    # replace all punctuation except for full stop with space\n",
    "    doc = re.sub(r'[^A-Za-z\\.]', ' ', doc)\n",
    "        \n",
    "    # normalise full stops\n",
    "    doc = re.sub(r'\\s\\.\\.+', '.', doc)\n",
    "    \n",
    "    # remove more than 1 whitespace\n",
    "    doc = re.sub('\\s\\s+', ' ', doc)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    docs = doc.split('. ')\n",
    "    docs = [t for t in docs if t != '']\n",
    "    return docs\n",
    "\n",
    "def doc_tokenize(doc):\n",
    "    doc = re.sub('\\.', ' ', doc) # remove full stops\n",
    "    \n",
    "    tokens = [t.lower() for t in doc.split() if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub('\\.', ' ', sentence) # remove full stops\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    tokens = [t.lower() for t in tokens if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caveat sentences: 73831\n"
     ]
    }
   ],
   "source": [
    "# load the non-dreprecated java doc caveat sentences\n",
    "def load_caveats():\n",
    "    caveat_files_dir = './output/java_12_spec_caveat_sentences_revised/'\n",
    "    caveats_list = []\n",
    "\n",
    "    files = glob.glob(caveat_files_dir + '*.json')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            arr = ujson.load(f)\n",
    "            full_class_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            simple_class_name = full_class_name.split('.')[-1]\n",
    "            for caveat in arr:\n",
    "                if not caveat['deprecated'] and 'name' in caveat:\n",
    "                    for sentence in caveat['sentences']:\n",
    "                        caveats_list.append({\n",
    "                            'simple_class_name': simple_class_name,\n",
    "                            'full_class_name': full_class_name,\n",
    "                            'api': caveat['name'],\n",
    "                            'sentence': sentence,\n",
    "                            'type': 'body'\n",
    "                        })\n",
    "\n",
    "                    # add all misc level sentences\n",
    "                    for misc_obj in caveat['caveat_misc']:\n",
    "                        if misc_obj['name'] in ['Parameters:', 'Throws:']:\n",
    "                            for obj in misc_obj['list']:\n",
    "                                for misc_sentence in obj['sentences']:\n",
    "                                    sentence = ''\n",
    "                                    if misc_obj['name'] == 'Parameters:':\n",
    "                                        sentence = obj['parameter'] + ' ' + misc_sentence\n",
    "                                    else:\n",
    "                                        sentence = obj['exception'] + ' ' + misc_sentence\n",
    "                                    caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': sentence,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "                        else:\n",
    "                            for s in misc_obj['list']:\n",
    "                                caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': s,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "    return pd.DataFrame(caveats_list)\n",
    "\n",
    "caveats = load_caveats()\n",
    "print('Number of caveat sentences: {}'.format(len(caveats.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "caveats['preprocessed'] = caveats['sentence'].apply(lambda x: preprocess(x))\n",
    "caveats['tokens'] = caveats['preprocessed'].map(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api</th>\n",
       "      <th>full_class_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>simple_class_name</th>\n",
       "      <th>type</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "      <td>This method returns if thread contention monit...</td>\n",
       "      <td>[method, returns, thread, contention, monitori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This statistic is reset when the thread conten...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "      <td>This statistic is reset when the thread conten...</td>\n",
       "      <td>[statistic, reset, thread, contention, monitor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>the approximate accumulated elapsed time in mi...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "      <td>the approximate accumulated elapsed time in mi...</td>\n",
       "      <td>[approximate, accumulated, elapsed, time, mill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>UnsupportedOperationException if the Java virt...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "      <td>UnsupportedOperationException if the Java virt...</td>\n",
       "      <td>[unsupportedoperationexception, java, virtual,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getWaitedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "      <td>This method returns if thread contention monit...</td>\n",
       "      <td>[method, returns, thread, contention, monitori...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              api                  full_class_name  \\\n",
       "0  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "1  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "2  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "3  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "4   getWaitedTime  java.lang.management.ThreadInfo   \n",
       "\n",
       "                                            sentence simple_class_name  type  \\\n",
       "0  This method returns -1 if thread contention mo...        ThreadInfo  body   \n",
       "1  This statistic is reset when the thread conten...        ThreadInfo  body   \n",
       "2  the approximate accumulated elapsed time in mi...        ThreadInfo  misc   \n",
       "3  UnsupportedOperationException if the Java virt...        ThreadInfo  misc   \n",
       "4  This method returns -1 if thread contention mo...        ThreadInfo  body   \n",
       "\n",
       "                                        preprocessed  \\\n",
       "0  This method returns if thread contention monit...   \n",
       "1  This statistic is reset when the thread conten...   \n",
       "2  the approximate accumulated elapsed time in mi...   \n",
       "3  UnsupportedOperationException if the Java virt...   \n",
       "4  This method returns if thread contention monit...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [method, returns, thread, contention, monitori...  \n",
       "1  [statistic, reset, thread, contention, monitor...  \n",
       "2  [approximate, accumulated, elapsed, time, mill...  \n",
       "3  [unsupportedoperationexception, java, virtual,...  \n",
       "4  [method, returns, thread, contention, monitori...  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caveats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>preprocessed_comments</th>\n",
       "      <th>tokenised_para</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tokenised_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just an example to see how to use the  `thymel...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/...</td>\n",
       "      <td>284439800</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>Just an example to see how to use the thymelea...</td>\n",
       "      <td>[example, see, use, thymeleaf, api, need, test...</td>\n",
       "      <td>[Just an example to see how to use the thymele...</td>\n",
       "      <td>[[example, see, use, thymeleaf, api, need, tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>nedtwigg Would it be practical for you to upgr...</td>\n",
       "      <td>[nedtwigg, would, practical, upgrade, propriet...</td>\n",
       "      <td>[nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>[[nedtwigg, would, practical, upgrade, proprie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nope - 4.4.1 brings a different, unrelated pro...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Nope brings a different unrelated problem</td>\n",
       "      <td>[nope, brings, different, unrelated, problem]</td>\n",
       "      <td>[Nope brings a different unrelated problem]</td>\n",
       "      <td>[[nope, brings, different, unrelated, problem]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>`Objects.requireNonNull` should also be suppor...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47#iss...</td>\n",
       "      <td>268767363</td>\n",
       "      <td>uber/NullAway</td>\n",
       "      <td>Objects.requireNonNull should also be supported.</td>\n",
       "      <td>[objects, requirenonnull, also, supported]</td>\n",
       "      <td>[Objects.requireNonNull should also be support...</td>\n",
       "      <td>[[objects, requirenonnull, also, supported]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Darn, that's a shame. :&lt;\\r\\n\\r\\nNothing comes ...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Darn that a shame. Nothing comes to mind yet b...</td>\n",
       "      <td>[darn, shame, nothing, comes, mind, yet, might...</td>\n",
       "      <td>[Darn that a shame, Nothing comes to mind yet ...</td>\n",
       "      <td>[[darn, shame], [nothing, comes, mind, yet, mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   Just an example to see how to use the  `thymel...   \n",
       "3   @nedtwigg Would it be practical for you to upg...   \n",
       "7   Nope - 4.4.1 brings a different, unrelated pro...   \n",
       "10  `Objects.requireNonNull` should also be suppor...   \n",
       "14  Darn, that's a shame. :<\\r\\n\\r\\nNothing comes ...   \n",
       "\n",
       "                                             html_url   issue_id  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/...  284439800   \n",
       "3   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "7   https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "10  https://github.com/uber/NullAway/issues/47#iss...  268767363   \n",
       "14  https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "\n",
       "              repo_name                              preprocessed_comments  \\\n",
       "2   jooby-project/jooby  Just an example to see how to use the thymelea...   \n",
       "3     diffplug/spotless  nedtwigg Would it be practical for you to upgr...   \n",
       "7     diffplug/spotless          Nope brings a different unrelated problem   \n",
       "10        uber/NullAway   Objects.requireNonNull should also be supported.   \n",
       "14    diffplug/spotless  Darn that a shame. Nothing comes to mind yet b...   \n",
       "\n",
       "                                       tokenised_para  \\\n",
       "2   [example, see, use, thymeleaf, api, need, test...   \n",
       "3   [nedtwigg, would, practical, upgrade, propriet...   \n",
       "7       [nope, brings, different, unrelated, problem]   \n",
       "10         [objects, requirenonnull, also, supported]   \n",
       "14  [darn, shame, nothing, comes, mind, yet, might...   \n",
       "\n",
       "                                            sentences  \\\n",
       "2   [Just an example to see how to use the thymele...   \n",
       "3   [nedtwigg Would it be practical for you to upg...   \n",
       "7         [Nope brings a different unrelated problem]   \n",
       "10  [Objects.requireNonNull should also be support...   \n",
       "14  [Darn that a shame, Nothing comes to mind yet ...   \n",
       "\n",
       "                                  tokenised_sentences  \n",
       "2   [[example, see, use, thymeleaf, api, need, tes...  \n",
       "3   [[nedtwigg, would, practical, upgrade, proprie...  \n",
       "7     [[nope, brings, different, unrelated, problem]]  \n",
       "10       [[objects, requirenonnull, also, supported]]  \n",
       "14  [[darn, shame], [nothing, comes, mind, yet, mi...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed all comments...\n",
      "Tokenized all paragraphs...\n",
      "Completed sentence tokenization...\n",
      "Tokenized all sentences...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence_list(sent_list):\n",
    "    return [tokenize(x) for x in sent_list]\n",
    "\n",
    "def calculate_preprocessed_comment_sentences(df):\n",
    "    df['preprocessed_comments'] = df['body'].map(lambda x: preprocess(x))\n",
    "    print('Preprocessed all comments...')\n",
    "    \n",
    "    df['tokenised_para'] = df['preprocessed_comments'].map(lambda x: doc_tokenize(x))\n",
    "    print('Tokenized all paragraphs...')\n",
    "    \n",
    "    df['sentences'] = df['preprocessed_comments'].map(lambda x: sent_tokenize(x))\n",
    "    print('Completed sentence tokenization...')\n",
    "    \n",
    "    df['tokenised_sentences'] = df['sentences'].map(lambda x: tokenize_sentence_list(x))\n",
    "    print('Tokenized all sentences...')\n",
    "\n",
    "\n",
    "# calculate_preprocessed_comment_sentences(block_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed all comments...\n",
      "Tokenized all paragraphs...\n",
      "Completed sentence tokenization...\n",
      "Tokenized all sentences...\n",
      "1855870\n"
     ]
    }
   ],
   "source": [
    "calculate_preprocessed_comment_sentences(comments_df)\n",
    "print(len(comments_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290019\n",
      "286010\n"
     ]
    }
   ],
   "source": [
    "# Get all comments that contain non-empty preprocessed sentences\n",
    "preprocessed_comments_df = block_comments_df[block_comments_df.astype(str)['tokenised_sentences'] != '[]'].copy()\n",
    "preprocessed_comments_df['original_index'] = preprocessed_comments_df.index\n",
    "print(len(block_comments_df.index))\n",
    "print(len(preprocessed_comments_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed relevant comment map for all APIs...\n",
      "Total of 213 ambiguous APIs found...\n",
      "Created mappings for all APIs to relevant comments...\n",
      "Relevant comments for each caveat calculated in 68.78327499999999 s\n"
     ]
    }
   ],
   "source": [
    "# Determine which comments are relevant for each caveat\n",
    "# Note: apply a class-name-must-also-appear-in-text restriction on apis that are found \n",
    "# in at least <ambigious_cutoff> comments to reduce computation later in bm25/w2v\n",
    "\n",
    "class_and_apis = set()\n",
    "relevant_comments_dict = {} # map apis to number of relevant comments\n",
    "ambigious_cutoff = 1000 # number of comments before an api is considered ambiguous\n",
    "\n",
    "for i in caveats.index:\n",
    "    pair = (re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower()), caveats.loc[i, 'api'].lower())\n",
    "    class_and_apis.add(pair)\n",
    "\n",
    "classes = set([a for a, b in class_and_apis])\n",
    "apis = set([b for a, b in class_and_apis])\n",
    "\n",
    "start = time.clock()\n",
    "for i in preprocessed_comments_df.index:\n",
    "    tokens = preprocessed_comments_df.loc[i, 'tokenised_para']\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in apis:\n",
    "            if not token in relevant_comments_dict:\n",
    "                relevant_comments_dict[token] = set()\n",
    "            relevant_comments_dict[token].add(i)\n",
    "print(\"Completed relevant comment map for all APIs...\")\n",
    "\n",
    "ambiguous_apis = {} # map apis to list of possible classes\n",
    "for api in relevant_comments_dict:\n",
    "    if len(relevant_comments_dict[api]) > ambigious_cutoff:\n",
    "        for c in classes:\n",
    "            if (c, api) in class_and_apis:\n",
    "                if not api in ambiguous_apis:\n",
    "                    ambiguous_apis[api] = set()\n",
    "                ambiguous_apis[api].add(c)\n",
    "print(\"Total of {} ambiguous APIs found...\".format(len(ambiguous_apis)))\n",
    "                \n",
    "relevant_comments_dict = {} # map api to set of relevant comment indices\n",
    "restricted_relevant_comments_dict = {} # map <class, api> pairs to set of relevant comment indices\n",
    "         \n",
    "for i in preprocessed_comments_df.index:\n",
    "    tokens = preprocessed_comments_df.loc[i, 'tokenised_para']\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in apis:\n",
    "            if token in ambiguous_apis:\n",
    "                for c in ambiguous_apis[token]:\n",
    "                    if c in tokens:\n",
    "                        if not (c, token) in restricted_relevant_comments_dict:\n",
    "                            restricted_relevant_comments_dict[(c, token)] = set()\n",
    "                        restricted_relevant_comments_dict[(c, token)].add(i)\n",
    "            else:\n",
    "                if not token in relevant_comments_dict:\n",
    "                    relevant_comments_dict[token] = set()\n",
    "                relevant_comments_dict[token].add(i)\n",
    "            \n",
    "print(\"Created mappings for all APIs to relevant comments...\")\n",
    "            \n",
    "end =time.clock()\n",
    "training_time=end-start\n",
    "print('Relevant comments for each caveat calculated in ' + str(training_time)+ ' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21932\n",
      "11215\n"
     ]
    }
   ],
   "source": [
    "print(len(class_and_apis))\n",
    "print(len(apis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit number of comments considered relevant\n",
    "for key in restricted_relevant_comments_dict:\n",
    "    if len(restricted_relevant_comments_dict[key]) >= ambigious_cutoff:\n",
    "        restricted_relevant_comments_dict[key] = \\\n",
    "            sample(restricted_relevant_comments_dict[key], ambigious_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the comment sentences to file, alongside relevant info to retrieve df row later\n",
    "with open('./output/issue-comment-sentences-new.txt', 'w+') as f_out_sents, \\\n",
    "    open('./output/comment_index_to_sentence_index.jsonl', 'w+') as f_out_index:\n",
    "        \n",
    "    sent_str = ''\n",
    "    index_str = ''\n",
    "    \n",
    "    c = 0\n",
    "    for i in preprocessed_comments_df.index:\n",
    "        sentences = preprocessed_comments_df.loc[i, 'tokenised_sentences']\n",
    "        sentence_indices = []\n",
    "        for sentence in sentences:\n",
    "            sent_str += ' '.join(sentence) + '\\n'\n",
    "            sentence_indices.append(c)\n",
    "            c += 1\n",
    "            \n",
    "        index_str += ujson.dumps({'comment_index': i, 'sentence_indices': sentence_indices}) + '\\n'\n",
    "    \n",
    "    f_out_sents.write(sent_str)\n",
    "    f_out_index.write(index_str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training and cost 69.31662900000083 s\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "start = time.clock()\n",
    "model = word2vec.Word2Vec(PathLineSentences('./output/issue-comment-sentences-new.txt'), size=100, window=5, min_count=5, workers=cores-1, iter=1, sg=1)\n",
    "end =time.clock()\n",
    "training_time=end-start\n",
    "print('end training and cost ' + str(training_time)+ ' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec model\n",
    "model.save('./output/word2vec-new.model')\n",
    "model.wv.save_word2vec_format('./output/word2vec-new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading sentences from file...\n",
      "IDF computation time: 4.449582000000021s\n"
     ]
    }
   ],
   "source": [
    "# calculate idf\n",
    "idf = {}\n",
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    start = time.clock()\n",
    "    lines = f.readlines()\n",
    "    print('Finished reading sentences from file...')    \n",
    "    vocab = list(Word2Vec.load('./output/word2vec-new.model').wv.vocab.keys())\n",
    "    N = len(lines)\n",
    "    docs = [sentence.split() for sentence in lines]\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            if word not in idf:\n",
    "                idf[word] = 1\n",
    "            else:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word in idf:\n",
    "        idf[word] = math.log(N / float(idf[word] + 1))\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('IDF computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average document length: 10.688945395875214\n"
     ]
    }
   ],
   "source": [
    "s_avg = 0 # avg doc length\n",
    "with open('./output/issue-comment-sentences-new.txt','r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    doc_lengths = [len(line.split()) for line in lines]\n",
    "    s_avg = sum(doc_lengths) / len(doc_lengths)\n",
    "    print(\"average document length: {}\".format(s_avg)) \n",
    "\n",
    "# Calculate combination scores of word2vec and bm25\n",
    "def bm25(doc, s2, idf):\n",
    "    score = 0\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "\n",
    "    for w in doc:\n",
    "        idf_s = idf.get(w, 1)\n",
    "        bm25_ra = s2.count(w) * (k1 + 1)\n",
    "        bm25_rb = s2.count(w) + k1 * (1 - b + b * len(s2) / s_avg)\n",
    "        score += idf_s * (bm25_ra / bm25_rb)\n",
    "    return score\n",
    "\n",
    "def compute(s1, s2, voc):   \n",
    "    v2 = np.array([voc[s] for s in s2 if s in voc])\n",
    "    v2 = v2.sum(axis=0)\n",
    "\n",
    "    v1 = np.array([voc[s] for s in s1 if s in voc])\n",
    "    v1 = v1.sum(axis=0)\n",
    "    \n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "def cosine(sentences, s2, voc):\n",
    "    s1_df_score = pd.Series(sentences)\n",
    "    s1_df_score = s1_df_score.map(lambda x: compute(x, s2, voc))\n",
    "    \n",
    "    s1_df_score.dropna(inplace=True)\n",
    "    return s1_df_score.sort_values(ascending=False).head(1)\n",
    "        \n",
    "def load_voc(file_voc):\n",
    "    vector_file = codecs.open(file_voc, 'r', encoding='utf-8')\n",
    "    line = vector_file.readline()\n",
    "    voc_size, vec_dim = map(int, line.split(' '))\n",
    "    embedding = dict()\n",
    "    line = vector_file.readline()\n",
    "    while line:\n",
    "        items = line.split(' ')\n",
    "        item = items[0]\n",
    "        vec = np.array(items[1:], dtype='float32')\n",
    "        embedding[item] = vec\n",
    "        line = vector_file.readline()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF score computation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_tf_idf_query_similarity(vectorizer, vecs, query):\n",
    "    \"\"\"\n",
    "    vectorizer: TfIdfVectorizer model\n",
    "    docs_tfidf: tfidf vectors for all docs\n",
    "    query: query doc\n",
    "\n",
    "    return: cosine similarity between query and all docs\n",
    "    \"\"\"\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    sim_scores = cosine_similarity(query_tfidf, vecs).flatten()\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed TF-IDF vectors for all documents...\n"
     ]
    }
   ],
   "source": [
    "vocab = load_voc('./output/word2vec-new.txt')\n",
    "tf_idf_vectorizer = None\n",
    "tf_idf_vectors = None\n",
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    docs = f.readlines()\n",
    "    \n",
    "    tf_idf_vectorizer = TfidfVectorizer(lowercase=None)\n",
    "    tf_idf_vectors = tf_idf_vectorizer.fit_transform(docs)\n",
    "    print(\"Computed TF-IDF vectors for all documents...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict to map issue sentences to metadata\n",
    "comment_index_to_sentence_indices = {}\n",
    "sentence_index_to_comment_index = {}\n",
    "with open('./output/comment_index_to_sentence_index.jsonl') as f:\n",
    "    for line in f:\n",
    "        d = ujson.loads(line)\n",
    "        comment_index_to_sentence_indices[d['comment_index']] = d['sentence_indices']\n",
    "        \n",
    "        for index in d['sentence_indices']:\n",
    "            sentence_index_to_comment_index[index] = d['comment_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for TF-IDF vectors computation time: 88.99516799999998s\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "\n",
    "with open('./output/tfidf_results.jsonl', 'w+') as f:\n",
    "    complete_tfidf_sim_results = ''\n",
    "    c = 0\n",
    "    # calculate scores for each caveat sentence\n",
    "    for i in caveats.index:       \n",
    "        api = caveats.loc[i,'api'].lower()\n",
    "        class_name = re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower())\n",
    "        pair = (class_name, api)\n",
    "        has_relevant_comments = False\n",
    "        comment_indices = []\n",
    "        \n",
    "        if api in relevant_comments_dict:\n",
    "            comment_indices = relevant_comments_dict[api]\n",
    "            has_relevant_comments = True\n",
    "        elif pair in restricted_relevant_comments_dict:\n",
    "            comment_indices = restricted_relevant_comments_dict[pair]\n",
    "            has_relevant_comments = True\n",
    "    \n",
    "        comment_indices = list(comment_indices)\n",
    "        \n",
    "        if has_relevant_comments:\n",
    "            indices = []\n",
    "            for index in comment_indices:\n",
    "                if index in comment_index_to_sentence_indices:\n",
    "                    indices += comment_index_to_sentence_indices[index]\n",
    "\n",
    "            if len(indices) > 0:\n",
    "                relevant_vecs = tf_idf_vectors[indices,:]\n",
    "                sim_scores = get_tf_idf_query_similarity(tf_idf_vectorizer, relevant_vecs, ' '.join(caveats.loc[i, 'tokens']))\n",
    "\n",
    "                scores = {}\n",
    "                for j, score in enumerate(sim_scores):\n",
    "                    if score > 0:\n",
    "                        comment_id = sentence_index_to_comment_index[indices[j]]\n",
    "                        if not comment_id in scores or scores[comment_id]['score'] < score:\n",
    "                            scores[comment_id] = {\n",
    "                                'score': score,\n",
    "                                'comment_id': comment_id\n",
    "                            }\n",
    "\n",
    "                if len(scores) > 0:\n",
    "                    scores = [scores[key] for key in scores]\n",
    "                    complete_tfidf_sim_results += ujson.dumps({\n",
    "                        'caveat_id': i,\n",
    "                        'scores': scores\n",
    "                    }) + '\\n'\n",
    "\n",
    "                    c += 1\n",
    "\n",
    "                    if c >= 2000:\n",
    "                        f.write(complete_tfidf_sim_results)\n",
    "\n",
    "                        c = 0\n",
    "                        complete_tfidf_sim_results = ''\n",
    "\n",
    "    if len(complete_tfidf_sim_results) > 0:\n",
    "        f.write(complete_tfidf_sim_results)\n",
    "           \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('Cosine similarity for TF-IDF vectors computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity computation time: 8282.487697999997s\n"
     ]
    }
   ],
   "source": [
    "with open('./output/combined_sim_results.jsonl', 'w+') as f_combo_out, \\\n",
    "    open('./output/word2vec_results.jsonl', 'w+') as f_w2v_out, \\\n",
    "    open('./output/bm25_results.jsonl', 'w+') as f_bm25_out, \\\n",
    "    open('./output/ir_error_log.jsonl', 'w+') as f_err:\n",
    "        \n",
    "    start = time.clock()\n",
    "    complete_combined_results = ''\n",
    "    complete_bm25_results = ''\n",
    "    complete_word2vec_results = ''\n",
    "    errors = ''\n",
    "    c=0\n",
    "    \n",
    "    for i in caveats.index:\n",
    "        api = caveats.loc[i,'api'].lower()\n",
    "        class_name = re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower())\n",
    "        pair = (class_name, api)\n",
    "        has_relevant_comments = False\n",
    "        comment_indices = []\n",
    "        \n",
    "        if api in relevant_comments_dict:\n",
    "            comment_indices = relevant_comments_dict[api]\n",
    "            has_relevant_comments = True\n",
    "        elif pair in restricted_relevant_comments_dict:\n",
    "            comment_indices = restricted_relevant_comments_dict[pair]\n",
    "            has_relevant_comments = True\n",
    "            \n",
    "        comment_indices = list(comment_indices)\n",
    "        \n",
    "        if has_relevant_comments:\n",
    "            try:\n",
    "                caveat_sent = caveats.loc[i,'tokens']\n",
    "                combined_sim_results = []\n",
    "                bm25_results = []\n",
    "                word2vec_results = []\n",
    "\n",
    "                # retrieve issue comment sentences that are relevant\n",
    "                relevant_comments_df = preprocessed_comments_df[preprocessed_comments_df.index.isin(comment_indices)]\n",
    "                sim_w2v = relevant_comments_df['tokenised_sentences'].apply(cosine, s2=caveat_sent, voc=vocab)\n",
    "                sim_bm25 = relevant_comments_df['tokenised_para'].apply(bm25, s2=caveat_sent, idf=idf)\n",
    "\n",
    "                sim_bm25 = (sim_bm25 - sim_bm25.min()) / (sim_bm25.max() - sim_bm25.min())\n",
    "                sim_bm25 = pd.to_numeric(sim_bm25, downcast='float')\n",
    "                sim_w2v = pd.to_numeric(sim_w2v[0], downcast='float')\n",
    "\n",
    "                if len(sim_bm25) != 0 or len(sim_w2v) != 0:\n",
    "                    # word2vec cosine similarity\n",
    "                    for j in sim_w2v.index:\n",
    "                        if not np.isnan(sim_w2v[j]):\n",
    "                            word2vec_results.append({\n",
    "                                'score': float(sim_w2v[j]),\n",
    "                                'comment_id': int(relevant_comments_df.loc[j, 'original_index'])\n",
    "                            })\n",
    "\n",
    "                    # bm25 score\n",
    "                    for j in sim_bm25.index:\n",
    "                        if not np.isnan(sim_bm25[j]):\n",
    "                            bm25_results.append({\n",
    "                                'score': float(sim_bm25[j]),\n",
    "                                'comment_id': int(relevant_comments_df.loc[j, 'original_index'])\n",
    "                            })\n",
    "                    # calculate combination similarity score\n",
    "                    combined_sim = 0.5 * sim_bm25.add(0.5 * sim_w2v, fill_value=0)\n",
    "                    for j in combined_sim.index:\n",
    "                        if not np.isnan(combined_sim[j]):\n",
    "                            combined_sim_results.append({\n",
    "                                'score': float(combined_sim[j]),\n",
    "                                'comment_id': int(relevant_comments_df.loc[j, 'original_index'])\n",
    "                            })\n",
    "                    # Write results to relevant files\n",
    "                    if len(word2vec_results) > 0:\n",
    "                        complete_word2vec_results += ujson.dumps({\n",
    "                            'caveat_id': i,\n",
    "                            'scores': word2vec_results\n",
    "                        }) + '\\n'\n",
    "                        \n",
    "                    if len(bm25_results) > 0:\n",
    "                        complete_bm25_results += ujson.dumps({\n",
    "                            'caveat_id': i,\n",
    "                            'scores': bm25_results\n",
    "                        }) + '\\n'\n",
    "\n",
    "                    if len(combined_sim_results) > 0:\n",
    "                        complete_combined_results += ujson.dumps({\n",
    "                            'caveat_id': i,\n",
    "                            'scores': combined_sim_results\n",
    "                        }) + '\\n'\n",
    "\n",
    "                    c+=1\n",
    "\n",
    "                    # write buffered results to file\n",
    "                    if c >= 2000:\n",
    "                        c = 0\n",
    "\n",
    "                        f_combo_out.write(complete_combined_results)\n",
    "                        f_bm25_out.write(complete_bm25_results)\n",
    "                        f_w2v_out.write(complete_word2vec_results)\n",
    "\n",
    "                        # reset output strings\n",
    "                        complete_combined_results = ''\n",
    "                        complete_bm25_results = ''\n",
    "                        complete_word2vec_results = ''\n",
    "\n",
    "            except Exception as e:\n",
    "                errors += ujson.dumps({'caveat_index': i, 'error': e}) + '\\n'\n",
    "    \n",
    "    # write any buffered results remaining\n",
    "    if len(complete_combined_results) > 0:\n",
    "        f_combo_out.write(complete_combined_results.strip())\n",
    "    if len(complete_bm25_results) > 0:\n",
    "        f_bm25_out.write(complete_bm25_results.strip())\n",
    "    if len(complete_word2vec_results) > 0:\n",
    "        f_w2v_out.write(complete_word2vec_results.strip())\n",
    "        \n",
    "    # write error log\n",
    "    f_err.write(errors)\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('Similarity computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_label = set()\n",
    "\n",
    "\n",
    "def output_label_ready_file(results_path, doccano_path, output_complete_path, ids_to_label):\n",
    "    with open(results_path) as f, open(doccano_path, 'w+') as f_out_docanno, \\\n",
    "            open(output_complete_path, 'w+') as f_out:\n",
    "        \n",
    "        c = 0\n",
    "        results = []\n",
    "        for line in f:\n",
    "            obj = ujson.loads(line)\n",
    "            c += len(obj['scores'])\n",
    "\n",
    "            obj['scores'] = sorted(obj['scores'], key=lambda x: x['score'], reverse=True)\n",
    "            obj['scores'] = obj['scores'][:3] # limit to 3 results per caveat\n",
    "            results.append(obj)\n",
    "\n",
    "        print('Number of results: {}'.format(len(results)))\n",
    "        print('Total number of scores: {}'.format(c))\n",
    "        to_label = sample(results, 384)\n",
    "\n",
    "        for obj in to_label:\n",
    "            for res in obj['scores']:\n",
    "                comment_index = res['comment_id']\n",
    "                caveat_index = obj['caveat_id']\n",
    "                f_out.write(ujson.dumps({\n",
    "                    'score': res['score'],\n",
    "                    'comment': preprocessed_comments_df.loc[comment_index, 'body'],\n",
    "                    'class': caveats.loc[caveat_index, 'simple_class_name'],\n",
    "                    'api': caveats.loc[caveat_index,'api'],\n",
    "                    'caveat': caveats.loc[caveat_index, 'sentence'],\n",
    "                    'html_url': preprocessed_comments_df.loc[comment_index, 'html_url']\n",
    "                }) + '\\n')\n",
    "                \n",
    "                class_in_body = re.sub('<.*', '', caveats.loc[i, 'simple_class_name'].lower()) in preprocessed_comments_df.loc[comment_index, 'tokenised_para']\n",
    "                f_out_docanno.write(ujson.dumps({\n",
    "                    'text': 'contains class: ' + str(class_in_body) + '\\nclass: ' +  caveats.loc[caveat_index, 'simple_class_name'] + '\\napi: ' + caveats.loc[caveat_index,'api'] + \\\n",
    "                        '\\n--------------------------------------\\ncaveat: ' + caveats.loc[caveat_index,'sentence'] + \\\n",
    "                        '\\n--------------------------------------\\ncomment: '+ re.sub(r'```([^```]*)```', '', preprocessed_comments_df.loc[comment_index, 'body']),\n",
    "                    'labels': ['non-relevant']\n",
    "                }) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 29028\n",
      "Total number of scores: 1621408\n"
     ]
    }
   ],
   "source": [
    "# input paths\n",
    "tfidf_path = './output/tfidf_results.jsonl'\n",
    "w2v_path = './output/word2vec_results.jsonl'\n",
    "bm25_path = './output/bm25_results.jsonl'\n",
    "combo_path = './output/combined_sim_results.jsonl'\n",
    "\n",
    "# doccano labelling paths\n",
    "tfidf_docanno_path = './output/tfidf_to_label.jsonl'\n",
    "w2v_docanno_path = './output/w2v_to_label.jsonl'\n",
    "bm25_docanno_path = './output/bm25_to_label.jsonl'\n",
    "combo_docanno_path = './output/combo_to_label.jsonl'\n",
    "\n",
    "# output paths\n",
    "tfidf_sample_path = './output/tfidf_sample.jsonl'\n",
    "w2v_sample_path = './output/w2v_sample.jsonl'\n",
    "bm25_sample_path = './output/bm25_sample.jsonl'\n",
    "combo_sample_path = './output/combo_sample.jsonl'\n",
    "\n",
    "output_label_ready_file(tfidf_path, tfidf_docanno_path, tfidf_sample_path)\n",
    "# output_label_ready_file(w2v_path, w2v_docanno_path, w2v_sample_path)\n",
    "# output_label_ready_file(bm25_path, bm25_docanno_path, bm25_sample_path)\n",
    "# output_label_ready_file(combo_path, combo_docanno_path, combo_sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'annotations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9c2ecf74d223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# write_labelled_doccano_relevance(w2v_labelled_path, w2v_sample_path, w2v_output_labelled_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# write_labelled_doccano_relevance(bm25_labelled_path, bm25_sample_path, bm25_output_labelled_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mwrite_labelled_doccano_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo_labelled_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo_sample_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo_output_labelled_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-9c2ecf74d223>\u001b[0m in \u001b[0;36mwrite_labelled_doccano_relevance\u001b[0;34m(labelled_path, sample_path, output_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'annotations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relevant'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'annotations'"
     ]
    }
   ],
   "source": [
    "tfidf_labelled_path = './output/labelled/tfidf.jsonl'\n",
    "w2v_labelled_path = './output/labelled/w2v.jsonl'\n",
    "bm25_labelled_path = './output/labelled/bm25.jsonl'\n",
    "combo_labelled_path = './output/labelled/combo.jsonl'\n",
    "\n",
    "tfidf_output_labelled_path = './labelled_data/tfidf.jsonl'\n",
    "w2v_output_labelled_path = './labelled_data/w2v.jsonl'\n",
    "bm25_output_labelled_path = './labelled_data/bm25.jsonl'\n",
    "combo_output_labelled_path = './labelled_data/combo.jsonl'\n",
    "\n",
    "def write_labelled_doccano_relevance(labelled_path, sample_path, output_path):\n",
    "    with open(labelled_path) as f, open(sample_path) as f2, open(output_path, 'w+') as f_out:\n",
    "        labelled = []\n",
    "        objs = []\n",
    "        \n",
    "        for line in f:\n",
    "            labelled.append(ujson.loads(line))\n",
    "        for line in f2:\n",
    "            objs.append(ujson.loads(line))\n",
    "            \n",
    "        for i, obj in enumerate(objs):\n",
    "            if len(labelled[i]['annotations']) == 0:\n",
    "                obj['label'] = 'relevant'\n",
    "            else:\n",
    "                obj['label'] = 'non-relevant'\n",
    "            f_out.write(ujson.dumps(obj) + '\\n')\n",
    "            \n",
    "# write_labelled_doccano_relevance(tfidf_labelled_path, tfidf_sample_path, tfidf_output_labelled_path)\n",
    "# write_labelled_doccano_relevance(w2v_labelled_path, w2v_sample_path, w2v_output_labelled_path)\n",
    "# write_labelled_doccano_relevance(bm25_labelled_path, bm25_sample_path, bm25_output_labelled_path)\n",
    "write_labelled_doccano_relevance(combo_labelled_path, combo_sample_path, combo_output_labelled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
