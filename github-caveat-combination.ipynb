{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['had', 'any', 'it', 'they', 'that', 'but', 'should', 'until', 'other', 'shouldn', 'below', 'more', 'down', 'weren', 'so', 'own', 'hers', 'very', 'will', 'how', 'after', 'doesn', 'or', 'its', 'am', 'she', 'aren', 'their', 'than', 'his', 'few', 'over', 'hasn', 'you', 'didn', 'couldn', 'needn', 'shan', 'shouldn', 'me', 'our', 'do', 'wasn', 'her', 'don', 'why', 'who', 're', 'just', 'don', 'from', 'same', 'it', 'at', 'wouldn', 'have', 'haven', 'during', 'has', 'themselves', 'you', 'where', 'both', 'shan', 'doing', 'are', 'when', 'hadn', 'weren', 'what', 'up', 'you', 'your', 'most', 'he', 'doesn', 'there', 'isn', 'of', 'for', 'and', 'himself', 'against', 'we', 'such', 'needn', 'out', 'hasn', 'if', 'whom', 'theirs', 'above', 'which', 'no', 'not', 'my', 'didn', 'these', 'here', 'aren', 'ma', 'wasn', 'being', 'some', 'should', 'as', 've', 'herself', 'this', 'with', 'that', 'mustn', 'having', 'you', 'yourself', 'through', 'those', 'can', 'is', 'were', 'be', 'couldn', 'hadn', 'you', 'further', 'did', 'because', 'him', 'yours', 'mustn', 'to', 'each', 'nor', 'ours', 'then', 'all', 'she', 'does', 'itself', 'on', 'was', 'by', 'again', 'myself', 'too', 'been', 'under', 'll', 'before', 'about', 'ourselves', 'now', 'won', 'mightn', 'only', 'ain', 'won', 'haven', 'them', 'off', 'into', 'wouldn', 'the', 'mightn', 'yourselves', 'while', 'isn', 'between', 'an', 'in', 'once']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ujson\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from itertools import repeat\n",
    "from scipy import spatial\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = [s.split(\"'\")[0] for s in stopwords]\n",
    "custom_stopwords = [s for s in custom_stopwords if len(s) > 1]\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 627450\n",
      "Number of comments: 1855870\n"
     ]
    }
   ],
   "source": [
    "# load the extracted java-related GitHub data\n",
    "def load_github_issues():\n",
    "    with open('./output/issue-comments-revised.jsonl') as issue_comments_f:\n",
    "        issue_list = []\n",
    "        comments_list = []\n",
    "\n",
    "        for line in issue_comments_f:\n",
    "            obj = ujson.loads(line)\n",
    "\n",
    "            comments_list.append({\n",
    "                'body': obj['body'],\n",
    "                'repo_name': obj['repo_name'],\n",
    "                'html_url': obj['html_url'],\n",
    "                'issue_id': obj['issue']['id']\n",
    "            })\n",
    "\n",
    "            issue = obj['issue']\n",
    "            issue['repo_name'] = obj['repo_name']\n",
    "            issue_list.append(issue)\n",
    "\n",
    "        issues_df = pd.DataFrame(issue_list)\n",
    "        issues_df = issues_df.drop_duplicates(subset=['id'])\n",
    "        comments_df = pd.DataFrame(comments_list)\n",
    "        \n",
    "        return (issues_df, comments_df)\n",
    "\n",
    "\n",
    "issues_df, comments_df = load_github_issues()\n",
    "print(\"Number of issues: {}\".format(len(issues_df.index)))\n",
    "print(\"Number of comments: {}\".format(len(comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues with code block/s: 85318\n"
     ]
    }
   ],
   "source": [
    "# find the issues that contain code blocks\n",
    "code_issues_df = issues_df.dropna(subset=['body'])\n",
    "code_issues_df = code_issues_df[code_issues_df['body'].str.contains('```')]\n",
    "print(\"Number of issues with code block/s: {}\".format(len(code_issues_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>id</th>\n",
       "      <th>number</th>\n",
       "      <th>repo_name</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://github.com/jooby-project/jooby/blob/ma...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/965</td>\n",
       "      <td>284439800</td>\n",
       "      <td>965</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "      <td>2nd thymeleaf code snippet (in the documentati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When updating Spotless from `3.6.0` to `3.7.0`...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/182</td>\n",
       "      <td>285279535</td>\n",
       "      <td>182</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "      <td>Unable to store input properties... when upgra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I'm using Google Guava [Preconditions](https:/...</td>\n",
       "      <td>https://github.com/uber/NullAway/issues/47</td>\n",
       "      <td>268767363</td>\n",
       "      <td>47</td>\n",
       "      <td>uber/NullAway</td>\n",
       "      <td>NullAway doesn't recognize Guava Preconditions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&gt; Citing added javadoc:\\r\\n\\r\\nImmutables appl...</td>\n",
       "      <td>https://github.com/immutables/immutables/issue...</td>\n",
       "      <td>285281793</td>\n",
       "      <td>740</td>\n",
       "      <td>immutables/immutables</td>\n",
       "      <td>Style-level fence for annotation classpath aut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Hi, i try to build ffmpeg for windows 32 bit \\...</td>\n",
       "      <td>https://github.com/bytedeco/javacpp-presets/is...</td>\n",
       "      <td>285279281</td>\n",
       "      <td>503</td>\n",
       "      <td>bytedeco/javacpp-presets</td>\n",
       "      <td>Build for windows-x86 not working</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  \\\n",
       "2   https://github.com/jooby-project/jooby/blob/ma...   \n",
       "3   When updating Spotless from `3.6.0` to `3.7.0`...   \n",
       "10  I'm using Google Guava [Preconditions](https:/...   \n",
       "19  > Citing added javadoc:\\r\\n\\r\\nImmutables appl...   \n",
       "33  Hi, i try to build ffmpeg for windows 32 bit \\...   \n",
       "\n",
       "                                             html_url         id  number  \\\n",
       "2   https://github.com/jooby-project/jooby/issues/965  284439800     965   \n",
       "3     https://github.com/diffplug/spotless/issues/182  285279535     182   \n",
       "10         https://github.com/uber/NullAway/issues/47  268767363      47   \n",
       "19  https://github.com/immutables/immutables/issue...  285281793     740   \n",
       "33  https://github.com/bytedeco/javacpp-presets/is...  285279281     503   \n",
       "\n",
       "                   repo_name  \\\n",
       "2        jooby-project/jooby   \n",
       "3          diffplug/spotless   \n",
       "10             uber/NullAway   \n",
       "19     immutables/immutables   \n",
       "33  bytedeco/javacpp-presets   \n",
       "\n",
       "                                                title  \n",
       "2   2nd thymeleaf code snippet (in the documentati...  \n",
       "3   Unable to store input properties... when upgra...  \n",
       "10  NullAway doesn't recognize Guava Preconditions...  \n",
       "19  Style-level fence for annotation classpath aut...  \n",
       "33                  Build for windows-x86 not working  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_issues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments for issues with code block/s: 290019\n"
     ]
    }
   ],
   "source": [
    "block_comments_df = comments_df[comments_df['issue_id'].isin(code_issues_df['id'])].copy()\n",
    "print('Number of comments for issues with code block/s: {}'.format(len(block_comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove all code blocks\n",
    "    doc = re.sub(r'```([^```]*)```', '', doc)\n",
    "            \n",
    "    # remove urls\n",
    "    doc = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',             \n",
    "        '', doc)\n",
    "    \n",
    "    # remove line break characters\n",
    "    doc = re.sub(r'[\\r\\n]', ' ', doc)\n",
    "    \n",
    "    # remove apostrophes/suffixes\n",
    "    doc = re.sub(r\"'\\w |\\w' \", ' ', doc)\n",
    "    \n",
    "    # remove numbers\n",
    "    doc = re.sub(r'(\\d\\.?)+', ' ', doc)\n",
    "        \n",
    "    # replace all punctuation except for full stop with space\n",
    "    doc = re.sub(r'[^A-Za-z\\.]', ' ', doc)\n",
    "        \n",
    "    # normalise full stops\n",
    "    doc = re.sub(r'\\s\\.\\.+', '.', doc)\n",
    "    \n",
    "    # remove more than 1 whitespace\n",
    "    doc = re.sub('\\s\\s+', ' ', doc)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    docs = doc.split('. ')\n",
    "    docs = [t for t in docs if t != '']\n",
    "    return docs\n",
    "\n",
    "def doc_tokenize(doc):\n",
    "    doc = re.sub('\\.', ' ', doc) # remove full stops\n",
    "    \n",
    "    tokens = [t.lower() for t in doc.split() if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub('\\.', ' ', sentence) # remove full stops\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    tokens = [t.lower() for t in tokens if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caveat sentences: 73831\n"
     ]
    }
   ],
   "source": [
    "# load the non-dreprecated java doc caveat sentences\n",
    "def load_caveats():\n",
    "    caveat_files_dir = './output/java_12_spec_caveat_sentences_revised/'\n",
    "    caveats_list = []\n",
    "\n",
    "    files = glob.glob(caveat_files_dir + '*.json')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            arr = ujson.load(f)\n",
    "            full_class_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            simple_class_name = full_class_name.split('.')[-1]\n",
    "            for caveat in arr:\n",
    "                if not caveat['deprecated'] and 'name' in caveat:\n",
    "                    for sentence in caveat['sentences']:\n",
    "                        caveats_list.append({\n",
    "                            'simple_class_name': simple_class_name,\n",
    "                            'full_class_name': full_class_name,\n",
    "                            'api': caveat['name'],\n",
    "                            'sentence': sentence,\n",
    "                            'type': 'body'\n",
    "                        })\n",
    "\n",
    "                    # add all misc level sentences\n",
    "                    for misc_obj in caveat['caveat_misc']:\n",
    "                        if misc_obj['name'] in ['Parameters:', 'Throws:']:\n",
    "                            for obj in misc_obj['list']:\n",
    "                                for misc_sentence in obj['sentences']:\n",
    "                                    caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': misc_sentence,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "                        else:\n",
    "                            for s in misc_obj['list']:\n",
    "                                caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': s,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "    return pd.DataFrame(caveats_list)\n",
    "\n",
    "caveats = load_caveats()\n",
    "print('Number of caveat sentences: {}'.format(len(caveats.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api</th>\n",
       "      <th>full_class_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>simple_class_name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This statistic is reset when the thread conten...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>the approximate accumulated elapsed time in mi...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>if the Java virtual machine does not support t...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getWaitedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              api                  full_class_name  \\\n",
       "0  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "1  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "2  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "3  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "4   getWaitedTime  java.lang.management.ThreadInfo   \n",
       "\n",
       "                                            sentence simple_class_name  type  \n",
       "0  This method returns -1 if thread contention mo...        ThreadInfo  body  \n",
       "1  This statistic is reset when the thread conten...        ThreadInfo  body  \n",
       "2  the approximate accumulated elapsed time in mi...        ThreadInfo  misc  \n",
       "3  if the Java virtual machine does not support t...        ThreadInfo  misc  \n",
       "4  This method returns -1 if thread contention mo...        ThreadInfo  body  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caveats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "caveats['preprocessed'] = caveats['sentence'].apply(lambda x: preprocess(x))\n",
    "caveats['tokens'] = caveats['preprocessed'].map(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate text in the issue all comments for each issue\n",
    "# issue_to_concat_text = {}\n",
    "# for index, row in code_issues_df.iterrows():\n",
    "#     assoc_comments_df = comments_df[comments_df['issue_id'] == row['id']]\n",
    "    \n",
    "#     concat_text = row['title'] + row['body']\n",
    "    \n",
    "#     for j, comment_row in assoc_comments_df.iterrows():\n",
    "#         concat_text += row['body']\n",
    "        \n",
    "#     issue_to_concat_text[row['id']] = concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of apis: 21942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Identify the relevant issues for all caveats\n",
    "# apis = set()\n",
    "# for caveat in caveats_list:\n",
    "#     apis.add((caveat['simple_class_name'], caveat['api']))\n",
    "    \n",
    "# print('Number of apis: {}'.format(len(apis)))\n",
    "    \n",
    "# def get_relevant_issues(api_tuple, issue_to_text_dict):\n",
    "#     relevant = []\n",
    "#     for id in issue_to_text_dict:\n",
    "#         if api_tuple[0] in issue_to_text_dict[id]:\n",
    "#             if (api_tuple[0] == api_tuple[1]) or api_tuple[1] in issue_to_text_dict[id]:\n",
    "#                 relevant.append(id)\n",
    "            \n",
    "#     return relevant\n",
    "\n",
    "# p = multiprocessing.Pool(2)\n",
    "# relevant_issues = p.starmap(get_relevant_issues, zip(get_relevant_issues, repeat(issue_to_concat_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./output/relevant_issues_for_caveats.jsonl', 'w+') as f:\n",
    "#     for i, api in enumerate(apis):\n",
    "#         f.write(ujson.dumps({'class': api[0], 'api': api[1], 'issue_ids': relevant_issues[i]}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed all comments...\n",
      "Tokenized all paragraphs...\n",
      "Completed sentence tokenization...\n",
      "Tokenized all sentences...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence_list(sent_list):\n",
    "    return [tokenize(x) for x in sent_list]\n",
    "\n",
    "def calculate_preprocessed_comment_sentences(df):\n",
    "    df['preprocessed_comments'] = df['body'].map(lambda x: preprocess(x))\n",
    "    print('Preprocessed all comments...')\n",
    "    \n",
    "    df['tokenised_para'] = df['preprocessed_comments'].map(lambda x: doc_tokenize(x))\n",
    "    print('Tokenized all paragraphs...')\n",
    "    \n",
    "    df['sentences'] = df['preprocessed_comments'].map(lambda x: sent_tokenize(x))\n",
    "    print('Completed sentence tokenization...')\n",
    "    \n",
    "    df['tokenised_sentences'] = df['sentences'].map(lambda x: tokenize_sentence_list(x))\n",
    "    print('Tokenized all sentences...')\n",
    "                            \n",
    "calculate_preprocessed_comment_sentences(block_comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290019\n",
      "286010\n"
     ]
    }
   ],
   "source": [
    "# Get all comments that contain non-empty preprocessed sentences\n",
    "preprocessed_comments_df = block_comments_df[block_comments_df.astype(str)['tokenised_sentences'] != '[]']\n",
    "print(len(block_comments_df.index))\n",
    "print(len(preprocessed_comments_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the comment sentences to file, alongside relevant info to retrieve df row later\n",
    "with open('./output/issue-comment-sentences-new.txt', 'w+') as f_out_sents, \\\n",
    "    open('./output/associated-sentence-df-index-new.txt', 'w+') as f_out_index, \\\n",
    "    open('./output/associated-sentence-issue-ids-new.txt', 'w+') as f_out_id:\n",
    "        \n",
    "    sent_str = ''\n",
    "    index_str = ''\n",
    "    id_str = ''\n",
    "    \n",
    "    c = 0\n",
    "    for i in preprocessed_comments_df.index:\n",
    "        issue_id = preprocessed_comments_df.loc[i, 'issue_id']\n",
    "        sentences = preprocessed_comments_df.loc[i, 'tokenised_sentences']\n",
    "        for sentence in sentences:\n",
    "            sent_str += ' '.join(sentence) + '\\n'\n",
    "            index_str += str(i) + '\\n'\n",
    "            id_str += str(issue_id) + '\\n'\n",
    "            c += 1\n",
    "            \n",
    "            if c >= 100000:\n",
    "                f_out_sents.write(sent_str)\n",
    "                f_out_index.write(index_str)\n",
    "                f_out_id.write(id_str)\n",
    "                sent_str = ''\n",
    "                index_str = ''\n",
    "                id_str = ''\n",
    "                \n",
    "                c = 0\n",
    "    \n",
    "    if len(sent_str) > 0:\n",
    "        f_out_sents.write(sent_str)\n",
    "    if len(index_str) > 0:\n",
    "        f_out_index.write(index_str)\n",
    "    if len(id_str) > 0:\n",
    "        f_out_id.write(id_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training and cost 69.08170100000007 s\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "start = time.clock()\n",
    "model = word2vec.Word2Vec(PathLineSentences('./output/issue-comment-sentences-new.txt'), size=100, window=5, min_count=5, workers=cores-1, iter=1, sg=1)\n",
    "end =time.clock()\n",
    "training_time=end-start\n",
    "print('end training and cost ' + str(training_time)+ ' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec model\n",
    "model.save('./output/word2vec-new.model')\n",
    "model.wv.save_word2vec_format('./output/word2vec-new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading sentences from file...\n",
      "IDF computation time: 5.940250999999989s\n"
     ]
    }
   ],
   "source": [
    "# calculate idf\n",
    "idf = {}\n",
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    start = time.clock()\n",
    "    lines = f.readlines()\n",
    "    print('Finished reading sentences from file...')    \n",
    "    vocab = list(Word2Vec.load('./output/word2vec-new.model').wv.vocab.keys())\n",
    "    N = len(lines)\n",
    "    docs = [sentence.split() for sentence in lines]\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            if word not in idf:\n",
    "                idf[word] = 1\n",
    "            else:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word in idf:\n",
    "        idf[word] = math.log(N / float(idf[word] + 1))\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('IDF computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average document length: 10.688945395875214\n"
     ]
    }
   ],
   "source": [
    "s_avg = 0 # avg doc length\n",
    "with open('./output/issue-comment-sentences-new.txt','r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    doc_lengths = [len(line.split()) for line in lines]\n",
    "    s_avg = sum(doc_lengths) / len(doc_lengths)\n",
    "    print(\"average document length: {}\".format(s_avg)) \n",
    "\n",
    "# Calculate combination scores of word2vec and bm25\n",
    "def bm25(doc, s2, idf):\n",
    "    score = 0\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "\n",
    "    for w in doc:\n",
    "        idf_s = idf.get(w, 1)\n",
    "        bm25_ra = s2.count(w) * (k1 + 1)\n",
    "        bm25_rb = s2.count(w) + k1 * (1 - b + b * len(s2) / s_avg)\n",
    "        score += idf_s * (bm25_ra / bm25_rb)\n",
    "    return score\n",
    "\n",
    "def compute(s1, s2, voc):   \n",
    "    v2 = np.array([voc[s] for s in s2 if s in voc])\n",
    "    v2 = v2.sum(axis=0)\n",
    "\n",
    "    v1 = np.array([voc[s] for s in s1 if s in voc])\n",
    "    v1 = v1.sum(axis=0)\n",
    "    \n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "def cosine(sentences, s2, voc):\n",
    "    s1_df_score = pd.Series(sentences)\n",
    "    s1_df_score = s1_df_score.map(lambda x: compute(x, s2, voc))\n",
    "    \n",
    "    s1_df_score.dropna(inplace=True)\n",
    "    return s1_df_score.sort_values(ascending=False).head(1)\n",
    "        \n",
    "def load_voc(file_voc):\n",
    "    vector_file = codecs.open(file_voc, 'r', encoding='utf-8')\n",
    "    line = vector_file.readline()\n",
    "    voc_size, vec_dim = map(int, line.split(' '))\n",
    "    embedding = dict()\n",
    "    line = vector_file.readline()\n",
    "    while line:\n",
    "        items = line.split(' ')\n",
    "        item = items[0]\n",
    "        vec = np.array(items[1:], dtype='float32')\n",
    "        embedding[item] = vec\n",
    "        line = vector_file.readline()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of APIs with at least 1 relevant issue: 21942\n"
     ]
    }
   ],
   "source": [
    "vocab = load_voc('./output/word2vec-new.txt')\n",
    "    \n",
    "relevant_issues = {}\n",
    "with open('./output/relevant_issues_for_caveats.jsonl') as f:\n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        relevant_issues[(obj['class'], obj['api'])] = obj['issue_ids']\n",
    "print('Number of APIs with at least 1 relevant issue: {}'.format(len(relevant_issues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF score computation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_tf_idf_query_similarity(vectorizer, vecs, query):\n",
    "    \"\"\"\n",
    "    vectorizer: TfIdfVectorizer model\n",
    "    docs_tfidf: tfidf vectors for all docs\n",
    "    query: query doc\n",
    "\n",
    "    return: cosine similarity between query and all docs\n",
    "    \"\"\"\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    sim_scores = cosine_similarity(query_tfidf, vecs).flatten()\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed TF-IDF vectors for all documents...\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectorizer = None\n",
    "tf_idf_vectors = None\n",
    "with open('./output/issue-comment-sentences-new.txt') as f:\n",
    "    docs = f.readlines()\n",
    "    \n",
    "    tf_idf_vectorizer = TfidfVectorizer(lowercase=None)\n",
    "    tf_idf_vectors = tf_idf_vectorizer.fit_transform(docs)\n",
    "    print(\"Computed TF-IDF vectors for all documents...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dicts to map issue sentences to metadata\n",
    "issue_id_to_sentence_index = {}\n",
    "with open('./output/associated-sentence-issue-ids-new.txt') as f2:\n",
    "    ids = f2.readlines()\n",
    "    ids = [int(x) for x in ids]\n",
    "    \n",
    "    for index, id in enumerate(ids):\n",
    "        if not id in issue_id_to_sentence_index:\n",
    "            issue_id_to_sentence_index[id] = []\n",
    "        issue_id_to_sentence_index[id].append(index)\n",
    "        \n",
    "sentence_index_to_df_index = {}\n",
    "with open('./output/associated-sentence-df-index-new.txt') as f:\n",
    "    indices = f.readlines()\n",
    "    indices = [int(x) for x in indices]\n",
    "\n",
    "    for index, df_index in enumerate(indices):\n",
    "        sentence_index_to_df_index[index] = df_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity for TF-IDF vectors computation time: 306.95871099999977s\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "\n",
    "with open('./output/tfidf_results_new.jsonl', 'w+') as f:\n",
    "    complete_tfidf_sim_results = ''\n",
    "    c = 0\n",
    "    # calculate scores for each caveat sentence\n",
    "    for i in caveats.index:\n",
    "        key = (caveats.loc[i,'simple_class_name'], caveats.loc[i,'api'])\n",
    "        if key in relevant_issues:\n",
    "            issue_ids = relevant_issues[key]\n",
    "\n",
    "            if len(issue_ids) > 0:\n",
    "                indices = []\n",
    "\n",
    "                for id in issue_ids:\n",
    "                    if id in issue_id_to_sentence_index:\n",
    "                        indices += issue_id_to_sentence_index[id]\n",
    "\n",
    "                if len(indices) > 0:\n",
    "                    relevant_vecs = tf_idf_vectors[indices,:]\n",
    "                    sim_scores = get_tf_idf_query_similarity(tf_idf_vectorizer, relevant_vecs, ' '.join(caveats.loc[i, 'tokens']))\n",
    "\n",
    "                    scores = []\n",
    "                    for j, score in enumerate(sim_scores):\n",
    "                        if score > 0:\n",
    "                            comment_index = sentence_index_to_df_index[indices[j]]\n",
    "                            scores.append({\n",
    "                                'score': score,\n",
    "                                'sentence_index': indices[j]\n",
    "                            })\n",
    "\n",
    "                    if len(scores) > 0:\n",
    "                        complete_tfidf_sim_results += ujson.dumps({\n",
    "                            'simple_class_name': key[0],\n",
    "                            'api': key[1],\n",
    "                            'caveat_sentence': caveats.loc[i, 'sentence'],\n",
    "                            'caveat_sentence_id': i,\n",
    "                            'tfidf_sim_scores': scores\n",
    "                        }) + '\\n'\n",
    "\n",
    "                        c += 1\n",
    "\n",
    "                        if c >= 2000:\n",
    "                            f.write(complete_tfidf_sim_results)\n",
    "\n",
    "                            c = 0\n",
    "                            complete_tfidf_sim_results = ''\n",
    "                        \n",
    "    if len(complete_tfidf_sim_results) > 0:\n",
    "        f.write(complete_tfidf_sim_results)\n",
    "           \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('Cosine similarity for TF-IDF vectors computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity computation time: 1137.973101s\n"
     ]
    }
   ],
   "source": [
    "with open('./output/combined_sim_results_new.jsonl', 'w+') as f_combo_out, \\\n",
    "    open('./output/word2vec_results_new.jsonl', 'w+') as f_w2v_out, \\\n",
    "    open('./output/bm25_results_new.jsonl', 'w+') as f_bm25_out, \\\n",
    "    open('./output/ir_error_log_new.jsonl', 'w+') as f_err:\n",
    "        \n",
    "    start = time.clock()\n",
    "    complete_combined_results = ''\n",
    "    complete_bm25_results = ''\n",
    "    complete_word2vec_results = ''\n",
    "    errors = ''\n",
    "    c=0\n",
    "    \n",
    "    for i in caveats.index:\n",
    "        key = (caveats.loc[i,'simple_class_name'], caveats.loc[i,'api'])\n",
    "        if key in relevant_issues:\n",
    "            issue_ids = relevant_issues[key]\n",
    "\n",
    "            if len(issue_ids) > 0:\n",
    "                try:\n",
    "                    caveat_sent = caveats.loc[i,'tokens']\n",
    "                    combined_sim_results = []\n",
    "                    bm25_results = []\n",
    "                    word2vec_results = []\n",
    "\n",
    "                    # retrieve issue comment sentences that are relevant\n",
    "                    relevant_comments_df = preprocessed_comments_df[preprocessed_comments_df['issue_id'].isin(issue_ids)]\n",
    "\n",
    "                    sim_w2v = relevant_comments_df['tokenised_sentences'].apply(cosine, s2=caveat_sent, voc=vocab)\n",
    "                    sim_bm25 = relevant_comments_df['tokenised_para'].apply(bm25, s2=caveat_sent, idf=idf)\n",
    "\n",
    "                    sim_bm25 = (sim_bm25 - sim_bm25.min()) / (sim_bm25.max() - sim_bm25.min())\n",
    "                    sim_bm25 = pd.to_numeric(sim_bm25, downcast='float')\n",
    "                    sim_w2v = pd.to_numeric(sim_w2v[0], downcast='float')\n",
    "\n",
    "                    if len(sim_bm25) != 0 or len(sim_w2v) != 0:\n",
    "                        # word2vec cosine similarity\n",
    "                        for j in sim_w2v.index:\n",
    "                            word2vec_results.append({\n",
    "                                'score': str(sim_w2v[j]),\n",
    "                                'issue_number': int(relevant_comments_df.loc[j, 'issue_id']),\n",
    "                                'comment': relevant_comments_df.loc[j,'body']\n",
    "                            })\n",
    "\n",
    "                        # bm25 score\n",
    "                        for j in sim_bm25.index:\n",
    "                            bm25_results.append({\n",
    "                                'score': str(sim_bm25[j]),\n",
    "                                'issue_number': int(relevant_comments_df.loc[j, 'issue_id']),\n",
    "                                'comment': relevant_comments_df.loc[j,'body']\n",
    "                            })\n",
    "\n",
    "                        # calculate combination similarity score\n",
    "                        combined_sim = 0.5 * sim_bm25.add(0.5 * sim_w2v, fill_value=0)\n",
    "                        for j in combined_sim.index:\n",
    "                            combined_sim_results.append({\n",
    "                                'issue_number': int(relevant_comments_df.loc[j,'issue_id']),\n",
    "                                'comment': relevant_comments_df.loc[j,'body'],\n",
    "                                'score': str(combined_sim[j]),\n",
    "                            })\n",
    "\n",
    "                        # Write results to relevant files\n",
    "                        if len(word2vec_results) > 0:\n",
    "                            complete_word2vec_results += ujson.dumps({\n",
    "                                'simple_class_name': key[0],\n",
    "                                'api': key[1],\n",
    "                                'caveat_sentence': caveats.loc[i, 'sentence'],\n",
    "                                'caveat_sentence_id': i,\n",
    "                                'w2v_results': word2vec_results\n",
    "                            }) + '\\n'\n",
    "\n",
    "                        if len(bm25_results) > 0:\n",
    "                            complete_bm25_results += ujson.dumps({\n",
    "                                'simple_class_name': key[0],\n",
    "                                'api': key[1],\n",
    "                                'caveat_sentence': caveats.loc[i, 'sentence'],\n",
    "                                'caveat_sentence_id': i,\n",
    "                                'bm25_results': bm25_results\n",
    "                            }) + '\\n'\n",
    "\n",
    "                        if len(combined_sim_results) > 0:\n",
    "                            complete_combined_results += ujson.dumps({\n",
    "                                'simple_class_name': key[0],\n",
    "                                'api': key[1],\n",
    "                                'caveat_sentence': caveats.loc[i, 'sentence'],\n",
    "                                'caveat_sentence_id': i,\n",
    "                                'combination_results': combined_sim_results\n",
    "                            }) + '\\n'\n",
    "                        \n",
    "                        c+=1\n",
    "                        \n",
    "                        # write all results to file\n",
    "                        if c >= 2000:\n",
    "                            c = 0\n",
    "                            \n",
    "                            f_combo_out.write(complete_combined_results)\n",
    "                            f_bm25_out.write(complete_bm25_results)\n",
    "                            f_w2v_out.write(complete_word2vec_results)\n",
    "                            \n",
    "                            # reset output strings\n",
    "                            complete_combined_results = ''\n",
    "                            complete_bm25_results = ''\n",
    "                            complete_word2vec_results = ''\n",
    "                            \n",
    "                            break\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    errors += ujson.dumps({'caveat_index': i, 'error': e}) + '\\n'\n",
    "    \n",
    "    # write any buffered results remaining\n",
    "    if len(complete_combined_results) > 0:\n",
    "        f_combo_out.write(complete_combined_results)\n",
    "    if len(complete_bm25_results) > 0:\n",
    "        f_bm25_out.write(complete_bm25_results)\n",
    "    if len(complete_word2vec_results) > 0:\n",
    "        f_w2v_out.write(complete_word2vec_results)\n",
    "        \n",
    "    # write error log\n",
    "    f_err.write(errors)\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('Similarity computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
