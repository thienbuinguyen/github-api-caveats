{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'himself', 'through', 'should', 'now', 'couldn', 'him', 'all', 'by', 'mustn', 'were', 'do', 'shan', 'their', 'down', 'yourself', 'the', 'out', 'doing', 'will', 'ma', 'themselves', 'needn', 'you', 'no', 'he', 'such', 've', 'while', 'off', 'haven', 'own', 'about', 'up', 'both', 'there', 'my', 'being', 'more', 'mightn', 'again', 'that', 'it', 'hasn', 'are', 'needn', 'during', 'herself', 'wouldn', 'until', 'these', 're', 'mustn', 'your', 'if', 'each', 'does', 'doesn', 'between', 'you', 'his', 'some', 'not', 'haven', 'weren', 'hadn', 'those', 'with', 'has', 'was', 'most', 'why', 'isn', 'am', 'yourselves', 'ours', 'ourselves', 'have', 'wasn', 'who', 'as', 'me', 'what', 'after', 'that', 'any', 'an', 'didn', 'which', 'shan', 'you', 'had', 'couldn', 'before', 'is', 'once', 'where', 'at', 'for', 'they', 'further', 'aren', 'yours', 'into', 'other', 'below', 'myself', 'than', 'but', 'on', 'won', 'against', 'here', 'isn', 'shouldn', 'ain', 'don', 'wouldn', 'from', 'her', 'wasn', 'them', 'hers', 'it', 'nor', 'weren', 'over', 'and', 'having', 'she', 'only', 'be', 'should', 'this', 'mightn', 'won', 'to', 'in', 'how', 'few', 'll', 'shouldn', 'of', 'because', 'whom', 'aren', 'itself', 'you', 'same', 'don', 'our', 'just', 'can', 'she', 'been', 'when', 'theirs', 'or', 'very', 'we', 'too', 'doesn', 'hasn', 'didn', 'you', 'did', 'its', 'then', 'hadn', 'above', 'under']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ujson\n",
    "import glob\n",
    "import time\n",
    "import multiprocessing\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from gensim.models.word2vec import PathLineSentences\n",
    "from itertools import repeat\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "custom_stopwords = [s.split(\"'\")[0] for s in stopwords]\n",
    "custom_stopwords = [s for s in custom_stopwords if len(s) > 1]\n",
    "print(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues: 627450\n",
      "Number of comments: 1855870\n"
     ]
    }
   ],
   "source": [
    "# load the extracted java-related GitHub data\n",
    "\n",
    "def load_github_issues():\n",
    "    with open('./output/issue-comments-revised.jsonl') as issue_comments_f:\n",
    "        issue_list = []\n",
    "        comments_list = []\n",
    "\n",
    "        for line in issue_comments_f:\n",
    "            obj = ujson.loads(line)\n",
    "\n",
    "            comments_list.append({\n",
    "                'body': obj['body'],\n",
    "                'repo_name': obj['repo_name'],\n",
    "                'html_url': obj['html_url'],\n",
    "                'issue_id': obj['issue']['id']\n",
    "            })\n",
    "\n",
    "            issue = obj['issue']\n",
    "            issue['repo_name'] = obj['repo_name']\n",
    "            issue_list.append(issue)\n",
    "\n",
    "        issues_df = pd.DataFrame(issue_list)\n",
    "        issues_df = issues_df.drop_duplicates(subset=['id'])\n",
    "        comments_df = pd.DataFrame(comments_list)\n",
    "        \n",
    "        return (issues_df, comments_df)\n",
    "\n",
    "\n",
    "issues_df, comments_df = load_github_issues()\n",
    "print(\"Number of issues: {}\".format(len(issues_df.index)))\n",
    "print(\"Number of comments: {}\".format(len(comments_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of issues with code block/s: 85318\n"
     ]
    }
   ],
   "source": [
    "# find the issues that contain code blocks\n",
    "code_issues_df = issues_df.dropna(subset=['body'])\n",
    "code_issues_df = code_issues_df[code_issues_df['body'].str.contains('```')]\n",
    "print(\"Number of issues with code block/s: {}\".format(len(code_issues_df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>html_url</th>\n",
       "      <th>issue_id</th>\n",
       "      <th>repo_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I can reproduce this with the below test snipp...</td>\n",
       "      <td>https://github.com/elastic/elasticsearch/issue...</td>\n",
       "      <td>264716524</td>\n",
       "      <td>elastic/elasticsearch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't really like the snackbars; I think we ...</td>\n",
       "      <td>https://github.com/ritvikkar/MovieFinder/issue...</td>\n",
       "      <td>285146015</td>\n",
       "      <td>ritvikkar/MovieFinder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just an example to see how to use the  `thymel...</td>\n",
       "      <td>https://github.com/jooby-project/jooby/issues/...</td>\n",
       "      <td>284439800</td>\n",
       "      <td>jooby-project/jooby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@nedtwigg Would it be practical for you to upg...</td>\n",
       "      <td>https://github.com/diffplug/spotless/issues/18...</td>\n",
       "      <td>285279535</td>\n",
       "      <td>diffplug/spotless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed</td>\n",
       "      <td>https://github.com/Alex-the-666/Ice_and_Fire/i...</td>\n",
       "      <td>258569936</td>\n",
       "      <td>Alex-the-666/Ice_and_Fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  I can reproduce this with the below test snipp...   \n",
       "1  I don't really like the snackbars; I think we ...   \n",
       "2  Just an example to see how to use the  `thymel...   \n",
       "3  @nedtwigg Would it be practical for you to upg...   \n",
       "4                                              fixed   \n",
       "\n",
       "                                            html_url   issue_id  \\\n",
       "0  https://github.com/elastic/elasticsearch/issue...  264716524   \n",
       "1  https://github.com/ritvikkar/MovieFinder/issue...  285146015   \n",
       "2  https://github.com/jooby-project/jooby/issues/...  284439800   \n",
       "3  https://github.com/diffplug/spotless/issues/18...  285279535   \n",
       "4  https://github.com/Alex-the-666/Ice_and_Fire/i...  258569936   \n",
       "\n",
       "                   repo_name  \n",
       "0      elastic/elasticsearch  \n",
       "1      ritvikkar/MovieFinder  \n",
       "2        jooby-project/jooby  \n",
       "3          diffplug/spotless  \n",
       "4  Alex-the-666/Ice_and_Fire  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove all code blocks\n",
    "    doc = re.sub(r'```([^```]*)```', '', doc)\n",
    "            \n",
    "    # remove urls\n",
    "    doc = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',             \n",
    "        '', doc)\n",
    "    \n",
    "    # remove line break characters\n",
    "    doc = re.sub(r'[\\r\\n]', ' ', doc)\n",
    "    \n",
    "    # remove ownership apostrophe\n",
    "    doc = re.sub(r\"'\\w |\\w' \", ' ', doc)\n",
    "    \n",
    "    # remove numbers\n",
    "    doc = re.sub(r'(\\d\\.?)+', ' ', doc)\n",
    "        \n",
    "    # replace all punctuation except for full stop with space\n",
    "    doc = re.sub(r'[^A-Za-z\\.]', ' ', doc)\n",
    "        \n",
    "    # normalise full stops\n",
    "    doc = re.sub(r'\\s\\.\\.+', '.', doc)\n",
    "    \n",
    "    # remove more than 1 whitespace\n",
    "    doc = re.sub('\\s\\s+', ' ', doc)\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    return doc\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    docs = doc.split('. ')\n",
    "    docs = [t for t in docs if t != '']\n",
    "    return docs\n",
    "\n",
    "def doc_tokenize(doc):\n",
    "    doc = re.sub('\\.', ' ', doc) # remove full stops\n",
    "    \n",
    "    tokens = [t.lower() for t in doc.split() if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = re.sub('\\.', ' ', sentence) # remove full stops\n",
    "    \n",
    "    tokens = sentence.split()\n",
    "    tokens = [t.lower() for t in tokens if len(t) > 1]\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caveat sentences: 73831\n"
     ]
    }
   ],
   "source": [
    "# load the non-dreprecated java doc caveat sentences\n",
    "\n",
    "def load_caveats():\n",
    "    caveat_files_dir = './output/java_12_spec_caveat_sentences_revised/'\n",
    "    caveats_list = []\n",
    "\n",
    "    files = glob.glob(caveat_files_dir + '*.json')\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            arr = ujson.load(f)\n",
    "            full_class_name = os.path.splitext(os.path.basename(file))[0]\n",
    "            simple_class_name = full_class_name.split('.')[-1]\n",
    "            for caveat in arr:\n",
    "                if not caveat['deprecated'] and 'name' in caveat:\n",
    "                    for sentence in caveat['sentences']:\n",
    "                        caveats_list.append({\n",
    "                            'simple_class_name': simple_class_name,\n",
    "                            'full_class_name': full_class_name,\n",
    "                            'api': caveat['name'],\n",
    "                            'sentence': sentence,\n",
    "                            'type': 'body'\n",
    "                        })\n",
    "\n",
    "                    # add all misc level sentences\n",
    "                    for misc_obj in caveat['caveat_misc']:\n",
    "                        if misc_obj['name'] in ['Parameters:', 'Throws:']:\n",
    "                            for obj in misc_obj['list']:\n",
    "                                for misc_sentence in obj['sentences']:\n",
    "                                    caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': misc_sentence,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "                        else:\n",
    "                            for s in misc_obj['list']:\n",
    "                                caveats_list.append({\n",
    "                                        'simple_class_name': simple_class_name,\n",
    "                                        'full_class_name': full_class_name,\n",
    "                                        'api': caveat['name'],\n",
    "                                        'sentence': s,\n",
    "                                        'type': 'misc'\n",
    "                                    })\n",
    "\n",
    "    return pd.DataFrame(caveats_list)\n",
    "\n",
    "caveats = load_caveats()\n",
    "print('Number of caveat sentences: {}'.format(len(caveats.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>api</th>\n",
       "      <th>full_class_name</th>\n",
       "      <th>sentence</th>\n",
       "      <th>simple_class_name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This statistic is reset when the thread conten...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>the approximate accumulated elapsed time in mi...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>getBlockedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>if the Java virtual machine does not support t...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getWaitedTime</td>\n",
       "      <td>java.lang.management.ThreadInfo</td>\n",
       "      <td>This method returns -1 if thread contention mo...</td>\n",
       "      <td>ThreadInfo</td>\n",
       "      <td>body</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              api                  full_class_name  \\\n",
       "0  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "1  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "2  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "3  getBlockedTime  java.lang.management.ThreadInfo   \n",
       "4   getWaitedTime  java.lang.management.ThreadInfo   \n",
       "\n",
       "                                            sentence simple_class_name  type  \n",
       "0  This method returns -1 if thread contention mo...        ThreadInfo  body  \n",
       "1  This statistic is reset when the thread conten...        ThreadInfo  body  \n",
       "2  the approximate accumulated elapsed time in mi...        ThreadInfo  misc  \n",
       "3  if the Java virtual machine does not support t...        ThreadInfo  misc  \n",
       "4  This method returns -1 if thread contention mo...        ThreadInfo  body  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caveats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "caveats['preprocessed'] = caveats['sentence'].apply(lambda x: preprocess(x))\n",
    "caveats['tokens'] = caveats['preprocessed'].map(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate text in the issue all comments for each issue\n",
    "# issue_to_concat_text = {}\n",
    "# for index, row in code_issues_df.iterrows():\n",
    "#     assoc_comments_df = comments_df[comments_df['issue_id'] == row['id']]\n",
    "    \n",
    "#     concat_text = row['title'] + row['body']\n",
    "    \n",
    "#     for j, comment_row in assoc_comments_df.iterrows():\n",
    "#         concat_text += row['body']\n",
    "        \n",
    "#     issue_to_concat_text[row['id']] = concat_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of apis: 21942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/thien/anaconda2/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Identify the relevant issues for all caveats\n",
    "# apis = set()\n",
    "# for caveat in caveats_list:\n",
    "#     apis.add((caveat['simple_class_name'], caveat['api']))\n",
    "    \n",
    "# print('Number of apis: {}'.format(len(apis)))\n",
    "    \n",
    "# def get_relevant_issues(api_tuple, issue_to_text_dict):\n",
    "#     relevant = []\n",
    "#     for id in issue_to_text_dict:\n",
    "#         if api_tuple[0] in issue_to_text_dict[id]:\n",
    "#             if (api_tuple[0] == api_tuple[1]) or api_tuple[1] in issue_to_text_dict[id]:\n",
    "#                 relevant.append(id)\n",
    "            \n",
    "#     return relevant\n",
    "\n",
    "# p = multiprocessing.Pool(2)\n",
    "# relevant_issues = p.starmap(get_relevant_issues, zip(get_relevant_issues, repeat(issue_to_concat_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./output/relevant_issues_for_caveats.jsonl', 'w+') as f:\n",
    "#     for i, api in enumerate(apis):\n",
    "#         f.write(ujson.dumps({'class': api[0], 'api': api[1], 'issue_ids': relevant_issues[i]}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed all comments...\n",
      "Tokenized all paragraphs...\n",
      "Completed sentence tokenization...\n",
      "Tokenized all sentences...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence_list(sent_list):\n",
    "    return [tokenize(x) for x in sent_list]\n",
    "\n",
    "def calculate_preprocessed_comment_sentences(df):\n",
    "    df['preprocessed_comments'] = df['body'].map(lambda x: preprocess(x))\n",
    "    print('Preprocessed all comments...')\n",
    "    df['tokenised_para'] = df['preprocessed_comments'].map(lambda x: doc_tokenize(x))\n",
    "    print('Tokenized all paragraphs...')\n",
    "    df['sentences'] = df['preprocessed_comments'].map(lambda x: sent_tokenize(x))\n",
    "    print('Completed sentence tokenization...')\n",
    "    df['tokenised_sentences'] = df['sentences'].map(lambda x: tokenize_sentence_list(x))\n",
    "    print('Tokenized all sentences...')\n",
    "                \n",
    "# with open('./output/issue-comment-sentences-2.txt', 'w+') as f, \\\n",
    "#     open('./output/associated-sentence-issue-ids-2.txt', 'w+') as f_2:\n",
    "\n",
    "#     comment_sentences = []\n",
    "#     sentences = ''\n",
    "#     associated_issues = ''\n",
    "\n",
    "#     for i in df.index:\n",
    "#         issue_id = str(df.loc[i, 'issue_id'])\n",
    "\n",
    "#         for tokens in df.loc[i, 'tokenised_sentences']:\n",
    "#             sentence = ' '.join(tokens) # convert to gensim format for PathLineSentences\n",
    "#             sentences += sentence + '\\n'\n",
    "#             associated_issues += issue_id + '\\n'\n",
    "\n",
    "#     f.write(sentences)\n",
    "#     f_2.write(associated_issues)\n",
    "            \n",
    "calculate_preprocessed_comment_sentences(comments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end training and cost 450.77793999999994 s\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "start = time.clock()\n",
    "model = word2vec.Word2Vec(PathLineSentences('./output/issue-comment-sentences-2.txt'), size=100, window=5, min_count=5, workers=cores-1, iter=1, sg=1)\n",
    "end =time.clock()\n",
    "training_time=end-start\n",
    "print('end training and cost ' + str(training_time)+ ' s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec model\n",
    "model.save('./output/word2vec.model')\n",
    "model.wv.save_word2vec_format('./output/word2vec.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading sentences from file...\n",
      "IDF computation time: 21.712850000000003s\n"
     ]
    }
   ],
   "source": [
    "# calculate idf\n",
    "idf = {}\n",
    "with open('./output/issue-comment-sentences-2.txt') as f:\n",
    "    start = time.clock()\n",
    "    lines = f.readlines()\n",
    "    print('Finished reading sentences from file...')    \n",
    "    vocab = list(Word2Vec.load('./output/word2vec.model').wv.vocab.keys())\n",
    "    N = len(lines)\n",
    "    docs = [sentence.split() for sentence in lines]\n",
    "    \n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            if word not in idf:\n",
    "                idf[word] = 1\n",
    "            else:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    for word in idf:\n",
    "        idf[word] = math.log(N / float(idf[word] + 1))\n",
    "    \n",
    "    end = time.clock()\n",
    "    training_time=end-start\n",
    "    print('IDF computation time: {}s'.format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average document length: 11.103317980494701\n"
     ]
    }
   ],
   "source": [
    "s_avg = 0 # avg doc length\n",
    "with open('./output/issue-comment-sentences-2.txt','r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "    doc_lengths = [len(line.split()) for line in lines]\n",
    "    s_avg = sum(doc_lengths) / len(doc_lengths)\n",
    "    print(\"average document length: {}\".format(s_avg)) \n",
    "\n",
    "# Calculate combination scores of word2vec and bm25\n",
    "def bm25(doc, s2, idf):\n",
    "    score = 0\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "\n",
    "    for w in doc:\n",
    "        idf_s = idf.get(w, 1)\n",
    "        bm25_ra = s2.count(w) * (k1 + 1)\n",
    "        bm25_rb = s2.count(w) + k1 * (1 - b + b * len(s2) / s_avg)\n",
    "        score += idf_s * (bm25_ra / bm25_rb)\n",
    "    return score\n",
    "\n",
    "def compute(s1, s2, voc):\n",
    "    v2 = np.array([voc[s] for s in s2 if s in voc])\n",
    "    v2 = v2.sum(axis=0)\n",
    "\n",
    "    v1 = np.array([voc[s] for s in s1 if s in voc])\n",
    "    v1 = v1.sum(axis=0)\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "\n",
    "def cosine(sentences, s2, voc):\n",
    "    if not sentences:\n",
    "        sentences = []\n",
    "    s1_df_score = pd.Series(sentences)\n",
    "    s1_df_score = s1_df_score.map(lambda x: compute(x, s2, voc))\n",
    "    if type(s1_df_score.dropna()) != 'NoneType':\n",
    "        if len(s1_df_score.dropna()) != 0:\n",
    "            return s1_df_score.sort_values(ascending=False).head(1)\n",
    "        \n",
    "def load_voc(file_voc):\n",
    "    vector_file = codecs.open(file_voc, 'r', encoding='utf-8')\n",
    "    line = vector_file.readline()\n",
    "    voc_size, vec_dim = map(int, line.split(' '))\n",
    "    embedding = dict()\n",
    "    line = vector_file.readline()\n",
    "    while line:\n",
    "        items = line.split(' ')\n",
    "        item = items[0]\n",
    "        vec = np.array(items[1:], dtype='float32')\n",
    "        embedding[item] = vec\n",
    "        line = vector_file.readline()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = load_voc('./output/word2vec.txt')\n",
    "    \n",
    "relevant_issues = {}\n",
    "with open('./output/relevant_issues_for_caveats.jsonl') as f:\n",
    "    for line in f:\n",
    "        obj = ujson.loads(line)\n",
    "        relevant_issues[(obj['class'], obj['api'])] = obj['issue_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/thien/Data Drive1/combined_sim_results.jsonl', 'w+') as f_out, \\\n",
    "    open('/media/thien/Data Drive1/combined_sim_results_errors.jsonl', 'w+') as f_err:\n",
    "    \n",
    "    for i in caveats.index:\n",
    "        key = (caveats.loc[i,'simple_class_name'], caveats.loc[i,'api'])\n",
    "        if key in relevant_issues:\n",
    "            issue_ids = relevant_issues[key]\n",
    "        \n",
    "            if len(issue_ids) > 0:\n",
    "                try:\n",
    "                    caveat_sent = caveats.loc[i,'tokens']\n",
    "\n",
    "                    # retrieve issue comment sentences that are relevant\n",
    "                    relevant_comments_df = comments_df[comments_df['issue_id'].isin(issue_ids)]\n",
    "\n",
    "                    sim_w2v = relevant_comments_df['tokenised_sentences'].apply(cosine, s2=caveat_sent, voc=vocab)\n",
    "                    sim_bm25 = relevant_comments_df['tokenised_para'].apply(bm25, s2=caveat_sent, idf=idf)\n",
    "\n",
    "                    sim_bm25 = (sim_bm25 - sim_bm25.min()) / (sim_bm25.max() - sim_bm25.min())\n",
    "                    sim_bm25 = pd.to_numeric(sim_bm25, downcast='float')\n",
    "                    sim_w2v = pd.to_numeric(sim_w2v[0], downcast='float')\n",
    "                    \n",
    "                    if len(sim_bm25) != 0 or len(sim_w2v) != 0:\n",
    "                        combined_sim = 0.5 * sim_bm25.add(0.5 * sim_w2v, fill_value=0)\n",
    "\n",
    "                        for j in combined_sim.index:\n",
    "                            f_out.write(ujson.dumps({\n",
    "                                'issue_number': int(relevant_comments_df.loc[j,'issue_id']),\n",
    "                                'comment': relevant_comments_df.loc[j,'body'],\n",
    "                                'caveat_sentence': caveats.loc[i,'sentence'],\n",
    "                                'api': caveats.loc[i, 'api'],\n",
    "                                'simple_class_name': caveats.loc[i,'simple_class_name'],\n",
    "                                'full_class_name': caveats.loc[i, 'full_class_name'],\n",
    "                                'html_url': relevant_comments_df.loc[j,'html_url'],\n",
    "                                'score': str(combined_sim[j]),\n",
    "                                'caveat_sentence_id': i\n",
    "                            }) + '\\n')\n",
    "\n",
    "                except Exception as e:\n",
    "                    f_err.write(ujson.dumps({'index': i, 'error': e}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
